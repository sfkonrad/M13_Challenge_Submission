{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##### M13_Challenge_KonradK_venture_funding_with_deep_learning.ipynb\n",
    "## Konrad Kozicki\n",
    "### UCB-VIRT-FIN-PT-12-2020-U-B-TTH\n",
    "# Module 13 Challenge Submission\n",
    "---\n",
    "\n",
    "# Venture Funding with Deep Learning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler,OneHotEncoder\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Prepare the data to be used on a neural network model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Read the `applicants_data.csv` file into a Pandas DataFrame. Review the DataFrame, looking for categorical variables that will need to be encoded, as well as columns that could eventually define your features and target variables.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EIN</th>\n",
       "      <th>NAME</th>\n",
       "      <th>APPLICATION_TYPE</th>\n",
       "      <th>AFFILIATION</th>\n",
       "      <th>CLASSIFICATION</th>\n",
       "      <th>USE_CASE</th>\n",
       "      <th>ORGANIZATION</th>\n",
       "      <th>STATUS</th>\n",
       "      <th>INCOME_AMT</th>\n",
       "      <th>SPECIAL_CONSIDERATIONS</th>\n",
       "      <th>ASK_AMT</th>\n",
       "      <th>IS_SUCCESSFUL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10520599</td>\n",
       "      <td>BLUE KNIGHTS MOTORCYCLE CLUB</td>\n",
       "      <td>T10</td>\n",
       "      <td>Independent</td>\n",
       "      <td>C1000</td>\n",
       "      <td>ProductDev</td>\n",
       "      <td>Association</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>N</td>\n",
       "      <td>5000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10531628</td>\n",
       "      <td>AMERICAN CHESAPEAKE CLUB CHARITABLE TR</td>\n",
       "      <td>T3</td>\n",
       "      <td>Independent</td>\n",
       "      <td>C2000</td>\n",
       "      <td>Preservation</td>\n",
       "      <td>Co-operative</td>\n",
       "      <td>1</td>\n",
       "      <td>1-9999</td>\n",
       "      <td>N</td>\n",
       "      <td>108590</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10547893</td>\n",
       "      <td>ST CLOUD PROFESSIONAL FIREFIGHTERS</td>\n",
       "      <td>T5</td>\n",
       "      <td>CompanySponsored</td>\n",
       "      <td>C3000</td>\n",
       "      <td>ProductDev</td>\n",
       "      <td>Association</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>N</td>\n",
       "      <td>5000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10553066</td>\n",
       "      <td>SOUTHSIDE ATHLETIC ASSOCIATION</td>\n",
       "      <td>T3</td>\n",
       "      <td>CompanySponsored</td>\n",
       "      <td>C2000</td>\n",
       "      <td>Preservation</td>\n",
       "      <td>Trust</td>\n",
       "      <td>1</td>\n",
       "      <td>10000-24999</td>\n",
       "      <td>N</td>\n",
       "      <td>6692</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10556103</td>\n",
       "      <td>GENETIC RESEARCH INSTITUTE OF THE DESERT</td>\n",
       "      <td>T3</td>\n",
       "      <td>Independent</td>\n",
       "      <td>C1000</td>\n",
       "      <td>Heathcare</td>\n",
       "      <td>Trust</td>\n",
       "      <td>1</td>\n",
       "      <td>100000-499999</td>\n",
       "      <td>N</td>\n",
       "      <td>142590</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        EIN                                      NAME APPLICATION_TYPE  \\\n",
       "0  10520599              BLUE KNIGHTS MOTORCYCLE CLUB              T10   \n",
       "1  10531628    AMERICAN CHESAPEAKE CLUB CHARITABLE TR               T3   \n",
       "2  10547893        ST CLOUD PROFESSIONAL FIREFIGHTERS               T5   \n",
       "3  10553066            SOUTHSIDE ATHLETIC ASSOCIATION               T3   \n",
       "4  10556103  GENETIC RESEARCH INSTITUTE OF THE DESERT               T3   \n",
       "\n",
       "        AFFILIATION CLASSIFICATION      USE_CASE  ORGANIZATION  STATUS  \\\n",
       "0       Independent          C1000    ProductDev   Association       1   \n",
       "1       Independent          C2000  Preservation  Co-operative       1   \n",
       "2  CompanySponsored          C3000    ProductDev   Association       1   \n",
       "3  CompanySponsored          C2000  Preservation         Trust       1   \n",
       "4       Independent          C1000     Heathcare         Trust       1   \n",
       "\n",
       "      INCOME_AMT SPECIAL_CONSIDERATIONS  ASK_AMT  IS_SUCCESSFUL  \n",
       "0              0                      N     5000              1  \n",
       "1         1-9999                      N   108590              1  \n",
       "2              0                      N     5000              0  \n",
       "3    10000-24999                      N     6692              1  \n",
       "4  100000-499999                      N   142590              1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the applicants_data.csv file from the Resources folder into a Pandas DataFrame\n",
    "applicant_data_df = pd.read_csv(\n",
    "    Path(\"./Resources/applicants_data.csv\")\n",
    ")\n",
    "\n",
    "# Review the DataFrame\n",
    "applicant_data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EIN                        int64\n",
       "NAME                      object\n",
       "APPLICATION_TYPE          object\n",
       "AFFILIATION               object\n",
       "CLASSIFICATION            object\n",
       "USE_CASE                  object\n",
       "ORGANIZATION              object\n",
       "STATUS                     int64\n",
       "INCOME_AMT                object\n",
       "SPECIAL_CONSIDERATIONS    object\n",
       "ASK_AMT                    int64\n",
       "IS_SUCCESSFUL              int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Review the data types associated with the columns\n",
    "applicant_data_df.dtypes"
   ]
  },
  {
   "attachments": {
    "43165e11-dfea-4007-99fa-9bfdbd980c64.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">>>> ![image.png](attachment:43165e11-dfea-4007-99fa-9bfdbd980c64.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Drop the “EIN” (Employer Identification Number) and “NAME” columns from the DataFrame, because they are not relevant to the binary classification model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>APPLICATION_TYPE</th>\n",
       "      <th>AFFILIATION</th>\n",
       "      <th>CLASSIFICATION</th>\n",
       "      <th>USE_CASE</th>\n",
       "      <th>ORGANIZATION</th>\n",
       "      <th>STATUS</th>\n",
       "      <th>INCOME_AMT</th>\n",
       "      <th>SPECIAL_CONSIDERATIONS</th>\n",
       "      <th>ASK_AMT</th>\n",
       "      <th>IS_SUCCESSFUL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>T10</td>\n",
       "      <td>Independent</td>\n",
       "      <td>C1000</td>\n",
       "      <td>ProductDev</td>\n",
       "      <td>Association</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>N</td>\n",
       "      <td>5000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>T3</td>\n",
       "      <td>Independent</td>\n",
       "      <td>C2000</td>\n",
       "      <td>Preservation</td>\n",
       "      <td>Co-operative</td>\n",
       "      <td>1</td>\n",
       "      <td>1-9999</td>\n",
       "      <td>N</td>\n",
       "      <td>108590</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>T5</td>\n",
       "      <td>CompanySponsored</td>\n",
       "      <td>C3000</td>\n",
       "      <td>ProductDev</td>\n",
       "      <td>Association</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>N</td>\n",
       "      <td>5000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>T3</td>\n",
       "      <td>CompanySponsored</td>\n",
       "      <td>C2000</td>\n",
       "      <td>Preservation</td>\n",
       "      <td>Trust</td>\n",
       "      <td>1</td>\n",
       "      <td>10000-24999</td>\n",
       "      <td>N</td>\n",
       "      <td>6692</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>T3</td>\n",
       "      <td>Independent</td>\n",
       "      <td>C1000</td>\n",
       "      <td>Heathcare</td>\n",
       "      <td>Trust</td>\n",
       "      <td>1</td>\n",
       "      <td>100000-499999</td>\n",
       "      <td>N</td>\n",
       "      <td>142590</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  APPLICATION_TYPE       AFFILIATION CLASSIFICATION      USE_CASE  \\\n",
       "0              T10       Independent          C1000    ProductDev   \n",
       "1               T3       Independent          C2000  Preservation   \n",
       "2               T5  CompanySponsored          C3000    ProductDev   \n",
       "3               T3  CompanySponsored          C2000  Preservation   \n",
       "4               T3       Independent          C1000     Heathcare   \n",
       "\n",
       "   ORGANIZATION  STATUS     INCOME_AMT SPECIAL_CONSIDERATIONS  ASK_AMT  \\\n",
       "0   Association       1              0                      N     5000   \n",
       "1  Co-operative       1         1-9999                      N   108590   \n",
       "2   Association       1              0                      N     5000   \n",
       "3         Trust       1    10000-24999                      N     6692   \n",
       "4         Trust       1  100000-499999                      N   142590   \n",
       "\n",
       "   IS_SUCCESSFUL  \n",
       "0              1  \n",
       "1              1  \n",
       "2              0  \n",
       "3              1  \n",
       "4              1  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop the 'EIN' and 'NAME' columns from the DataFrame\n",
    "applicant_data_df = applicant_data_df.drop(columns = [\"EIN\", \"NAME\"])\n",
    "\n",
    "# Review the DataFrame\n",
    "applicant_data_df[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Encode the dataset’s categorical variables using `OneHotEncoder`, and then place the encoded variables into a new DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['APPLICATION_TYPE',\n",
       " 'AFFILIATION',\n",
       " 'CLASSIFICATION',\n",
       " 'USE_CASE',\n",
       " 'ORGANIZATION',\n",
       " 'INCOME_AMT',\n",
       " 'SPECIAL_CONSIDERATIONS']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a list of categorical variables \n",
    "categorical_variables = list(applicant_data_df.dtypes[applicant_data_df.dtypes == \"object\"].index)\n",
    "\n",
    "# Display the categorical variables list\n",
    "categorical_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a OneHotEncoder instance\n",
    "enc = OneHotEncoder(sparse=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the categorcal variables using OneHotEncoder\n",
    "encoded_data = enc.fit_transform(applicant_data_df[categorical_variables])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>APPLICATION_TYPE_T10</th>\n",
       "      <th>APPLICATION_TYPE_T12</th>\n",
       "      <th>APPLICATION_TYPE_T13</th>\n",
       "      <th>APPLICATION_TYPE_T14</th>\n",
       "      <th>APPLICATION_TYPE_T15</th>\n",
       "      <th>APPLICATION_TYPE_T17</th>\n",
       "      <th>APPLICATION_TYPE_T19</th>\n",
       "      <th>APPLICATION_TYPE_T2</th>\n",
       "      <th>APPLICATION_TYPE_T25</th>\n",
       "      <th>APPLICATION_TYPE_T29</th>\n",
       "      <th>...</th>\n",
       "      <th>INCOME_AMT_1-9999</th>\n",
       "      <th>INCOME_AMT_10000-24999</th>\n",
       "      <th>INCOME_AMT_100000-499999</th>\n",
       "      <th>INCOME_AMT_10M-50M</th>\n",
       "      <th>INCOME_AMT_1M-5M</th>\n",
       "      <th>INCOME_AMT_25000-99999</th>\n",
       "      <th>INCOME_AMT_50M+</th>\n",
       "      <th>INCOME_AMT_5M-10M</th>\n",
       "      <th>SPECIAL_CONSIDERATIONS_N</th>\n",
       "      <th>SPECIAL_CONSIDERATIONS_Y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 114 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   APPLICATION_TYPE_T10  APPLICATION_TYPE_T12  APPLICATION_TYPE_T13  \\\n",
       "0                   1.0                   0.0                   0.0   \n",
       "1                   0.0                   0.0                   0.0   \n",
       "2                   0.0                   0.0                   0.0   \n",
       "3                   0.0                   0.0                   0.0   \n",
       "4                   0.0                   0.0                   0.0   \n",
       "\n",
       "   APPLICATION_TYPE_T14  APPLICATION_TYPE_T15  APPLICATION_TYPE_T17  \\\n",
       "0                   0.0                   0.0                   0.0   \n",
       "1                   0.0                   0.0                   0.0   \n",
       "2                   0.0                   0.0                   0.0   \n",
       "3                   0.0                   0.0                   0.0   \n",
       "4                   0.0                   0.0                   0.0   \n",
       "\n",
       "   APPLICATION_TYPE_T19  APPLICATION_TYPE_T2  APPLICATION_TYPE_T25  \\\n",
       "0                   0.0                  0.0                   0.0   \n",
       "1                   0.0                  0.0                   0.0   \n",
       "2                   0.0                  0.0                   0.0   \n",
       "3                   0.0                  0.0                   0.0   \n",
       "4                   0.0                  0.0                   0.0   \n",
       "\n",
       "   APPLICATION_TYPE_T29  ...  INCOME_AMT_1-9999  INCOME_AMT_10000-24999  \\\n",
       "0                   0.0  ...                0.0                     0.0   \n",
       "1                   0.0  ...                1.0                     0.0   \n",
       "2                   0.0  ...                0.0                     0.0   \n",
       "3                   0.0  ...                0.0                     1.0   \n",
       "4                   0.0  ...                0.0                     0.0   \n",
       "\n",
       "   INCOME_AMT_100000-499999  INCOME_AMT_10M-50M  INCOME_AMT_1M-5M  \\\n",
       "0                       0.0                 0.0               0.0   \n",
       "1                       0.0                 0.0               0.0   \n",
       "2                       0.0                 0.0               0.0   \n",
       "3                       0.0                 0.0               0.0   \n",
       "4                       1.0                 0.0               0.0   \n",
       "\n",
       "   INCOME_AMT_25000-99999  INCOME_AMT_50M+  INCOME_AMT_5M-10M  \\\n",
       "0                     0.0              0.0                0.0   \n",
       "1                     0.0              0.0                0.0   \n",
       "2                     0.0              0.0                0.0   \n",
       "3                     0.0              0.0                0.0   \n",
       "4                     0.0              0.0                0.0   \n",
       "\n",
       "   SPECIAL_CONSIDERATIONS_N  SPECIAL_CONSIDERATIONS_Y  \n",
       "0                       1.0                       0.0  \n",
       "1                       1.0                       0.0  \n",
       "2                       1.0                       0.0  \n",
       "3                       1.0                       0.0  \n",
       "4                       1.0                       0.0  \n",
       "\n",
       "[5 rows x 114 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a DataFrame with the encoded variables\n",
    "encoded_df = pd.DataFrame(\n",
    "    encoded_data,\n",
    "    columns = enc.get_feature_names(categorical_variables)\n",
    ")\n",
    "\n",
    "# Review the DataFrame\n",
    "encoded_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Add the original DataFrame’s numerical variables to the DataFrame containing the encoded variables.\n",
    "\n",
    "> **Note** To complete this step, you will employ the Pandas `concat()` function that was introduced earlier in this course. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>STATUS</th>\n",
       "      <th>ASK_AMT</th>\n",
       "      <th>IS_SUCCESSFUL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>5000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>108590</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>5000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>6692</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>142590</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   STATUS  ASK_AMT  IS_SUCCESSFUL\n",
       "0       1     5000              1\n",
       "1       1   108590              1\n",
       "2       1     5000              0\n",
       "3       1     6692              1\n",
       "4       1   142590              1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numerical_variables_df = applicant_data_df.drop(columns = categorical_variables)\n",
    "\n",
    "# Review the DataFrame\n",
    "numerical_variables_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>STATUS</th>\n",
       "      <th>ASK_AMT</th>\n",
       "      <th>IS_SUCCESSFUL</th>\n",
       "      <th>APPLICATION_TYPE_T10</th>\n",
       "      <th>APPLICATION_TYPE_T12</th>\n",
       "      <th>APPLICATION_TYPE_T13</th>\n",
       "      <th>APPLICATION_TYPE_T14</th>\n",
       "      <th>APPLICATION_TYPE_T15</th>\n",
       "      <th>APPLICATION_TYPE_T17</th>\n",
       "      <th>APPLICATION_TYPE_T19</th>\n",
       "      <th>...</th>\n",
       "      <th>INCOME_AMT_1-9999</th>\n",
       "      <th>INCOME_AMT_10000-24999</th>\n",
       "      <th>INCOME_AMT_100000-499999</th>\n",
       "      <th>INCOME_AMT_10M-50M</th>\n",
       "      <th>INCOME_AMT_1M-5M</th>\n",
       "      <th>INCOME_AMT_25000-99999</th>\n",
       "      <th>INCOME_AMT_50M+</th>\n",
       "      <th>INCOME_AMT_5M-10M</th>\n",
       "      <th>SPECIAL_CONSIDERATIONS_N</th>\n",
       "      <th>SPECIAL_CONSIDERATIONS_Y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>5000</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>108590</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>5000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>6692</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>142590</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 117 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   STATUS  ASK_AMT  IS_SUCCESSFUL  APPLICATION_TYPE_T10  APPLICATION_TYPE_T12  \\\n",
       "0       1     5000              1                   1.0                   0.0   \n",
       "1       1   108590              1                   0.0                   0.0   \n",
       "2       1     5000              0                   0.0                   0.0   \n",
       "3       1     6692              1                   0.0                   0.0   \n",
       "4       1   142590              1                   0.0                   0.0   \n",
       "\n",
       "   APPLICATION_TYPE_T13  APPLICATION_TYPE_T14  APPLICATION_TYPE_T15  \\\n",
       "0                   0.0                   0.0                   0.0   \n",
       "1                   0.0                   0.0                   0.0   \n",
       "2                   0.0                   0.0                   0.0   \n",
       "3                   0.0                   0.0                   0.0   \n",
       "4                   0.0                   0.0                   0.0   \n",
       "\n",
       "   APPLICATION_TYPE_T17  APPLICATION_TYPE_T19  ...  INCOME_AMT_1-9999  \\\n",
       "0                   0.0                   0.0  ...                0.0   \n",
       "1                   0.0                   0.0  ...                1.0   \n",
       "2                   0.0                   0.0  ...                0.0   \n",
       "3                   0.0                   0.0  ...                0.0   \n",
       "4                   0.0                   0.0  ...                0.0   \n",
       "\n",
       "   INCOME_AMT_10000-24999  INCOME_AMT_100000-499999  INCOME_AMT_10M-50M  \\\n",
       "0                     0.0                       0.0                 0.0   \n",
       "1                     0.0                       0.0                 0.0   \n",
       "2                     0.0                       0.0                 0.0   \n",
       "3                     1.0                       0.0                 0.0   \n",
       "4                     0.0                       1.0                 0.0   \n",
       "\n",
       "   INCOME_AMT_1M-5M  INCOME_AMT_25000-99999  INCOME_AMT_50M+  \\\n",
       "0               0.0                     0.0              0.0   \n",
       "1               0.0                     0.0              0.0   \n",
       "2               0.0                     0.0              0.0   \n",
       "3               0.0                     0.0              0.0   \n",
       "4               0.0                     0.0              0.0   \n",
       "\n",
       "   INCOME_AMT_5M-10M  SPECIAL_CONSIDERATIONS_N  SPECIAL_CONSIDERATIONS_Y  \n",
       "0                0.0                       1.0                       0.0  \n",
       "1                0.0                       1.0                       0.0  \n",
       "2                0.0                       1.0                       0.0  \n",
       "3                0.0                       1.0                       0.0  \n",
       "4                0.0                       1.0                       0.0  \n",
       "\n",
       "[5 rows x 117 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add the numerical variables from the original DataFrame to the one-hot encoding DataFrame\n",
    "fully_encoded_df = pd.concat(\n",
    "    [\n",
    "        numerical_variables_df,\n",
    "        encoded_df\n",
    "    ],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Review the Dataframe\n",
    "fully_encoded_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "APPLICATION_TYPE          object\n",
       "AFFILIATION               object\n",
       "CLASSIFICATION            object\n",
       "USE_CASE                  object\n",
       "ORGANIZATION              object\n",
       "STATUS                     int64\n",
       "INCOME_AMT                object\n",
       "SPECIAL_CONSIDERATIONS    object\n",
       "ASK_AMT                    int64\n",
       "IS_SUCCESSFUL              int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "applicant_data_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "STATUS                        int64\n",
       "ASK_AMT                       int64\n",
       "IS_SUCCESSFUL                 int64\n",
       "APPLICATION_TYPE_T10        float64\n",
       "APPLICATION_TYPE_T12        float64\n",
       "                             ...   \n",
       "INCOME_AMT_25000-99999      float64\n",
       "INCOME_AMT_50M+             float64\n",
       "INCOME_AMT_5M-10M           float64\n",
       "SPECIAL_CONSIDERATIONS_N    float64\n",
       "SPECIAL_CONSIDERATIONS_Y    float64\n",
       "Length: 117, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fully_encoded_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Using the preprocessed data, create the features (`X`) and target (`y`) datasets. The target dataset should be defined by the  DataFrame column “IS_SUCCESSFUL”. The remaining columns should define the features dataset. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1\n",
       "1    1\n",
       "2    0\n",
       "3    1\n",
       "4    1\n",
       "Name: IS_SUCCESSFUL, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the target set y using the IS_SUCCESSFUL column\n",
    "y = fully_encoded_df[\"IS_SUCCESSFUL\"]\n",
    "\n",
    "# Display a sample of y\n",
    "y[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>STATUS</th>\n",
       "      <th>ASK_AMT</th>\n",
       "      <th>APPLICATION_TYPE_T10</th>\n",
       "      <th>APPLICATION_TYPE_T12</th>\n",
       "      <th>APPLICATION_TYPE_T13</th>\n",
       "      <th>APPLICATION_TYPE_T14</th>\n",
       "      <th>APPLICATION_TYPE_T15</th>\n",
       "      <th>APPLICATION_TYPE_T17</th>\n",
       "      <th>APPLICATION_TYPE_T19</th>\n",
       "      <th>APPLICATION_TYPE_T2</th>\n",
       "      <th>...</th>\n",
       "      <th>INCOME_AMT_1-9999</th>\n",
       "      <th>INCOME_AMT_10000-24999</th>\n",
       "      <th>INCOME_AMT_100000-499999</th>\n",
       "      <th>INCOME_AMT_10M-50M</th>\n",
       "      <th>INCOME_AMT_1M-5M</th>\n",
       "      <th>INCOME_AMT_25000-99999</th>\n",
       "      <th>INCOME_AMT_50M+</th>\n",
       "      <th>INCOME_AMT_5M-10M</th>\n",
       "      <th>SPECIAL_CONSIDERATIONS_N</th>\n",
       "      <th>SPECIAL_CONSIDERATIONS_Y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>5000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>108590</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>5000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>6692</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>142590</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 116 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   STATUS  ASK_AMT  APPLICATION_TYPE_T10  APPLICATION_TYPE_T12  \\\n",
       "0       1     5000                   1.0                   0.0   \n",
       "1       1   108590                   0.0                   0.0   \n",
       "2       1     5000                   0.0                   0.0   \n",
       "3       1     6692                   0.0                   0.0   \n",
       "4       1   142590                   0.0                   0.0   \n",
       "\n",
       "   APPLICATION_TYPE_T13  APPLICATION_TYPE_T14  APPLICATION_TYPE_T15  \\\n",
       "0                   0.0                   0.0                   0.0   \n",
       "1                   0.0                   0.0                   0.0   \n",
       "2                   0.0                   0.0                   0.0   \n",
       "3                   0.0                   0.0                   0.0   \n",
       "4                   0.0                   0.0                   0.0   \n",
       "\n",
       "   APPLICATION_TYPE_T17  APPLICATION_TYPE_T19  APPLICATION_TYPE_T2  ...  \\\n",
       "0                   0.0                   0.0                  0.0  ...   \n",
       "1                   0.0                   0.0                  0.0  ...   \n",
       "2                   0.0                   0.0                  0.0  ...   \n",
       "3                   0.0                   0.0                  0.0  ...   \n",
       "4                   0.0                   0.0                  0.0  ...   \n",
       "\n",
       "   INCOME_AMT_1-9999  INCOME_AMT_10000-24999  INCOME_AMT_100000-499999  \\\n",
       "0                0.0                     0.0                       0.0   \n",
       "1                1.0                     0.0                       0.0   \n",
       "2                0.0                     0.0                       0.0   \n",
       "3                0.0                     1.0                       0.0   \n",
       "4                0.0                     0.0                       1.0   \n",
       "\n",
       "   INCOME_AMT_10M-50M  INCOME_AMT_1M-5M  INCOME_AMT_25000-99999  \\\n",
       "0                 0.0               0.0                     0.0   \n",
       "1                 0.0               0.0                     0.0   \n",
       "2                 0.0               0.0                     0.0   \n",
       "3                 0.0               0.0                     0.0   \n",
       "4                 0.0               0.0                     0.0   \n",
       "\n",
       "   INCOME_AMT_50M+  INCOME_AMT_5M-10M  SPECIAL_CONSIDERATIONS_N  \\\n",
       "0              0.0                0.0                       1.0   \n",
       "1              0.0                0.0                       1.0   \n",
       "2              0.0                0.0                       1.0   \n",
       "3              0.0                0.0                       1.0   \n",
       "4              0.0                0.0                       1.0   \n",
       "\n",
       "   SPECIAL_CONSIDERATIONS_Y  \n",
       "0                       0.0  \n",
       "1                       0.0  \n",
       "2                       0.0  \n",
       "3                       0.0  \n",
       "4                       0.0  \n",
       "\n",
       "[5 rows x 116 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define features set X by selecting all columns but IS_SUCCESSFUL\n",
    "X = fully_encoded_df.drop(columns=[\"IS_SUCCESSFUL\"])\n",
    "\n",
    "\n",
    "# Review the features DataFrame\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Split the features and target sets into training and testing datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the preprocessed data into a training and testing dataset\n",
    "# Assign the function a random_state equal to 1\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Use scikit-learn's `StandardScaler` to scale the features data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a StandardScaler instance\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler to the features training dataset\n",
    "X_scaler = scaler.fit(X_train)\n",
    "\n",
    "# Fit the scaler to the features training dataset\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Compile and Evaluate a Binary Classification Model Using a Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Create a deep neural network by assigning the number of input features, the number of layers, and the number of neurons on each layer using Tensorflow’s Keras.\n",
    "\n",
    "> **Hint** You can start with a two-layer deep neural network model that uses the `relu` activation function for both layers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "116"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the the number of inputs (features) to the model\n",
    "number_input_features = len(X_train.iloc[0])\n",
    "# Review the number of features\n",
    "number_input_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the number of neurons in the output layer\n",
    "number_output_neurons = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the number of hidden nodes for the first hidden layer\n",
    "hidden_nodes_layer1 =  (number_input_features + number_output_neurons) // 2\n",
    "# Review the number hidden nodes in the first layer\n",
    "hidden_nodes_layer1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the number of hidden nodes for the second hidden layer\n",
    "hidden_nodes_layer2 = (hidden_nodes_layer1 + number_output_neurons) // 2\n",
    "# Review the number hidden nodes in the second layer\n",
    "hidden_nodes_layer2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Sequential model instance\n",
    "nn = Sequential() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the first hidden layer\n",
    "nn.add(Dense(units=hidden_nodes_layer1, input_dim=number_input_features, activation=\"relu\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the second hidden layer\n",
    "nn.add(Dense(units=hidden_nodes_layer2, activation=\"relu\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the output layer to the model specifying the number of output neurons and activation function\n",
    "nn.add(Dense(units=number_output_neurons, activation=\"sigmoid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 58)                6786      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 29)                1711      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 30        \n",
      "=================================================================\n",
      "Total params: 8,527\n",
      "Trainable params: 8,527\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Display the Sequential model summary\n",
    "nn.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Compile and fit the model using the `binary_crossentropy` loss function, the `adam` optimizer, and the `accuracy` evaluation metric.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the Sequential model\n",
    "nn.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "804/804 [==============================] - 1s 750us/step - loss: 0.6045 - accuracy: 0.7048\n",
      "Epoch 2/50\n",
      "804/804 [==============================] - 1s 730us/step - loss: 0.5570 - accuracy: 0.7260\n",
      "Epoch 3/50\n",
      "804/804 [==============================] - 1s 775us/step - loss: 0.5508 - accuracy: 0.7300\n",
      "Epoch 4/50\n",
      "804/804 [==============================] - 1s 740us/step - loss: 0.5534 - accuracy: 0.7259\n",
      "Epoch 5/50\n",
      "804/804 [==============================] - 1s 729us/step - loss: 0.5426 - accuracy: 0.7392\n",
      "Epoch 6/50\n",
      "804/804 [==============================] - 1s 765us/step - loss: 0.5429 - accuracy: 0.7364\n",
      "Epoch 7/50\n",
      "804/804 [==============================] - 1s 737us/step - loss: 0.5387 - accuracy: 0.7379\n",
      "Epoch 8/50\n",
      "804/804 [==============================] - 1s 777us/step - loss: 0.5402 - accuracy: 0.7370\n",
      "Epoch 9/50\n",
      "804/804 [==============================] - 1s 805us/step - loss: 0.5379 - accuracy: 0.7367\n",
      "Epoch 10/50\n",
      "804/804 [==============================] - 1s 756us/step - loss: 0.5382 - accuracy: 0.7369\n",
      "Epoch 11/50\n",
      "804/804 [==============================] - 1s 741us/step - loss: 0.5394 - accuracy: 0.7385\n",
      "Epoch 12/50\n",
      "804/804 [==============================] - 1s 877us/step - loss: 0.5423 - accuracy: 0.7348\n",
      "Epoch 13/50\n",
      "804/804 [==============================] - 1s 840us/step - loss: 0.5366 - accuracy: 0.7356\n",
      "Epoch 14/50\n",
      "804/804 [==============================] - 1s 732us/step - loss: 0.5393 - accuracy: 0.7374\n",
      "Epoch 15/50\n",
      "804/804 [==============================] - 1s 776us/step - loss: 0.5352 - accuracy: 0.7394\n",
      "Epoch 16/50\n",
      "804/804 [==============================] - 1s 783us/step - loss: 0.5347 - accuracy: 0.7441\n",
      "Epoch 17/50\n",
      "804/804 [==============================] - 1s 761us/step - loss: 0.5319 - accuracy: 0.7391\n",
      "Epoch 18/50\n",
      "804/804 [==============================] - 1s 798us/step - loss: 0.5321 - accuracy: 0.7453\n",
      "Epoch 19/50\n",
      "804/804 [==============================] - 1s 774us/step - loss: 0.5334 - accuracy: 0.7406\n",
      "Epoch 20/50\n",
      "804/804 [==============================] - 1s 756us/step - loss: 0.5352 - accuracy: 0.7420\n",
      "Epoch 21/50\n",
      "804/804 [==============================] - 1s 726us/step - loss: 0.5347 - accuracy: 0.7402\n",
      "Epoch 22/50\n",
      "804/804 [==============================] - 1s 796us/step - loss: 0.5398 - accuracy: 0.7348\n",
      "Epoch 23/50\n",
      "804/804 [==============================] - 1s 752us/step - loss: 0.5338 - accuracy: 0.7410\n",
      "Epoch 24/50\n",
      "804/804 [==============================] - 1s 734us/step - loss: 0.5333 - accuracy: 0.7394\n",
      "Epoch 25/50\n",
      "804/804 [==============================] - 1s 752us/step - loss: 0.5347 - accuracy: 0.7401\n",
      "Epoch 26/50\n",
      "804/804 [==============================] - 1s 727us/step - loss: 0.5302 - accuracy: 0.7423\n",
      "Epoch 27/50\n",
      "804/804 [==============================] - 1s 755us/step - loss: 0.5334 - accuracy: 0.7402\n",
      "Epoch 28/50\n",
      "804/804 [==============================] - 1s 746us/step - loss: 0.5381 - accuracy: 0.7366\n",
      "Epoch 29/50\n",
      "804/804 [==============================] - 1s 730us/step - loss: 0.5364 - accuracy: 0.7375\n",
      "Epoch 30/50\n",
      "804/804 [==============================] - 1s 735us/step - loss: 0.5355 - accuracy: 0.7410\n",
      "Epoch 31/50\n",
      "804/804 [==============================] - 1s 757us/step - loss: 0.5355 - accuracy: 0.7383\n",
      "Epoch 32/50\n",
      "804/804 [==============================] - 1s 805us/step - loss: 0.5303 - accuracy: 0.7426\n",
      "Epoch 33/50\n",
      "804/804 [==============================] - 1s 722us/step - loss: 0.5352 - accuracy: 0.7391\n",
      "Epoch 34/50\n",
      "804/804 [==============================] - 1s 727us/step - loss: 0.5385 - accuracy: 0.7365\n",
      "Epoch 35/50\n",
      "804/804 [==============================] - 1s 774us/step - loss: 0.5372 - accuracy: 0.7370\n",
      "Epoch 36/50\n",
      "804/804 [==============================] - 1s 760us/step - loss: 0.5305 - accuracy: 0.7434\n",
      "Epoch 37/50\n",
      "804/804 [==============================] - 1s 759us/step - loss: 0.5369 - accuracy: 0.7390\n",
      "Epoch 38/50\n",
      "804/804 [==============================] - 1s 759us/step - loss: 0.5315 - accuracy: 0.7428\n",
      "Epoch 39/50\n",
      "804/804 [==============================] - 1s 790us/step - loss: 0.5342 - accuracy: 0.7378\n",
      "Epoch 40/50\n",
      "804/804 [==============================] - 1s 779us/step - loss: 0.5269 - accuracy: 0.7453\n",
      "Epoch 41/50\n",
      "804/804 [==============================] - 1s 806us/step - loss: 0.5298 - accuracy: 0.7439\n",
      "Epoch 42/50\n",
      "804/804 [==============================] - 1s 769us/step - loss: 0.5324 - accuracy: 0.7402\n",
      "Epoch 43/50\n",
      "804/804 [==============================] - 1s 712us/step - loss: 0.5287 - accuracy: 0.7415\n",
      "Epoch 44/50\n",
      "804/804 [==============================] - 1s 732us/step - loss: 0.5301 - accuracy: 0.7399\n",
      "Epoch 45/50\n",
      "804/804 [==============================] - 1s 761us/step - loss: 0.5328 - accuracy: 0.7400\n",
      "Epoch 46/50\n",
      "804/804 [==============================] - 1s 732us/step - loss: 0.5340 - accuracy: 0.7367\n",
      "Epoch 47/50\n",
      "804/804 [==============================] - 1s 721us/step - loss: 0.5329 - accuracy: 0.7402\n",
      "Epoch 48/50\n",
      "804/804 [==============================] - 1s 772us/step - loss: 0.5365 - accuracy: 0.7391\n",
      "Epoch 49/50\n",
      "804/804 [==============================] - 1s 784us/step - loss: 0.5276 - accuracy: 0.7457\n",
      "Epoch 50/50\n",
      "804/804 [==============================] - 1s 761us/step - loss: 0.5337 - accuracy: 0.7372\n"
     ]
    }
   ],
   "source": [
    "# Fit the model using 50 epochs and the training data\n",
    "fit_model_00 = nn.fit(X_train_scaled, y_train, epochs=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Evaluate the model using the test data to determine the model’s loss and accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "268/268 [==============================] - 0s 577us/step - loss: 0.5549 - accuracy: 0.7296\n",
      "ORIGINAL Model - Loss: 0.5549191236495972, Accuracy: 0.7295626997947693\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model loss and accuracy metrics using the evaluate method and the test data\n",
    "model_loss_00, model_accuracy_00 = nn.evaluate(X_test_scaled,y_test,verbose=1)\n",
    "# Display the model loss and accuracy results\n",
    "print(f\"ORIGINAL Model - Loss: {model_loss_00}, Accuracy: {model_accuracy_00}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Save and export your model to an HDF5 file, and name the file `AlphabetSoup.h5`. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the model's file path\n",
    "file_path = Path(\"./Resources/AlphabetSoup_00.h5\")\n",
    "# Export your model to a HDF5 file\n",
    "nn.save(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Optimize the neural network model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Define at least three new deep neural network models (resulting in the original plus 3 optimization attempts). With each, try to improve on your first model’s predictive accuracy.\n",
    "\n",
    "> **Rewind** Recall that perfect accuracy has a value of 1, so accuracy improves as its value moves closer to 1. To optimize your model for a predictive accuracy as close to 1 as possible, you can use any or all of the following techniques:\n",
    ">\n",
    "> * Adjust the input data by dropping different features columns to ensure that no variables or outliers confuse the model.\n",
    ">\n",
    "> * Add more neurons (nodes) to a hidden layer.\n",
    ">\n",
    "> * Add more hidden layers.\n",
    ">\n",
    "> * Use different activation functions for the hidden layers.\n",
    ">\n",
    "> * Add to or reduce the number of epochs in the training regimen.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>APPLICATION_TYPE</th>\n",
       "      <th>AFFILIATION</th>\n",
       "      <th>CLASSIFICATION</th>\n",
       "      <th>USE_CASE</th>\n",
       "      <th>ORGANIZATION</th>\n",
       "      <th>STATUS</th>\n",
       "      <th>INCOME_AMT</th>\n",
       "      <th>ASK_AMT</th>\n",
       "      <th>IS_SUCCESSFUL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>T10</td>\n",
       "      <td>Independent</td>\n",
       "      <td>C1000</td>\n",
       "      <td>ProductDev</td>\n",
       "      <td>Association</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>T3</td>\n",
       "      <td>Independent</td>\n",
       "      <td>C2000</td>\n",
       "      <td>Preservation</td>\n",
       "      <td>Co-operative</td>\n",
       "      <td>1</td>\n",
       "      <td>1-9999</td>\n",
       "      <td>108590</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>T5</td>\n",
       "      <td>CompanySponsored</td>\n",
       "      <td>C3000</td>\n",
       "      <td>ProductDev</td>\n",
       "      <td>Association</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>T3</td>\n",
       "      <td>CompanySponsored</td>\n",
       "      <td>C2000</td>\n",
       "      <td>Preservation</td>\n",
       "      <td>Trust</td>\n",
       "      <td>1</td>\n",
       "      <td>10000-24999</td>\n",
       "      <td>6692</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>T3</td>\n",
       "      <td>Independent</td>\n",
       "      <td>C1000</td>\n",
       "      <td>Heathcare</td>\n",
       "      <td>Trust</td>\n",
       "      <td>1</td>\n",
       "      <td>100000-499999</td>\n",
       "      <td>142590</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>T3</td>\n",
       "      <td>Independent</td>\n",
       "      <td>C1200</td>\n",
       "      <td>Preservation</td>\n",
       "      <td>Trust</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>T3</td>\n",
       "      <td>Independent</td>\n",
       "      <td>C1000</td>\n",
       "      <td>Preservation</td>\n",
       "      <td>Trust</td>\n",
       "      <td>1</td>\n",
       "      <td>100000-499999</td>\n",
       "      <td>31452</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>T3</td>\n",
       "      <td>Independent</td>\n",
       "      <td>C2000</td>\n",
       "      <td>Preservation</td>\n",
       "      <td>Trust</td>\n",
       "      <td>1</td>\n",
       "      <td>10M-50M</td>\n",
       "      <td>7508025</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>T7</td>\n",
       "      <td>Independent</td>\n",
       "      <td>C1000</td>\n",
       "      <td>ProductDev</td>\n",
       "      <td>Trust</td>\n",
       "      <td>1</td>\n",
       "      <td>1-9999</td>\n",
       "      <td>94389</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>T5</td>\n",
       "      <td>CompanySponsored</td>\n",
       "      <td>C3000</td>\n",
       "      <td>ProductDev</td>\n",
       "      <td>Association</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>T3</td>\n",
       "      <td>Independent</td>\n",
       "      <td>C1200</td>\n",
       "      <td>Preservation</td>\n",
       "      <td>Trust</td>\n",
       "      <td>1</td>\n",
       "      <td>25000-99999</td>\n",
       "      <td>69656</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>T3</td>\n",
       "      <td>Independent</td>\n",
       "      <td>C2000</td>\n",
       "      <td>Preservation</td>\n",
       "      <td>Trust</td>\n",
       "      <td>1</td>\n",
       "      <td>100000-499999</td>\n",
       "      <td>165593</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>T3</td>\n",
       "      <td>CompanySponsored</td>\n",
       "      <td>C1200</td>\n",
       "      <td>Preservation</td>\n",
       "      <td>Association</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>T3</td>\n",
       "      <td>Independent</td>\n",
       "      <td>C2700</td>\n",
       "      <td>Preservation</td>\n",
       "      <td>Trust</td>\n",
       "      <td>1</td>\n",
       "      <td>25000-99999</td>\n",
       "      <td>5301</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>T3</td>\n",
       "      <td>Independent</td>\n",
       "      <td>C1000</td>\n",
       "      <td>Preservation</td>\n",
       "      <td>Trust</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   APPLICATION_TYPE       AFFILIATION CLASSIFICATION      USE_CASE  \\\n",
       "0               T10       Independent          C1000    ProductDev   \n",
       "1                T3       Independent          C2000  Preservation   \n",
       "2                T5  CompanySponsored          C3000    ProductDev   \n",
       "3                T3  CompanySponsored          C2000  Preservation   \n",
       "4                T3       Independent          C1000     Heathcare   \n",
       "5                T3       Independent          C1200  Preservation   \n",
       "6                T3       Independent          C1000  Preservation   \n",
       "7                T3       Independent          C2000  Preservation   \n",
       "8                T7       Independent          C1000    ProductDev   \n",
       "9                T5  CompanySponsored          C3000    ProductDev   \n",
       "10               T3       Independent          C1200  Preservation   \n",
       "11               T3       Independent          C2000  Preservation   \n",
       "12               T3  CompanySponsored          C1200  Preservation   \n",
       "13               T3       Independent          C2700  Preservation   \n",
       "14               T3       Independent          C1000  Preservation   \n",
       "\n",
       "    ORGANIZATION  STATUS     INCOME_AMT  ASK_AMT  IS_SUCCESSFUL  \n",
       "0    Association       1              0     5000              1  \n",
       "1   Co-operative       1         1-9999   108590              1  \n",
       "2    Association       1              0     5000              0  \n",
       "3          Trust       1    10000-24999     6692              1  \n",
       "4          Trust       1  100000-499999   142590              1  \n",
       "5          Trust       1              0     5000              1  \n",
       "6          Trust       1  100000-499999    31452              1  \n",
       "7          Trust       1        10M-50M  7508025              1  \n",
       "8          Trust       1         1-9999    94389              1  \n",
       "9    Association       1              0     5000              0  \n",
       "10         Trust       1    25000-99999    69656              0  \n",
       "11         Trust       1  100000-499999   165593              0  \n",
       "12   Association       1              0     5000              1  \n",
       "13         Trust       1    25000-99999     5301              1  \n",
       "14         Trust       1              0     5000              1  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop the other columns from the DataFrame as needed\n",
    "applicant_data_df_cropped = applicant_data_df.drop(columns = [\"SPECIAL_CONSIDERATIONS\"])\n",
    "\n",
    "\n",
    "#.drop(columns = [\"AFFILIATION\", \"APPLICATION_TYPE\", \"CLASSIFICATION\", \"ORGANIZATION\", \"INCOME_AMT\", \"ASK_AMT\", \"STATUS\", \"USE_CASE\", \"SPECIAL_CONSIDERATIONS\"])\n",
    "\n",
    "# # Review the DataFrame\n",
    "applicant_data_df_cropped[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['APPLICATION_TYPE',\n",
       " 'AFFILIATION',\n",
       " 'CLASSIFICATION',\n",
       " 'USE_CASE',\n",
       " 'ORGANIZATION',\n",
       " 'INCOME_AMT']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a list of categorical variables \n",
    "categorical_variables_cropped = list(applicant_data_df_cropped.dtypes[applicant_data_df_cropped.dtypes == \"object\"].index)\n",
    "\n",
    "# Display the categorical variables list\n",
    "categorical_variables_cropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a OneHotEncoder instance\n",
    "enc = OneHotEncoder(sparse=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the categorcal variables using OneHotEncoder\n",
    "encoded_data = enc.fit_transform(applicant_data_df_cropped[categorical_variables_cropped])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame with the encoded variables\n",
    "encoded_df = pd.DataFrame(\n",
    "    encoded_data,\n",
    "    columns = enc.get_feature_names(categorical_variables_cropped)\n",
    ")\n",
    "\n",
    "# # Review the DataFrame\n",
    "# encoded_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_variables_df = applicant_data_df_cropped.drop(columns = categorical_variables_cropped\n",
    "                                                       )\n",
    "\n",
    "# # Review the DataFrame\n",
    "# numerical_variables_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the numerical variables from the original DataFrame to the one-hot encoding DataFrame\n",
    "fully_encoded_df = pd.concat(\n",
    "    [\n",
    "        numerical_variables_df,\n",
    "        encoded_df\n",
    "    ],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# # Review the Dataframe\n",
    "# fully_encoded_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "APPLICATION_TYPE    object\n",
       "AFFILIATION         object\n",
       "CLASSIFICATION      object\n",
       "USE_CASE            object\n",
       "ORGANIZATION        object\n",
       "STATUS               int64\n",
       "INCOME_AMT          object\n",
       "ASK_AMT              int64\n",
       "IS_SUCCESSFUL        int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "applicant_data_df_cropped.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "STATUS                      int64\n",
       "ASK_AMT                     int64\n",
       "IS_SUCCESSFUL               int64\n",
       "APPLICATION_TYPE_T10      float64\n",
       "APPLICATION_TYPE_T12      float64\n",
       "                           ...   \n",
       "INCOME_AMT_10M-50M        float64\n",
       "INCOME_AMT_1M-5M          float64\n",
       "INCOME_AMT_25000-99999    float64\n",
       "INCOME_AMT_50M+           float64\n",
       "INCOME_AMT_5M-10M         float64\n",
       "Length: 115, dtype: object"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fully_encoded_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the target set y using the IS_SUCCESSFUL column\n",
    "y = fully_encoded_df[\"IS_SUCCESSFUL\"]\n",
    "\n",
    "# # Display a sample of y\n",
    "# y[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features set X by selecting all columns but IS_SUCCESSFUL\n",
    "X = fully_encoded_df.drop(columns=[\"IS_SUCCESSFUL\"])\n",
    "\n",
    "\n",
    "# # Review the features DataFrame\n",
    "# X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the preprocessed data into a training and testing dataset\n",
    "# Assign the function a random_state equal to 1\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a StandardScaler instance\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler to the features training dataset\n",
    "X_scaler = scaler.fit(X_train)\n",
    "\n",
    "# Fit the scaler to the features training dataset\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alternative Model 41+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "114"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the the number of inputs (features) to the model\n",
    "number_input_features = len(X_train.iloc[0])\n",
    "# Review the number of features\n",
    "number_input_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the number of neurons in the output layer\n",
    "number_output_neurons_A41 = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "114"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the number of hidden nodes for the first hidden layer\n",
    "hidden_nodes_layer1_A41 = number_input_features\n",
    "# Review the number of hidden nodes in the first layer\n",
    "hidden_nodes_layer1_A41"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "114"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the number of hidden nodes for the second hidden layer\n",
    "hidden_nodes_layer2_A41 = hidden_nodes_layer1_A41\n",
    "\n",
    "# Review the number of hidden nodes in the second layer\n",
    "hidden_nodes_layer2_A41"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "114"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the number of hidden nodes for the third hidden layer\n",
    "hidden_nodes_layer3_A41 = hidden_nodes_layer2_A41\n",
    "# Review the number of hidden nodes in the third layer\n",
    "hidden_nodes_layer3_A41"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "114"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the number of hidden nodes for the fourth hidden layer\n",
    "hidden_nodes_layer4_A41 = hidden_nodes_layer3_A41\n",
    "# Review the number of hidden nodes in the fourth layer\n",
    "hidden_nodes_layer4_A41"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "114"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the number of hidden nodes for the fifth hidden layer\n",
    "hidden_nodes_layer5_A41 = hidden_nodes_layer4_A41\n",
    "# Review the number of hidden nodes in the fifth layer\n",
    "hidden_nodes_layer5_A41"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the number of hidden nodes for the Sixth hidden layer\n",
    "hidden_nodes_layer6_A41 = (hidden_nodes_layer5_A41 + number_output_neurons_A41) // 2\n",
    "# Review the number of hidden nodes in the sixth layer\n",
    "hidden_nodes_layer6_A41"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_3 (Dense)              (None, 114)               13110     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 114)               13110     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 114)               13110     \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 114)               13110     \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 114)               13110     \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 57)                6555      \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 1)                 58        \n",
      "=================================================================\n",
      "Total params: 72,163\n",
      "Trainable params: 72,163\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Create and Display the Sequential Model Instance \n",
    "# for Model A41\n",
    "nn_A41 = Sequential() \n",
    "\n",
    "# Add the first hidden layer\n",
    "nn_A41.add(Dense(units=hidden_nodes_layer1_A41, input_dim=number_input_features, activation=\"relu\"))\n",
    "# Add the second hidden layer\n",
    "nn_A41.add(Dense(units=hidden_nodes_layer2_A41, input_dim=number_input_features, activation=\"relu\"))\n",
    "# Add the third hidden layer\n",
    "nn_A41.add(Dense(units=hidden_nodes_layer3_A41, input_dim=number_input_features, activation=\"relu\"))\n",
    "# Add the fourth hidden layer\n",
    "nn_A41.add(Dense(units=hidden_nodes_layer4_A41, input_dim=number_input_features, activation=\"relu\"))\n",
    "# Add the fifth hidden layer\n",
    "nn_A41.add(Dense(units=hidden_nodes_layer5_A41, input_dim=number_input_features, activation=\"relu\"))\n",
    "# Add the sixth hidden layer\n",
    "nn_A41.add(Dense(units=hidden_nodes_layer6_A41, activation=\"relu\"))\n",
    "\n",
    "# Add the output layer to the model specifying the number of output neurons and activation function\n",
    "nn_A41.add(Dense(units=number_output_neurons_A41, activation=\"sigmoid\"))\n",
    "\n",
    "# Display the Sequential model summary\n",
    "nn_A41.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the Sequential model\n",
    "nn_A41.compile(loss=\"mse\", optimizer=\"adam\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.2031 - accuracy: 0.6968 - val_loss: 0.1842 - val_accuracy: 0.7367\n",
      "Epoch 2/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1864 - accuracy: 0.7281 - val_loss: 0.1829 - val_accuracy: 0.7387\n",
      "Epoch 3/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1846 - accuracy: 0.7345 - val_loss: 0.1833 - val_accuracy: 0.7396\n",
      "Epoch 4/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1848 - accuracy: 0.7313 - val_loss: 0.1830 - val_accuracy: 0.7334\n",
      "Epoch 5/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1860 - accuracy: 0.7274 - val_loss: 0.1826 - val_accuracy: 0.7392\n",
      "Epoch 6/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1813 - accuracy: 0.7362 - val_loss: 0.1831 - val_accuracy: 0.7400\n",
      "Epoch 7/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1844 - accuracy: 0.7312 - val_loss: 0.1837 - val_accuracy: 0.7391\n",
      "Epoch 8/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1798 - accuracy: 0.7400 - val_loss: 0.1835 - val_accuracy: 0.7384\n",
      "Epoch 9/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1852 - accuracy: 0.7283 - val_loss: 0.1823 - val_accuracy: 0.7401\n",
      "Epoch 10/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1855 - accuracy: 0.7291 - val_loss: 0.1833 - val_accuracy: 0.7378\n",
      "Epoch 11/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1797 - accuracy: 0.7392 - val_loss: 0.1849 - val_accuracy: 0.7350\n",
      "Epoch 12/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1808 - accuracy: 0.7384 - val_loss: 0.1843 - val_accuracy: 0.7371\n",
      "Epoch 13/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1821 - accuracy: 0.7368 - val_loss: 0.1866 - val_accuracy: 0.7293\n",
      "Epoch 14/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1828 - accuracy: 0.7341 - val_loss: 0.1838 - val_accuracy: 0.7361\n",
      "Epoch 15/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1827 - accuracy: 0.7334 - val_loss: 0.1837 - val_accuracy: 0.7353\n",
      "Epoch 16/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1777 - accuracy: 0.7452 - val_loss: 0.1843 - val_accuracy: 0.7345\n",
      "Epoch 17/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1797 - accuracy: 0.7382 - val_loss: 0.1830 - val_accuracy: 0.7378\n",
      "Epoch 18/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1792 - accuracy: 0.7398 - val_loss: 0.1852 - val_accuracy: 0.7379\n",
      "Epoch 19/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1794 - accuracy: 0.7414 - val_loss: 0.1837 - val_accuracy: 0.7356\n",
      "Epoch 20/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1816 - accuracy: 0.7343 - val_loss: 0.1838 - val_accuracy: 0.7375\n",
      "Epoch 21/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1808 - accuracy: 0.7373 - val_loss: 0.1848 - val_accuracy: 0.7336\n",
      "Epoch 22/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1809 - accuracy: 0.7360 - val_loss: 0.1833 - val_accuracy: 0.7375\n",
      "Epoch 23/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1800 - accuracy: 0.7388 - val_loss: 0.1851 - val_accuracy: 0.7353\n",
      "Epoch 24/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1815 - accuracy: 0.7357 - val_loss: 0.1855 - val_accuracy: 0.7317\n",
      "Epoch 25/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1813 - accuracy: 0.7350 - val_loss: 0.1851 - val_accuracy: 0.7372\n",
      "Epoch 26/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1787 - accuracy: 0.7416 - val_loss: 0.1849 - val_accuracy: 0.7370\n",
      "Epoch 27/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1767 - accuracy: 0.7434 - val_loss: 0.1833 - val_accuracy: 0.7388\n",
      "Epoch 28/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1785 - accuracy: 0.7380 - val_loss: 0.1838 - val_accuracy: 0.7388\n",
      "Epoch 29/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1795 - accuracy: 0.7380 - val_loss: 0.1843 - val_accuracy: 0.7362\n",
      "Epoch 30/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1813 - accuracy: 0.7323 - val_loss: 0.1840 - val_accuracy: 0.7379\n",
      "Epoch 31/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1798 - accuracy: 0.7380 - val_loss: 0.1834 - val_accuracy: 0.7381\n",
      "Epoch 32/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1769 - accuracy: 0.7415 - val_loss: 0.1839 - val_accuracy: 0.7376\n",
      "Epoch 33/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1776 - accuracy: 0.7418 - val_loss: 0.1840 - val_accuracy: 0.7374\n",
      "Epoch 34/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1770 - accuracy: 0.7449 - val_loss: 0.1835 - val_accuracy: 0.7376\n",
      "Epoch 35/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1790 - accuracy: 0.7375 - val_loss: 0.1847 - val_accuracy: 0.7376\n",
      "Epoch 36/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1784 - accuracy: 0.7399 - val_loss: 0.1842 - val_accuracy: 0.7374\n",
      "Epoch 37/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1772 - accuracy: 0.7427 - val_loss: 0.1838 - val_accuracy: 0.7385\n",
      "Epoch 38/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1782 - accuracy: 0.7413 - val_loss: 0.1851 - val_accuracy: 0.7376\n",
      "Epoch 39/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1775 - accuracy: 0.7410 - val_loss: 0.1843 - val_accuracy: 0.7383\n",
      "Epoch 40/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1787 - accuracy: 0.7379 - val_loss: 0.1861 - val_accuracy: 0.7366\n",
      "Epoch 41/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1780 - accuracy: 0.7432 - val_loss: 0.1840 - val_accuracy: 0.7384\n",
      "Epoch 42/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1778 - accuracy: 0.7400 - val_loss: 0.1841 - val_accuracy: 0.7376\n",
      "Epoch 43/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1789 - accuracy: 0.7362 - val_loss: 0.1851 - val_accuracy: 0.7379\n",
      "Epoch 44/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1782 - accuracy: 0.7399 - val_loss: 0.1848 - val_accuracy: 0.7381\n",
      "Epoch 45/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1773 - accuracy: 0.7406 - val_loss: 0.1845 - val_accuracy: 0.7375\n",
      "Epoch 46/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1769 - accuracy: 0.7405 - val_loss: 0.1861 - val_accuracy: 0.7368\n",
      "Epoch 47/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1809 - accuracy: 0.7343 - val_loss: 0.1842 - val_accuracy: 0.7372\n",
      "Epoch 48/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1779 - accuracy: 0.7417 - val_loss: 0.1852 - val_accuracy: 0.7388\n",
      "Epoch 49/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1778 - accuracy: 0.7404 - val_loss: 0.1852 - val_accuracy: 0.7372\n",
      "Epoch 50/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1746 - accuracy: 0.7465 - val_loss: 0.1862 - val_accuracy: 0.7378\n",
      "Epoch 51/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1767 - accuracy: 0.7421 - val_loss: 0.1846 - val_accuracy: 0.7380\n",
      "Epoch 52/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1743 - accuracy: 0.7474 - val_loss: 0.1847 - val_accuracy: 0.7359\n",
      "Epoch 53/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1768 - accuracy: 0.7396 - val_loss: 0.1857 - val_accuracy: 0.7376\n",
      "Epoch 54/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1771 - accuracy: 0.7422 - val_loss: 0.1867 - val_accuracy: 0.7309\n",
      "Epoch 55/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1788 - accuracy: 0.7387 - val_loss: 0.1850 - val_accuracy: 0.7383\n",
      "Epoch 56/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1759 - accuracy: 0.7463 - val_loss: 0.1849 - val_accuracy: 0.7371\n",
      "Epoch 57/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1783 - accuracy: 0.7399 - val_loss: 0.1851 - val_accuracy: 0.7374\n",
      "Epoch 58/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1782 - accuracy: 0.7405 - val_loss: 0.1850 - val_accuracy: 0.7372\n",
      "Epoch 59/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1784 - accuracy: 0.7445 - val_loss: 0.1857 - val_accuracy: 0.7375\n",
      "Epoch 60/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1776 - accuracy: 0.7410 - val_loss: 0.1850 - val_accuracy: 0.7380\n",
      "Epoch 61/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1780 - accuracy: 0.7394 - val_loss: 0.1850 - val_accuracy: 0.7381\n",
      "Epoch 62/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1788 - accuracy: 0.7375 - val_loss: 0.1852 - val_accuracy: 0.7376\n",
      "Epoch 63/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1793 - accuracy: 0.7376 - val_loss: 0.1855 - val_accuracy: 0.7376\n",
      "Epoch 64/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1746 - accuracy: 0.7479 - val_loss: 0.1850 - val_accuracy: 0.7381\n",
      "Epoch 65/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1768 - accuracy: 0.7404 - val_loss: 0.1847 - val_accuracy: 0.7384\n",
      "Epoch 66/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1745 - accuracy: 0.7471 - val_loss: 0.1858 - val_accuracy: 0.7381\n",
      "Epoch 67/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1764 - accuracy: 0.7424 - val_loss: 0.1849 - val_accuracy: 0.7378\n",
      "Epoch 68/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1792 - accuracy: 0.7386 - val_loss: 0.1856 - val_accuracy: 0.7365\n",
      "Epoch 69/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1786 - accuracy: 0.7382 - val_loss: 0.1858 - val_accuracy: 0.7379\n",
      "Epoch 70/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1756 - accuracy: 0.7415 - val_loss: 0.1870 - val_accuracy: 0.7361\n",
      "Epoch 71/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1782 - accuracy: 0.7396 - val_loss: 0.1841 - val_accuracy: 0.7383\n",
      "Epoch 72/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1765 - accuracy: 0.7418 - val_loss: 0.1848 - val_accuracy: 0.7340\n",
      "Epoch 73/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1755 - accuracy: 0.7443 - val_loss: 0.1847 - val_accuracy: 0.7372\n",
      "Epoch 74/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1776 - accuracy: 0.7401 - val_loss: 0.1855 - val_accuracy: 0.7365\n",
      "Epoch 75/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1755 - accuracy: 0.7450 - val_loss: 0.1847 - val_accuracy: 0.7374\n",
      "Epoch 76/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1782 - accuracy: 0.7364 - val_loss: 0.1855 - val_accuracy: 0.7366\n",
      "Epoch 77/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1778 - accuracy: 0.7376 - val_loss: 0.1858 - val_accuracy: 0.7365\n",
      "Epoch 78/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1755 - accuracy: 0.7471 - val_loss: 0.1860 - val_accuracy: 0.7350\n",
      "Epoch 79/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1781 - accuracy: 0.7388 - val_loss: 0.1855 - val_accuracy: 0.7353\n",
      "Epoch 80/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1765 - accuracy: 0.7416 - val_loss: 0.1852 - val_accuracy: 0.7353\n",
      "Epoch 81/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1732 - accuracy: 0.7501 - val_loss: 0.1857 - val_accuracy: 0.7368\n",
      "Epoch 82/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1776 - accuracy: 0.7410 - val_loss: 0.1859 - val_accuracy: 0.7334\n",
      "Epoch 83/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1777 - accuracy: 0.7422 - val_loss: 0.1853 - val_accuracy: 0.7371\n",
      "Epoch 84/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1779 - accuracy: 0.7406 - val_loss: 0.1837 - val_accuracy: 0.7368\n",
      "Epoch 85/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1773 - accuracy: 0.7411 - val_loss: 0.1832 - val_accuracy: 0.7384\n",
      "Epoch 86/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1782 - accuracy: 0.7398 - val_loss: 0.1834 - val_accuracy: 0.7366\n",
      "Epoch 87/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1748 - accuracy: 0.7451 - val_loss: 0.1854 - val_accuracy: 0.7374\n",
      "Epoch 88/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1776 - accuracy: 0.7384 - val_loss: 0.1847 - val_accuracy: 0.7372\n",
      "Epoch 89/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1756 - accuracy: 0.7404 - val_loss: 0.1848 - val_accuracy: 0.7374\n",
      "Epoch 90/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1777 - accuracy: 0.7418 - val_loss: 0.1850 - val_accuracy: 0.7366\n",
      "Epoch 91/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1773 - accuracy: 0.7388 - val_loss: 0.1901 - val_accuracy: 0.7317\n",
      "Epoch 92/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1802 - accuracy: 0.7356 - val_loss: 0.1880 - val_accuracy: 0.7353\n",
      "Epoch 93/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1798 - accuracy: 0.7364 - val_loss: 0.1838 - val_accuracy: 0.7381\n",
      "Epoch 94/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1759 - accuracy: 0.7415 - val_loss: 0.1843 - val_accuracy: 0.7368\n",
      "Epoch 95/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1760 - accuracy: 0.7431 - val_loss: 0.1847 - val_accuracy: 0.7385\n",
      "Epoch 96/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1787 - accuracy: 0.7365 - val_loss: 0.1841 - val_accuracy: 0.7391\n",
      "Epoch 97/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1773 - accuracy: 0.7398 - val_loss: 0.1841 - val_accuracy: 0.7388\n",
      "Epoch 98/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1755 - accuracy: 0.7440 - val_loss: 0.1837 - val_accuracy: 0.7392\n",
      "Epoch 99/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1768 - accuracy: 0.7402 - val_loss: 0.1834 - val_accuracy: 0.7388\n",
      "Epoch 100/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1750 - accuracy: 0.7452 - val_loss: 0.1839 - val_accuracy: 0.7400\n",
      "Epoch 101/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1804 - accuracy: 0.7350 - val_loss: 0.1836 - val_accuracy: 0.7394\n",
      "Epoch 102/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1771 - accuracy: 0.7414 - val_loss: 0.1848 - val_accuracy: 0.7374\n",
      "Epoch 103/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1780 - accuracy: 0.7410 - val_loss: 0.1846 - val_accuracy: 0.7383\n",
      "Epoch 104/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1783 - accuracy: 0.7386 - val_loss: 0.1845 - val_accuracy: 0.7383\n",
      "Epoch 105/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1758 - accuracy: 0.7441 - val_loss: 0.1902 - val_accuracy: 0.7326\n",
      "Epoch 106/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1780 - accuracy: 0.7396 - val_loss: 0.1850 - val_accuracy: 0.7380\n",
      "Epoch 107/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1753 - accuracy: 0.7443 - val_loss: 0.1848 - val_accuracy: 0.7394\n",
      "Epoch 108/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1773 - accuracy: 0.7382 - val_loss: 0.1849 - val_accuracy: 0.7396\n",
      "Epoch 109/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1762 - accuracy: 0.7429 - val_loss: 0.1872 - val_accuracy: 0.7363\n",
      "Epoch 110/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1782 - accuracy: 0.7394 - val_loss: 0.1847 - val_accuracy: 0.7396\n",
      "Epoch 111/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1776 - accuracy: 0.7363 - val_loss: 0.1859 - val_accuracy: 0.7379\n",
      "Epoch 112/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1771 - accuracy: 0.7389 - val_loss: 0.1865 - val_accuracy: 0.7389\n",
      "Epoch 113/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1769 - accuracy: 0.7402 - val_loss: 0.1857 - val_accuracy: 0.7370\n",
      "Epoch 114/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1766 - accuracy: 0.7400 - val_loss: 0.1854 - val_accuracy: 0.7380\n",
      "Epoch 115/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1770 - accuracy: 0.7426 - val_loss: 0.1861 - val_accuracy: 0.7371\n",
      "Epoch 116/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1778 - accuracy: 0.7418 - val_loss: 0.1854 - val_accuracy: 0.7375\n",
      "Epoch 117/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1767 - accuracy: 0.7426 - val_loss: 0.1871 - val_accuracy: 0.7362\n",
      "Epoch 118/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1754 - accuracy: 0.7451 - val_loss: 0.1869 - val_accuracy: 0.7362\n",
      "Epoch 119/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1768 - accuracy: 0.7376 - val_loss: 0.1858 - val_accuracy: 0.7380\n",
      "Epoch 120/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1771 - accuracy: 0.7379 - val_loss: 0.1859 - val_accuracy: 0.7368\n",
      "Epoch 121/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1773 - accuracy: 0.7403 - val_loss: 0.1854 - val_accuracy: 0.7380\n",
      "Epoch 122/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1729 - accuracy: 0.7499 - val_loss: 0.1857 - val_accuracy: 0.7367\n",
      "Epoch 123/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1746 - accuracy: 0.7457 - val_loss: 0.1848 - val_accuracy: 0.7389\n",
      "Epoch 124/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1753 - accuracy: 0.7433 - val_loss: 0.1861 - val_accuracy: 0.7375\n",
      "Epoch 125/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1767 - accuracy: 0.7415 - val_loss: 0.1863 - val_accuracy: 0.7375\n",
      "Epoch 126/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1770 - accuracy: 0.7420 - val_loss: 0.1869 - val_accuracy: 0.7368\n",
      "Epoch 127/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1772 - accuracy: 0.7422 - val_loss: 0.1867 - val_accuracy: 0.7376\n",
      "Epoch 128/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1773 - accuracy: 0.7408 - val_loss: 0.1863 - val_accuracy: 0.7367\n",
      "Epoch 129/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1801 - accuracy: 0.7349 - val_loss: 0.1858 - val_accuracy: 0.7363\n",
      "Epoch 130/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1761 - accuracy: 0.7442 - val_loss: 0.1852 - val_accuracy: 0.7371\n",
      "Epoch 131/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1773 - accuracy: 0.7398 - val_loss: 0.1872 - val_accuracy: 0.7376\n",
      "Epoch 132/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1777 - accuracy: 0.7390 - val_loss: 0.1863 - val_accuracy: 0.7366\n",
      "Epoch 133/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1789 - accuracy: 0.7349 - val_loss: 0.1861 - val_accuracy: 0.7370\n",
      "Epoch 134/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1780 - accuracy: 0.7378 - val_loss: 0.1858 - val_accuracy: 0.7371\n",
      "Epoch 135/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1741 - accuracy: 0.7454 - val_loss: 0.1860 - val_accuracy: 0.7367\n",
      "Epoch 136/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1794 - accuracy: 0.7358 - val_loss: 0.1861 - val_accuracy: 0.7375\n",
      "Epoch 137/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1761 - accuracy: 0.7431 - val_loss: 0.1865 - val_accuracy: 0.7367\n",
      "Epoch 138/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1756 - accuracy: 0.7448 - val_loss: 0.1858 - val_accuracy: 0.7363\n",
      "Epoch 139/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1759 - accuracy: 0.7418 - val_loss: 0.1881 - val_accuracy: 0.7361\n",
      "Epoch 140/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1782 - accuracy: 0.7365 - val_loss: 0.1867 - val_accuracy: 0.7361\n",
      "Epoch 141/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1782 - accuracy: 0.7399 - val_loss: 0.1860 - val_accuracy: 0.7362\n",
      "Epoch 142/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1782 - accuracy: 0.7371 - val_loss: 0.1864 - val_accuracy: 0.7367\n",
      "Epoch 143/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1745 - accuracy: 0.7433 - val_loss: 0.1860 - val_accuracy: 0.7376\n",
      "Epoch 144/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1753 - accuracy: 0.7424 - val_loss: 0.1866 - val_accuracy: 0.7343\n",
      "Epoch 145/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1761 - accuracy: 0.7382 - val_loss: 0.1863 - val_accuracy: 0.7365\n",
      "Epoch 146/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1764 - accuracy: 0.7420 - val_loss: 0.1854 - val_accuracy: 0.7381\n",
      "Epoch 147/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1784 - accuracy: 0.7368 - val_loss: 0.1852 - val_accuracy: 0.7389\n",
      "Epoch 148/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1759 - accuracy: 0.7411 - val_loss: 0.1854 - val_accuracy: 0.7380\n",
      "Epoch 149/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1775 - accuracy: 0.7390 - val_loss: 0.1872 - val_accuracy: 0.7358\n",
      "Epoch 150/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1753 - accuracy: 0.7429 - val_loss: 0.1861 - val_accuracy: 0.7371\n",
      "Epoch 151/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1802 - accuracy: 0.7371 - val_loss: 0.1863 - val_accuracy: 0.7374\n",
      "Epoch 152/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1777 - accuracy: 0.7430 - val_loss: 0.1859 - val_accuracy: 0.7385\n",
      "Epoch 153/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1750 - accuracy: 0.7448 - val_loss: 0.1873 - val_accuracy: 0.7366\n",
      "Epoch 154/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1773 - accuracy: 0.7377 - val_loss: 0.1864 - val_accuracy: 0.7391\n",
      "Epoch 155/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1802 - accuracy: 0.7349 - val_loss: 0.1860 - val_accuracy: 0.7393\n",
      "Epoch 156/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1768 - accuracy: 0.7434 - val_loss: 0.1864 - val_accuracy: 0.7380\n",
      "Epoch 157/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1773 - accuracy: 0.7395 - val_loss: 0.1870 - val_accuracy: 0.7380\n",
      "Epoch 158/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1752 - accuracy: 0.7452 - val_loss: 0.1871 - val_accuracy: 0.7372\n",
      "Epoch 159/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1781 - accuracy: 0.7376 - val_loss: 0.1869 - val_accuracy: 0.7384\n",
      "Epoch 160/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1800 - accuracy: 0.7370 - val_loss: 0.1865 - val_accuracy: 0.7392\n",
      "Epoch 161/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1775 - accuracy: 0.7412 - val_loss: 0.1857 - val_accuracy: 0.7381\n",
      "Epoch 162/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1793 - accuracy: 0.7400 - val_loss: 0.1853 - val_accuracy: 0.7391\n",
      "Epoch 163/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1770 - accuracy: 0.7421 - val_loss: 0.1856 - val_accuracy: 0.7378\n",
      "Epoch 164/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1762 - accuracy: 0.7421 - val_loss: 0.1849 - val_accuracy: 0.7396\n",
      "Epoch 165/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1758 - accuracy: 0.7426 - val_loss: 0.1854 - val_accuracy: 0.7385\n",
      "Epoch 166/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1775 - accuracy: 0.7421 - val_loss: 0.1857 - val_accuracy: 0.7370\n",
      "Epoch 167/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1772 - accuracy: 0.7409 - val_loss: 0.1858 - val_accuracy: 0.7378\n",
      "Epoch 168/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1769 - accuracy: 0.7418 - val_loss: 0.1858 - val_accuracy: 0.7372\n",
      "Epoch 169/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1751 - accuracy: 0.7450 - val_loss: 0.1864 - val_accuracy: 0.7384\n",
      "Epoch 170/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1765 - accuracy: 0.7447 - val_loss: 0.1867 - val_accuracy: 0.7380\n",
      "Epoch 171/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1770 - accuracy: 0.7424 - val_loss: 0.1866 - val_accuracy: 0.7375\n",
      "Epoch 172/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1768 - accuracy: 0.7409 - val_loss: 0.1866 - val_accuracy: 0.7368\n",
      "Epoch 173/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1769 - accuracy: 0.7413 - val_loss: 0.1865 - val_accuracy: 0.7380\n",
      "Epoch 174/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1790 - accuracy: 0.7376 - val_loss: 0.1856 - val_accuracy: 0.7384\n",
      "Epoch 175/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1760 - accuracy: 0.7423 - val_loss: 0.1864 - val_accuracy: 0.7387\n",
      "Epoch 176/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1787 - accuracy: 0.7398 - val_loss: 0.1862 - val_accuracy: 0.7385\n",
      "Epoch 177/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1798 - accuracy: 0.7366 - val_loss: 0.1866 - val_accuracy: 0.7387\n",
      "Epoch 178/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1781 - accuracy: 0.7375 - val_loss: 0.1867 - val_accuracy: 0.7379\n",
      "Epoch 179/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1750 - accuracy: 0.7431 - val_loss: 0.1860 - val_accuracy: 0.7387\n",
      "Epoch 180/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1767 - accuracy: 0.7405 - val_loss: 0.1860 - val_accuracy: 0.7391\n",
      "Epoch 181/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1766 - accuracy: 0.7422 - val_loss: 0.1854 - val_accuracy: 0.7394\n",
      "Epoch 182/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1749 - accuracy: 0.7465 - val_loss: 0.1864 - val_accuracy: 0.7385\n",
      "Epoch 183/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1779 - accuracy: 0.7394 - val_loss: 0.1860 - val_accuracy: 0.7379\n",
      "Epoch 184/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1775 - accuracy: 0.7408 - val_loss: 0.1904 - val_accuracy: 0.7292\n",
      "Epoch 185/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1772 - accuracy: 0.7400 - val_loss: 0.1865 - val_accuracy: 0.7367\n",
      "Epoch 186/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1788 - accuracy: 0.7374 - val_loss: 0.1865 - val_accuracy: 0.7374\n",
      "Epoch 187/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1756 - accuracy: 0.7411 - val_loss: 0.1859 - val_accuracy: 0.7387\n",
      "Epoch 188/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1767 - accuracy: 0.7408 - val_loss: 0.1853 - val_accuracy: 0.7366\n",
      "Epoch 189/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1750 - accuracy: 0.7414 - val_loss: 0.1859 - val_accuracy: 0.7388\n",
      "Epoch 190/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1751 - accuracy: 0.7413 - val_loss: 0.1863 - val_accuracy: 0.7379\n",
      "Epoch 191/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1740 - accuracy: 0.7464 - val_loss: 0.1854 - val_accuracy: 0.7396\n",
      "Epoch 192/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1765 - accuracy: 0.7405 - val_loss: 0.1860 - val_accuracy: 0.7371\n",
      "Epoch 193/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1756 - accuracy: 0.7428 - val_loss: 0.1856 - val_accuracy: 0.7393\n",
      "Epoch 194/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1767 - accuracy: 0.7393 - val_loss: 0.1850 - val_accuracy: 0.7402\n",
      "Epoch 195/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1766 - accuracy: 0.7435 - val_loss: 0.1853 - val_accuracy: 0.7411\n",
      "Epoch 196/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1789 - accuracy: 0.7376 - val_loss: 0.1854 - val_accuracy: 0.7388\n",
      "Epoch 197/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1770 - accuracy: 0.7394 - val_loss: 0.1848 - val_accuracy: 0.7394\n",
      "Epoch 198/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1789 - accuracy: 0.7391 - val_loss: 0.1861 - val_accuracy: 0.7363\n",
      "Epoch 199/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1767 - accuracy: 0.7411 - val_loss: 0.1860 - val_accuracy: 0.7381\n",
      "Epoch 200/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1742 - accuracy: 0.7471 - val_loss: 0.1862 - val_accuracy: 0.7388\n",
      "Epoch 201/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1782 - accuracy: 0.7390 - val_loss: 0.1848 - val_accuracy: 0.7392\n",
      "Epoch 202/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1788 - accuracy: 0.7364 - val_loss: 0.1849 - val_accuracy: 0.7391\n",
      "Epoch 203/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1787 - accuracy: 0.7372 - val_loss: 0.1865 - val_accuracy: 0.7339\n",
      "Epoch 204/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1781 - accuracy: 0.7344 - val_loss: 0.1875 - val_accuracy: 0.7362\n",
      "Epoch 205/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1782 - accuracy: 0.7393 - val_loss: 0.1860 - val_accuracy: 0.7387\n",
      "Epoch 206/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1754 - accuracy: 0.7444 - val_loss: 0.1875 - val_accuracy: 0.7361\n",
      "Epoch 207/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1760 - accuracy: 0.7451 - val_loss: 0.1879 - val_accuracy: 0.7356\n",
      "Epoch 208/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1772 - accuracy: 0.7453 - val_loss: 0.1858 - val_accuracy: 0.7384\n",
      "Epoch 209/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1767 - accuracy: 0.7436 - val_loss: 0.1861 - val_accuracy: 0.7370\n",
      "Epoch 210/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1783 - accuracy: 0.7385 - val_loss: 0.1858 - val_accuracy: 0.7393\n",
      "Epoch 211/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1774 - accuracy: 0.7392 - val_loss: 0.1859 - val_accuracy: 0.7375\n",
      "Epoch 212/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1747 - accuracy: 0.7433 - val_loss: 0.1856 - val_accuracy: 0.7381\n",
      "Epoch 213/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1735 - accuracy: 0.7459 - val_loss: 0.1855 - val_accuracy: 0.7380\n",
      "Epoch 214/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1796 - accuracy: 0.7371 - val_loss: 0.1860 - val_accuracy: 0.7380\n",
      "Epoch 215/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1779 - accuracy: 0.7403 - val_loss: 0.1872 - val_accuracy: 0.7379\n",
      "Epoch 216/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1764 - accuracy: 0.7419 - val_loss: 0.1861 - val_accuracy: 0.7383\n",
      "Epoch 217/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1789 - accuracy: 0.7345 - val_loss: 0.1867 - val_accuracy: 0.7375\n",
      "Epoch 218/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1780 - accuracy: 0.7370 - val_loss: 0.1857 - val_accuracy: 0.7374\n",
      "Epoch 219/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1785 - accuracy: 0.7357 - val_loss: 0.1868 - val_accuracy: 0.7372\n",
      "Epoch 220/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1795 - accuracy: 0.7376 - val_loss: 0.1850 - val_accuracy: 0.7388\n",
      "Epoch 221/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1757 - accuracy: 0.7410 - val_loss: 0.1861 - val_accuracy: 0.7389\n",
      "Epoch 222/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1739 - accuracy: 0.7447 - val_loss: 0.1860 - val_accuracy: 0.7383\n",
      "Epoch 223/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1785 - accuracy: 0.7391 - val_loss: 0.1855 - val_accuracy: 0.7352\n",
      "Epoch 224/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1752 - accuracy: 0.7438 - val_loss: 0.1854 - val_accuracy: 0.7384\n",
      "Epoch 225/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1789 - accuracy: 0.7384 - val_loss: 0.1853 - val_accuracy: 0.7376\n",
      "Epoch 226/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1775 - accuracy: 0.7418 - val_loss: 0.1872 - val_accuracy: 0.7363\n",
      "Epoch 227/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1764 - accuracy: 0.7417 - val_loss: 0.1880 - val_accuracy: 0.7372\n",
      "Epoch 228/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1779 - accuracy: 0.7405 - val_loss: 0.1875 - val_accuracy: 0.7368\n",
      "Epoch 229/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1759 - accuracy: 0.7430 - val_loss: 0.1858 - val_accuracy: 0.7372\n",
      "Epoch 230/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1762 - accuracy: 0.7418 - val_loss: 0.1857 - val_accuracy: 0.7384\n",
      "Epoch 231/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1744 - accuracy: 0.7470 - val_loss: 0.1858 - val_accuracy: 0.7378\n",
      "Epoch 232/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1753 - accuracy: 0.7447 - val_loss: 0.1860 - val_accuracy: 0.7380\n",
      "Epoch 233/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1761 - accuracy: 0.7428 - val_loss: 0.1866 - val_accuracy: 0.7365\n",
      "Epoch 234/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1752 - accuracy: 0.7425 - val_loss: 0.1858 - val_accuracy: 0.7371\n",
      "Epoch 235/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1777 - accuracy: 0.7395 - val_loss: 0.1858 - val_accuracy: 0.7374\n",
      "Epoch 236/1000\n",
      "563/563 [==============================] - ETA: 0s - loss: 0.1765 - accuracy: 0.74 - 1s 1ms/step - loss: 0.1765 - accuracy: 0.7423 - val_loss: 0.1861 - val_accuracy: 0.7379\n",
      "Epoch 237/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1779 - accuracy: 0.7407 - val_loss: 0.1859 - val_accuracy: 0.7385\n",
      "Epoch 238/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1741 - accuracy: 0.7469 - val_loss: 0.1860 - val_accuracy: 0.7384\n",
      "Epoch 239/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1763 - accuracy: 0.7403 - val_loss: 0.1860 - val_accuracy: 0.7368\n",
      "Epoch 240/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1773 - accuracy: 0.7430 - val_loss: 0.1855 - val_accuracy: 0.7388\n",
      "Epoch 241/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1745 - accuracy: 0.7447 - val_loss: 0.1918 - val_accuracy: 0.7326\n",
      "Epoch 242/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1775 - accuracy: 0.7411 - val_loss: 0.1881 - val_accuracy: 0.7354\n",
      "Epoch 243/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1768 - accuracy: 0.7408 - val_loss: 0.1864 - val_accuracy: 0.7381\n",
      "Epoch 244/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1764 - accuracy: 0.7426 - val_loss: 0.1870 - val_accuracy: 0.7374\n",
      "Epoch 245/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1762 - accuracy: 0.7446 - val_loss: 0.1884 - val_accuracy: 0.7387\n",
      "Epoch 246/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1766 - accuracy: 0.7395 - val_loss: 0.1873 - val_accuracy: 0.7368\n",
      "Epoch 247/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1743 - accuracy: 0.7454 - val_loss: 0.1866 - val_accuracy: 0.7384\n",
      "Epoch 248/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1762 - accuracy: 0.7379 - val_loss: 0.1892 - val_accuracy: 0.7349\n",
      "Epoch 249/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1780 - accuracy: 0.7430 - val_loss: 0.1894 - val_accuracy: 0.7346\n",
      "Epoch 250/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1766 - accuracy: 0.7417 - val_loss: 0.1882 - val_accuracy: 0.7371\n",
      "Epoch 251/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1811 - accuracy: 0.7295 - val_loss: 0.1856 - val_accuracy: 0.7388\n",
      "Epoch 252/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1770 - accuracy: 0.7414 - val_loss: 0.1865 - val_accuracy: 0.7380\n",
      "Epoch 253/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1775 - accuracy: 0.7411 - val_loss: 0.1938 - val_accuracy: 0.7305\n",
      "Epoch 254/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1866 - accuracy: 0.7309 - val_loss: 0.1906 - val_accuracy: 0.7340\n",
      "Epoch 255/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1791 - accuracy: 0.7412 - val_loss: 0.1915 - val_accuracy: 0.7336\n",
      "Epoch 256/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1799 - accuracy: 0.7407 - val_loss: 0.1925 - val_accuracy: 0.7332\n",
      "Epoch 257/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1819 - accuracy: 0.7350 - val_loss: 0.1861 - val_accuracy: 0.7376\n",
      "Epoch 258/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1781 - accuracy: 0.7376 - val_loss: 0.1861 - val_accuracy: 0.7387\n",
      "Epoch 259/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1755 - accuracy: 0.7450 - val_loss: 0.1861 - val_accuracy: 0.7374\n",
      "Epoch 260/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1765 - accuracy: 0.7405 - val_loss: 0.1867 - val_accuracy: 0.7372\n",
      "Epoch 261/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1768 - accuracy: 0.7410 - val_loss: 0.1868 - val_accuracy: 0.7367\n",
      "Epoch 262/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1748 - accuracy: 0.7474 - val_loss: 0.1868 - val_accuracy: 0.7368\n",
      "Epoch 263/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1803 - accuracy: 0.7359 - val_loss: 0.1870 - val_accuracy: 0.7349\n",
      "Epoch 264/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1764 - accuracy: 0.7410 - val_loss: 0.1873 - val_accuracy: 0.7341\n",
      "Epoch 265/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1773 - accuracy: 0.7398 - val_loss: 0.1896 - val_accuracy: 0.7341\n",
      "Epoch 266/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1785 - accuracy: 0.7404 - val_loss: 0.1848 - val_accuracy: 0.7391\n",
      "Epoch 267/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1786 - accuracy: 0.7375 - val_loss: 0.1853 - val_accuracy: 0.7392\n",
      "Epoch 268/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1784 - accuracy: 0.7365 - val_loss: 0.1863 - val_accuracy: 0.7359\n",
      "Epoch 269/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1749 - accuracy: 0.7446 - val_loss: 0.1868 - val_accuracy: 0.7344\n",
      "Epoch 270/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1765 - accuracy: 0.7396 - val_loss: 0.1868 - val_accuracy: 0.7372\n",
      "Epoch 271/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1767 - accuracy: 0.7409 - val_loss: 0.1872 - val_accuracy: 0.7366\n",
      "Epoch 272/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1763 - accuracy: 0.7400 - val_loss: 0.1868 - val_accuracy: 0.7368\n",
      "Epoch 273/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1749 - accuracy: 0.7446 - val_loss: 0.1871 - val_accuracy: 0.7361\n",
      "Epoch 274/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1747 - accuracy: 0.7443 - val_loss: 0.1857 - val_accuracy: 0.7359\n",
      "Epoch 275/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1791 - accuracy: 0.7382 - val_loss: 0.1854 - val_accuracy: 0.7381\n",
      "Epoch 276/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1760 - accuracy: 0.7440 - val_loss: 0.1856 - val_accuracy: 0.7375\n",
      "Epoch 277/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1773 - accuracy: 0.7430 - val_loss: 0.1858 - val_accuracy: 0.7374\n",
      "Epoch 278/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1752 - accuracy: 0.7459 - val_loss: 0.1861 - val_accuracy: 0.7379\n",
      "Epoch 279/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1769 - accuracy: 0.7409 - val_loss: 0.1854 - val_accuracy: 0.7387\n",
      "Epoch 280/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1755 - accuracy: 0.7435 - val_loss: 0.1862 - val_accuracy: 0.7381\n",
      "Epoch 281/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1737 - accuracy: 0.7470 - val_loss: 0.1871 - val_accuracy: 0.7374\n",
      "Epoch 282/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1779 - accuracy: 0.7404 - val_loss: 0.1868 - val_accuracy: 0.7367\n",
      "Epoch 283/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1769 - accuracy: 0.7401 - val_loss: 0.1873 - val_accuracy: 0.7362\n",
      "Epoch 284/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1772 - accuracy: 0.7425 - val_loss: 0.1867 - val_accuracy: 0.7376\n",
      "Epoch 285/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1765 - accuracy: 0.7422 - val_loss: 0.1861 - val_accuracy: 0.7376\n",
      "Epoch 286/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1762 - accuracy: 0.7427 - val_loss: 0.1872 - val_accuracy: 0.7380\n",
      "Epoch 287/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1745 - accuracy: 0.7456 - val_loss: 0.1864 - val_accuracy: 0.7376\n",
      "Epoch 288/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1777 - accuracy: 0.7380 - val_loss: 0.1896 - val_accuracy: 0.7344\n",
      "Epoch 289/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1788 - accuracy: 0.7406 - val_loss: 0.1865 - val_accuracy: 0.7392\n",
      "Epoch 290/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1760 - accuracy: 0.7418 - val_loss: 0.1860 - val_accuracy: 0.7392\n",
      "Epoch 291/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1766 - accuracy: 0.7415 - val_loss: 0.1864 - val_accuracy: 0.7392\n",
      "Epoch 292/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1754 - accuracy: 0.7437 - val_loss: 0.1860 - val_accuracy: 0.7397\n",
      "Epoch 293/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1778 - accuracy: 0.7382 - val_loss: 0.1859 - val_accuracy: 0.7384\n",
      "Epoch 294/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1732 - accuracy: 0.7477 - val_loss: 0.1860 - val_accuracy: 0.7389\n",
      "Epoch 295/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1772 - accuracy: 0.7389 - val_loss: 0.1864 - val_accuracy: 0.7383\n",
      "Epoch 296/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1784 - accuracy: 0.7393 - val_loss: 0.1864 - val_accuracy: 0.7372\n",
      "Epoch 297/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1756 - accuracy: 0.7424 - val_loss: 0.1862 - val_accuracy: 0.7368\n",
      "Epoch 298/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1754 - accuracy: 0.7435 - val_loss: 0.1858 - val_accuracy: 0.7380\n",
      "Epoch 299/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1765 - accuracy: 0.7439 - val_loss: 0.1854 - val_accuracy: 0.7379\n",
      "Epoch 300/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1779 - accuracy: 0.7411 - val_loss: 0.1861 - val_accuracy: 0.7376\n",
      "Epoch 301/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1778 - accuracy: 0.7384 - val_loss: 0.1855 - val_accuracy: 0.7384\n",
      "Epoch 302/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1758 - accuracy: 0.7452 - val_loss: 0.1853 - val_accuracy: 0.7378\n",
      "Epoch 303/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1735 - accuracy: 0.7487 - val_loss: 0.1864 - val_accuracy: 0.7374\n",
      "Epoch 304/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1760 - accuracy: 0.7392 - val_loss: 0.1869 - val_accuracy: 0.7384\n",
      "Epoch 305/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1754 - accuracy: 0.7440 - val_loss: 0.1862 - val_accuracy: 0.7371\n",
      "Epoch 306/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1747 - accuracy: 0.7457 - val_loss: 0.1853 - val_accuracy: 0.7379\n",
      "Epoch 307/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1746 - accuracy: 0.7459 - val_loss: 0.1860 - val_accuracy: 0.7381\n",
      "Epoch 308/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1762 - accuracy: 0.7428 - val_loss: 0.1863 - val_accuracy: 0.7374\n",
      "Epoch 309/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1791 - accuracy: 0.7351 - val_loss: 0.1857 - val_accuracy: 0.7391\n",
      "Epoch 310/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1761 - accuracy: 0.7426 - val_loss: 0.1862 - val_accuracy: 0.7380\n",
      "Epoch 311/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1766 - accuracy: 0.7430 - val_loss: 0.1862 - val_accuracy: 0.7384\n",
      "Epoch 312/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1743 - accuracy: 0.7456 - val_loss: 0.1865 - val_accuracy: 0.7385\n",
      "Epoch 313/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1743 - accuracy: 0.7459 - val_loss: 0.1863 - val_accuracy: 0.7388\n",
      "Epoch 314/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1758 - accuracy: 0.7397 - val_loss: 0.1873 - val_accuracy: 0.7359\n",
      "Epoch 315/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1765 - accuracy: 0.7407 - val_loss: 0.1870 - val_accuracy: 0.7372\n",
      "Epoch 316/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1742 - accuracy: 0.7467 - val_loss: 0.1881 - val_accuracy: 0.7376\n",
      "Epoch 317/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1798 - accuracy: 0.7384 - val_loss: 0.1870 - val_accuracy: 0.7379\n",
      "Epoch 318/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1774 - accuracy: 0.7394 - val_loss: 0.1860 - val_accuracy: 0.7381\n",
      "Epoch 319/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1757 - accuracy: 0.7464 - val_loss: 0.1867 - val_accuracy: 0.7376\n",
      "Epoch 320/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1761 - accuracy: 0.7411 - val_loss: 0.1871 - val_accuracy: 0.7378\n",
      "Epoch 321/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1771 - accuracy: 0.7389 - val_loss: 0.1865 - val_accuracy: 0.7383\n",
      "Epoch 322/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1754 - accuracy: 0.7437 - val_loss: 0.1853 - val_accuracy: 0.7393\n",
      "Epoch 323/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1777 - accuracy: 0.7394 - val_loss: 0.1862 - val_accuracy: 0.7388\n",
      "Epoch 324/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1761 - accuracy: 0.7451 - val_loss: 0.1869 - val_accuracy: 0.7365\n",
      "Epoch 325/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1775 - accuracy: 0.7410 - val_loss: 0.1867 - val_accuracy: 0.7376\n",
      "Epoch 326/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1758 - accuracy: 0.7414 - val_loss: 0.1868 - val_accuracy: 0.7379\n",
      "Epoch 327/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1740 - accuracy: 0.7483 - val_loss: 0.1869 - val_accuracy: 0.7376\n",
      "Epoch 328/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1736 - accuracy: 0.7475 - val_loss: 0.1869 - val_accuracy: 0.7372\n",
      "Epoch 329/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1803 - accuracy: 0.7333 - val_loss: 0.1864 - val_accuracy: 0.7365\n",
      "Epoch 330/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1733 - accuracy: 0.7480 - val_loss: 0.1870 - val_accuracy: 0.7365\n",
      "Epoch 331/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1741 - accuracy: 0.7448 - val_loss: 0.1871 - val_accuracy: 0.7371\n",
      "Epoch 332/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1747 - accuracy: 0.7440 - val_loss: 0.1869 - val_accuracy: 0.7365\n",
      "Epoch 333/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1738 - accuracy: 0.7480 - val_loss: 0.1871 - val_accuracy: 0.7370\n",
      "Epoch 334/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1766 - accuracy: 0.7414 - val_loss: 0.1864 - val_accuracy: 0.7370\n",
      "Epoch 335/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1778 - accuracy: 0.7395 - val_loss: 0.1871 - val_accuracy: 0.7353\n",
      "Epoch 336/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1785 - accuracy: 0.7378 - val_loss: 0.1865 - val_accuracy: 0.7363\n",
      "Epoch 337/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1766 - accuracy: 0.7435 - val_loss: 0.1866 - val_accuracy: 0.7371\n",
      "Epoch 338/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1755 - accuracy: 0.7441 - val_loss: 0.1873 - val_accuracy: 0.7366\n",
      "Epoch 339/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1774 - accuracy: 0.7410 - val_loss: 0.1867 - val_accuracy: 0.7376\n",
      "Epoch 340/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1795 - accuracy: 0.7359 - val_loss: 0.1873 - val_accuracy: 0.7371\n",
      "Epoch 341/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1785 - accuracy: 0.7389 - val_loss: 0.1879 - val_accuracy: 0.7372\n",
      "Epoch 342/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1764 - accuracy: 0.7457 - val_loss: 0.1870 - val_accuracy: 0.7343\n",
      "Epoch 343/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1767 - accuracy: 0.7414 - val_loss: 0.1888 - val_accuracy: 0.7370\n",
      "Epoch 344/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1769 - accuracy: 0.7407 - val_loss: 0.1884 - val_accuracy: 0.7357\n",
      "Epoch 345/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1734 - accuracy: 0.7481 - val_loss: 0.1871 - val_accuracy: 0.7346\n",
      "Epoch 346/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1749 - accuracy: 0.7449 - val_loss: 0.1871 - val_accuracy: 0.7365\n",
      "Epoch 347/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1749 - accuracy: 0.7443 - val_loss: 0.1877 - val_accuracy: 0.7352\n",
      "Epoch 348/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1770 - accuracy: 0.7378 - val_loss: 0.1863 - val_accuracy: 0.7371\n",
      "Epoch 349/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1776 - accuracy: 0.7396 - val_loss: 0.1871 - val_accuracy: 0.7359\n",
      "Epoch 350/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1755 - accuracy: 0.7433 - val_loss: 0.1872 - val_accuracy: 0.7370\n",
      "Epoch 351/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1740 - accuracy: 0.7455 - val_loss: 0.1870 - val_accuracy: 0.7371\n",
      "Epoch 352/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1718 - accuracy: 0.7527 - val_loss: 0.1875 - val_accuracy: 0.7368\n",
      "Epoch 353/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1772 - accuracy: 0.7407 - val_loss: 0.1870 - val_accuracy: 0.7371\n",
      "Epoch 354/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1769 - accuracy: 0.7392 - val_loss: 0.1874 - val_accuracy: 0.7368\n",
      "Epoch 355/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1740 - accuracy: 0.7463 - val_loss: 0.1881 - val_accuracy: 0.7362\n",
      "Epoch 356/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1753 - accuracy: 0.7438 - val_loss: 0.1880 - val_accuracy: 0.7362\n",
      "Epoch 357/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1772 - accuracy: 0.7403 - val_loss: 0.1871 - val_accuracy: 0.7385\n",
      "Epoch 358/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1757 - accuracy: 0.7451 - val_loss: 0.1879 - val_accuracy: 0.7374\n",
      "Epoch 359/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1769 - accuracy: 0.7399 - val_loss: 0.1871 - val_accuracy: 0.7374\n",
      "Epoch 360/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1756 - accuracy: 0.7424 - val_loss: 0.1873 - val_accuracy: 0.7374\n",
      "Epoch 361/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1765 - accuracy: 0.7414 - val_loss: 0.1855 - val_accuracy: 0.7391\n",
      "Epoch 362/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1767 - accuracy: 0.7431 - val_loss: 0.1852 - val_accuracy: 0.7391\n",
      "Epoch 363/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1760 - accuracy: 0.7434 - val_loss: 0.1860 - val_accuracy: 0.7379\n",
      "Epoch 364/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1771 - accuracy: 0.7411 - val_loss: 0.1856 - val_accuracy: 0.7381\n",
      "Epoch 365/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1773 - accuracy: 0.7410 - val_loss: 0.1861 - val_accuracy: 0.7391\n",
      "Epoch 366/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1762 - accuracy: 0.7436 - val_loss: 0.1860 - val_accuracy: 0.7388\n",
      "Epoch 367/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1765 - accuracy: 0.7419 - val_loss: 0.1867 - val_accuracy: 0.7384\n",
      "Epoch 368/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1758 - accuracy: 0.7435 - val_loss: 0.1870 - val_accuracy: 0.7384\n",
      "Epoch 369/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1780 - accuracy: 0.7411 - val_loss: 0.1860 - val_accuracy: 0.7381\n",
      "Epoch 370/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1758 - accuracy: 0.7443 - val_loss: 0.1868 - val_accuracy: 0.7374\n",
      "Epoch 371/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1733 - accuracy: 0.7486 - val_loss: 0.1862 - val_accuracy: 0.7376\n",
      "Epoch 372/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1747 - accuracy: 0.7448 - val_loss: 0.1863 - val_accuracy: 0.7378\n",
      "Epoch 373/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1771 - accuracy: 0.7401 - val_loss: 0.1869 - val_accuracy: 0.7372\n",
      "Epoch 374/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1768 - accuracy: 0.7428 - val_loss: 0.1887 - val_accuracy: 0.7348\n",
      "Epoch 375/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1794 - accuracy: 0.7365 - val_loss: 0.1871 - val_accuracy: 0.7383\n",
      "Epoch 376/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1774 - accuracy: 0.7424 - val_loss: 0.1871 - val_accuracy: 0.7381\n",
      "Epoch 377/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1784 - accuracy: 0.7381 - val_loss: 0.1873 - val_accuracy: 0.7372\n",
      "Epoch 378/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1751 - accuracy: 0.7466 - val_loss: 0.1881 - val_accuracy: 0.7371\n",
      "Epoch 379/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1767 - accuracy: 0.7385 - val_loss: 0.1871 - val_accuracy: 0.7372\n",
      "Epoch 380/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1762 - accuracy: 0.7424 - val_loss: 0.1866 - val_accuracy: 0.7350\n",
      "Epoch 381/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1767 - accuracy: 0.7413 - val_loss: 0.1880 - val_accuracy: 0.7345\n",
      "Epoch 382/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1760 - accuracy: 0.7416 - val_loss: 0.1883 - val_accuracy: 0.7340\n",
      "Epoch 383/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1776 - accuracy: 0.7408 - val_loss: 0.1884 - val_accuracy: 0.7352\n",
      "Epoch 384/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1789 - accuracy: 0.7335 - val_loss: 0.1863 - val_accuracy: 0.7368\n",
      "Epoch 385/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1767 - accuracy: 0.7413 - val_loss: 0.1859 - val_accuracy: 0.7383\n",
      "Epoch 386/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1737 - accuracy: 0.7475 - val_loss: 0.1862 - val_accuracy: 0.7343\n",
      "Epoch 387/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1753 - accuracy: 0.7412 - val_loss: 0.1875 - val_accuracy: 0.7358\n",
      "Epoch 388/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1778 - accuracy: 0.7365 - val_loss: 0.1869 - val_accuracy: 0.7368\n",
      "Epoch 389/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1767 - accuracy: 0.7441 - val_loss: 0.1855 - val_accuracy: 0.7365\n",
      "Epoch 390/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1755 - accuracy: 0.7445 - val_loss: 0.1870 - val_accuracy: 0.7362\n",
      "Epoch 391/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1761 - accuracy: 0.7437 - val_loss: 0.1866 - val_accuracy: 0.7363\n",
      "Epoch 392/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1779 - accuracy: 0.7360 - val_loss: 0.1879 - val_accuracy: 0.7341\n",
      "Epoch 393/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1780 - accuracy: 0.7411 - val_loss: 0.1876 - val_accuracy: 0.7349\n",
      "Epoch 394/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1760 - accuracy: 0.7449 - val_loss: 0.1863 - val_accuracy: 0.7371\n",
      "Epoch 395/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1769 - accuracy: 0.7429 - val_loss: 0.1869 - val_accuracy: 0.7379\n",
      "Epoch 396/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1774 - accuracy: 0.7397 - val_loss: 0.1871 - val_accuracy: 0.7376\n",
      "Epoch 397/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1799 - accuracy: 0.7357 - val_loss: 0.1896 - val_accuracy: 0.7357\n",
      "Epoch 398/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1778 - accuracy: 0.7404 - val_loss: 0.1865 - val_accuracy: 0.7367\n",
      "Epoch 399/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1775 - accuracy: 0.7428 - val_loss: 0.1873 - val_accuracy: 0.7374\n",
      "Epoch 400/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1771 - accuracy: 0.7395 - val_loss: 0.1868 - val_accuracy: 0.7375\n",
      "Epoch 401/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1762 - accuracy: 0.7429 - val_loss: 0.1871 - val_accuracy: 0.7362\n",
      "Epoch 402/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1776 - accuracy: 0.7370 - val_loss: 0.1894 - val_accuracy: 0.7226\n",
      "Epoch 403/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1779 - accuracy: 0.7319 - val_loss: 0.1895 - val_accuracy: 0.7227\n",
      "Epoch 404/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1782 - accuracy: 0.7293 - val_loss: 0.1900 - val_accuracy: 0.7174\n",
      "Epoch 405/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1760 - accuracy: 0.7304 - val_loss: 0.1927 - val_accuracy: 0.7186\n",
      "Epoch 406/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1780 - accuracy: 0.7295 - val_loss: 0.1903 - val_accuracy: 0.7209\n",
      "Epoch 407/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1768 - accuracy: 0.7318 - val_loss: 0.1890 - val_accuracy: 0.7219\n",
      "Epoch 408/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1787 - accuracy: 0.7372 - val_loss: 0.1873 - val_accuracy: 0.7374\n",
      "Epoch 409/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1772 - accuracy: 0.7429 - val_loss: 0.1872 - val_accuracy: 0.7368\n",
      "Epoch 410/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1791 - accuracy: 0.7361 - val_loss: 0.1864 - val_accuracy: 0.7371\n",
      "Epoch 411/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1765 - accuracy: 0.7429 - val_loss: 0.1873 - val_accuracy: 0.7354\n",
      "Epoch 412/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1787 - accuracy: 0.7421 - val_loss: 0.1866 - val_accuracy: 0.7379\n",
      "Epoch 413/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1777 - accuracy: 0.7406 - val_loss: 0.1867 - val_accuracy: 0.7368\n",
      "Epoch 414/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1767 - accuracy: 0.7433 - val_loss: 0.1866 - val_accuracy: 0.7378\n",
      "Epoch 415/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1747 - accuracy: 0.7424 - val_loss: 0.1864 - val_accuracy: 0.7380\n",
      "Epoch 416/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1773 - accuracy: 0.7403 - val_loss: 0.1861 - val_accuracy: 0.7376\n",
      "Epoch 417/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1787 - accuracy: 0.7346 - val_loss: 0.1871 - val_accuracy: 0.7376\n",
      "Epoch 418/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1766 - accuracy: 0.7442 - val_loss: 0.1868 - val_accuracy: 0.7385\n",
      "Epoch 419/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1759 - accuracy: 0.7417 - val_loss: 0.1865 - val_accuracy: 0.7384\n",
      "Epoch 420/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1735 - accuracy: 0.7462 - val_loss: 0.1865 - val_accuracy: 0.7385\n",
      "Epoch 421/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1779 - accuracy: 0.7372 - val_loss: 0.1875 - val_accuracy: 0.7371\n",
      "Epoch 422/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1766 - accuracy: 0.7393 - val_loss: 0.1874 - val_accuracy: 0.7371\n",
      "Epoch 423/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1752 - accuracy: 0.7444 - val_loss: 0.1874 - val_accuracy: 0.7375\n",
      "Epoch 424/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1767 - accuracy: 0.7423 - val_loss: 0.1862 - val_accuracy: 0.7387\n",
      "Epoch 425/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1748 - accuracy: 0.7422 - val_loss: 0.1869 - val_accuracy: 0.7387\n",
      "Epoch 426/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1747 - accuracy: 0.7432 - val_loss: 0.1863 - val_accuracy: 0.7381\n",
      "Epoch 427/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1745 - accuracy: 0.7469 - val_loss: 0.1867 - val_accuracy: 0.7384\n",
      "Epoch 428/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1727 - accuracy: 0.7476 - val_loss: 0.1866 - val_accuracy: 0.7375\n",
      "Epoch 429/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1745 - accuracy: 0.7450 - val_loss: 0.1866 - val_accuracy: 0.7366\n",
      "Epoch 430/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1787 - accuracy: 0.7390 - val_loss: 0.1866 - val_accuracy: 0.7370\n",
      "Epoch 431/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1753 - accuracy: 0.7450 - val_loss: 0.1880 - val_accuracy: 0.7361\n",
      "Epoch 432/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1732 - accuracy: 0.7452 - val_loss: 0.1873 - val_accuracy: 0.7367\n",
      "Epoch 433/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1797 - accuracy: 0.7378 - val_loss: 0.1876 - val_accuracy: 0.7375\n",
      "Epoch 434/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1767 - accuracy: 0.7439 - val_loss: 0.1876 - val_accuracy: 0.7363\n",
      "Epoch 435/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1748 - accuracy: 0.7444 - val_loss: 0.1907 - val_accuracy: 0.7199\n",
      "Epoch 436/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1784 - accuracy: 0.7308 - val_loss: 0.1883 - val_accuracy: 0.7346\n",
      "Epoch 437/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1774 - accuracy: 0.7404 - val_loss: 0.1874 - val_accuracy: 0.7371\n",
      "Epoch 438/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1770 - accuracy: 0.7380 - val_loss: 0.1876 - val_accuracy: 0.7362\n",
      "Epoch 439/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1767 - accuracy: 0.7405 - val_loss: 0.1874 - val_accuracy: 0.7365\n",
      "Epoch 440/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1778 - accuracy: 0.7386 - val_loss: 0.1881 - val_accuracy: 0.7367\n",
      "Epoch 441/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1766 - accuracy: 0.7408 - val_loss: 0.1887 - val_accuracy: 0.7361\n",
      "Epoch 442/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1770 - accuracy: 0.7437 - val_loss: 0.1875 - val_accuracy: 0.7371\n",
      "Epoch 443/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1775 - accuracy: 0.7375 - val_loss: 0.1877 - val_accuracy: 0.7366\n",
      "Epoch 444/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1761 - accuracy: 0.7439 - val_loss: 0.1868 - val_accuracy: 0.7376\n",
      "Epoch 445/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1771 - accuracy: 0.7421 - val_loss: 0.1906 - val_accuracy: 0.7308\n",
      "Epoch 446/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1800 - accuracy: 0.7368 - val_loss: 0.1874 - val_accuracy: 0.7367\n",
      "Epoch 447/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1753 - accuracy: 0.7440 - val_loss: 0.1872 - val_accuracy: 0.7353\n",
      "Epoch 448/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1759 - accuracy: 0.7458 - val_loss: 0.1878 - val_accuracy: 0.7365\n",
      "Epoch 449/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1774 - accuracy: 0.7410 - val_loss: 0.1875 - val_accuracy: 0.7358\n",
      "Epoch 450/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1769 - accuracy: 0.7412 - val_loss: 0.1875 - val_accuracy: 0.7359\n",
      "Epoch 451/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1772 - accuracy: 0.7388 - val_loss: 0.1868 - val_accuracy: 0.7367\n",
      "Epoch 452/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1762 - accuracy: 0.7427 - val_loss: 0.1862 - val_accuracy: 0.7374\n",
      "Epoch 453/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1778 - accuracy: 0.7396 - val_loss: 0.1880 - val_accuracy: 0.7368\n",
      "Epoch 454/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1773 - accuracy: 0.7413 - val_loss: 0.1873 - val_accuracy: 0.7345\n",
      "Epoch 455/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1728 - accuracy: 0.7487 - val_loss: 0.1862 - val_accuracy: 0.7385\n",
      "Epoch 456/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1774 - accuracy: 0.7372 - val_loss: 0.1870 - val_accuracy: 0.7370\n",
      "Epoch 457/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1765 - accuracy: 0.7418 - val_loss: 0.1885 - val_accuracy: 0.7371\n",
      "Epoch 458/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1784 - accuracy: 0.7375 - val_loss: 0.1860 - val_accuracy: 0.7370\n",
      "Epoch 459/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1799 - accuracy: 0.7347 - val_loss: 0.1872 - val_accuracy: 0.7384\n",
      "Epoch 460/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1762 - accuracy: 0.7413 - val_loss: 0.1863 - val_accuracy: 0.7392\n",
      "Epoch 461/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1773 - accuracy: 0.7395 - val_loss: 0.1873 - val_accuracy: 0.7387\n",
      "Epoch 462/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1732 - accuracy: 0.7477 - val_loss: 0.1867 - val_accuracy: 0.7385\n",
      "Epoch 463/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1795 - accuracy: 0.7388 - val_loss: 0.1860 - val_accuracy: 0.7381\n",
      "Epoch 464/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1768 - accuracy: 0.7418 - val_loss: 0.1863 - val_accuracy: 0.7387\n",
      "Epoch 465/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1757 - accuracy: 0.7423 - val_loss: 0.1863 - val_accuracy: 0.7362\n",
      "Epoch 466/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1764 - accuracy: 0.7428 - val_loss: 0.1867 - val_accuracy: 0.7374\n",
      "Epoch 467/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1757 - accuracy: 0.7419 - val_loss: 0.1895 - val_accuracy: 0.7353\n",
      "Epoch 468/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1739 - accuracy: 0.7483 - val_loss: 0.1875 - val_accuracy: 0.7371\n",
      "Epoch 469/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1747 - accuracy: 0.7450 - val_loss: 0.1868 - val_accuracy: 0.7357\n",
      "Epoch 470/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1790 - accuracy: 0.7370 - val_loss: 0.1871 - val_accuracy: 0.7349\n",
      "Epoch 471/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1781 - accuracy: 0.7380 - val_loss: 0.1869 - val_accuracy: 0.7357\n",
      "Epoch 472/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1758 - accuracy: 0.7441 - val_loss: 0.1868 - val_accuracy: 0.7384\n",
      "Epoch 473/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1773 - accuracy: 0.7396 - val_loss: 0.1866 - val_accuracy: 0.7391\n",
      "Epoch 474/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1754 - accuracy: 0.7440 - val_loss: 0.1866 - val_accuracy: 0.7385\n",
      "Epoch 475/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1753 - accuracy: 0.7450 - val_loss: 0.1872 - val_accuracy: 0.7378\n",
      "Epoch 476/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1793 - accuracy: 0.7361 - val_loss: 0.1877 - val_accuracy: 0.7375\n",
      "Epoch 477/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1772 - accuracy: 0.7407 - val_loss: 0.1877 - val_accuracy: 0.7379\n",
      "Epoch 478/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1747 - accuracy: 0.7471 - val_loss: 0.1866 - val_accuracy: 0.7375\n",
      "Epoch 479/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1741 - accuracy: 0.7493 - val_loss: 0.1860 - val_accuracy: 0.7380\n",
      "Epoch 480/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1778 - accuracy: 0.7414 - val_loss: 0.1871 - val_accuracy: 0.7378\n",
      "Epoch 481/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1778 - accuracy: 0.7384 - val_loss: 0.1871 - val_accuracy: 0.7380\n",
      "Epoch 482/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1776 - accuracy: 0.7357 - val_loss: 0.1879 - val_accuracy: 0.7284\n",
      "Epoch 483/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1758 - accuracy: 0.7409 - val_loss: 0.1871 - val_accuracy: 0.7314\n",
      "Epoch 484/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1760 - accuracy: 0.7429 - val_loss: 0.1877 - val_accuracy: 0.7350\n",
      "Epoch 485/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1750 - accuracy: 0.7444 - val_loss: 0.1870 - val_accuracy: 0.7363\n",
      "Epoch 486/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1768 - accuracy: 0.7410 - val_loss: 0.1867 - val_accuracy: 0.7368\n",
      "Epoch 487/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1770 - accuracy: 0.7392 - val_loss: 0.1879 - val_accuracy: 0.7365\n",
      "Epoch 488/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1787 - accuracy: 0.7368 - val_loss: 0.1877 - val_accuracy: 0.7362\n",
      "Epoch 489/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1753 - accuracy: 0.7417 - val_loss: 0.1856 - val_accuracy: 0.7381\n",
      "Epoch 490/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1761 - accuracy: 0.7434 - val_loss: 0.1891 - val_accuracy: 0.7366\n",
      "Epoch 491/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1751 - accuracy: 0.7443 - val_loss: 0.1879 - val_accuracy: 0.7376\n",
      "Epoch 492/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1766 - accuracy: 0.7400 - val_loss: 0.1881 - val_accuracy: 0.7368\n",
      "Epoch 493/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1749 - accuracy: 0.7439 - val_loss: 0.1877 - val_accuracy: 0.7370\n",
      "Epoch 494/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1779 - accuracy: 0.7387 - val_loss: 0.1878 - val_accuracy: 0.7368\n",
      "Epoch 495/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1763 - accuracy: 0.7418 - val_loss: 0.1879 - val_accuracy: 0.7361\n",
      "Epoch 496/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1726 - accuracy: 0.7494 - val_loss: 0.1884 - val_accuracy: 0.7363\n",
      "Epoch 497/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1771 - accuracy: 0.7394 - val_loss: 0.1863 - val_accuracy: 0.7376\n",
      "Epoch 498/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1771 - accuracy: 0.7413 - val_loss: 0.1878 - val_accuracy: 0.7372\n",
      "Epoch 499/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1776 - accuracy: 0.7394 - val_loss: 0.1876 - val_accuracy: 0.7374\n",
      "Epoch 500/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1757 - accuracy: 0.7449 - val_loss: 0.1870 - val_accuracy: 0.7376\n",
      "Epoch 501/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1786 - accuracy: 0.7414 - val_loss: 0.1865 - val_accuracy: 0.7389\n",
      "Epoch 502/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1758 - accuracy: 0.7425 - val_loss: 0.1850 - val_accuracy: 0.7371\n",
      "Epoch 503/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1770 - accuracy: 0.7374 - val_loss: 0.1860 - val_accuracy: 0.7378\n",
      "Epoch 504/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1763 - accuracy: 0.7426 - val_loss: 0.1853 - val_accuracy: 0.7387\n",
      "Epoch 505/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1765 - accuracy: 0.7419 - val_loss: 0.1854 - val_accuracy: 0.7385\n",
      "Epoch 506/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1772 - accuracy: 0.7412 - val_loss: 0.1855 - val_accuracy: 0.7385\n",
      "Epoch 507/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1754 - accuracy: 0.7428 - val_loss: 0.1855 - val_accuracy: 0.7385\n",
      "Epoch 508/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1762 - accuracy: 0.7419 - val_loss: 0.1864 - val_accuracy: 0.7375\n",
      "Epoch 509/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1746 - accuracy: 0.7455 - val_loss: 0.1858 - val_accuracy: 0.7372\n",
      "Epoch 510/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1774 - accuracy: 0.7408 - val_loss: 0.1869 - val_accuracy: 0.7375\n",
      "Epoch 511/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1780 - accuracy: 0.7361 - val_loss: 0.1864 - val_accuracy: 0.7354\n",
      "Epoch 512/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1776 - accuracy: 0.7375 - val_loss: 0.1860 - val_accuracy: 0.7375\n",
      "Epoch 513/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1756 - accuracy: 0.7426 - val_loss: 0.1880 - val_accuracy: 0.7357\n",
      "Epoch 514/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1743 - accuracy: 0.7447 - val_loss: 0.1864 - val_accuracy: 0.7365\n",
      "Epoch 515/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1759 - accuracy: 0.7425 - val_loss: 0.1863 - val_accuracy: 0.7361\n",
      "Epoch 516/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1773 - accuracy: 0.7388 - val_loss: 0.1864 - val_accuracy: 0.7367\n",
      "Epoch 517/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1753 - accuracy: 0.7451 - val_loss: 0.1861 - val_accuracy: 0.7359\n",
      "Epoch 518/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1773 - accuracy: 0.7388 - val_loss: 0.1869 - val_accuracy: 0.7361\n",
      "Epoch 519/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1732 - accuracy: 0.7470 - val_loss: 0.1863 - val_accuracy: 0.7378\n",
      "Epoch 520/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1772 - accuracy: 0.7407 - val_loss: 0.1856 - val_accuracy: 0.7384\n",
      "Epoch 521/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1756 - accuracy: 0.7443 - val_loss: 0.1869 - val_accuracy: 0.7361\n",
      "Epoch 522/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1765 - accuracy: 0.7407 - val_loss: 0.1866 - val_accuracy: 0.7358\n",
      "Epoch 523/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1788 - accuracy: 0.7364 - val_loss: 0.1857 - val_accuracy: 0.7375\n",
      "Epoch 524/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1743 - accuracy: 0.7449 - val_loss: 0.1857 - val_accuracy: 0.7358\n",
      "Epoch 525/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1775 - accuracy: 0.7394 - val_loss: 0.1858 - val_accuracy: 0.7367\n",
      "Epoch 526/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1780 - accuracy: 0.7369 - val_loss: 0.1862 - val_accuracy: 0.7365\n",
      "Epoch 527/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1776 - accuracy: 0.7356 - val_loss: 0.1871 - val_accuracy: 0.7361\n",
      "Epoch 528/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1760 - accuracy: 0.7417 - val_loss: 0.1864 - val_accuracy: 0.7379\n",
      "Epoch 529/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1764 - accuracy: 0.7393 - val_loss: 0.1851 - val_accuracy: 0.7385\n",
      "Epoch 530/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1770 - accuracy: 0.7419 - val_loss: 0.1853 - val_accuracy: 0.7379\n",
      "Epoch 531/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1751 - accuracy: 0.7413 - val_loss: 0.1868 - val_accuracy: 0.7368\n",
      "Epoch 532/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1734 - accuracy: 0.7452 - val_loss: 0.1878 - val_accuracy: 0.7365\n",
      "Epoch 533/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1751 - accuracy: 0.7455 - val_loss: 0.1879 - val_accuracy: 0.7370\n",
      "Epoch 534/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1781 - accuracy: 0.7384 - val_loss: 0.1865 - val_accuracy: 0.7375\n",
      "Epoch 535/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1765 - accuracy: 0.7443 - val_loss: 0.1860 - val_accuracy: 0.7384\n",
      "Epoch 536/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1744 - accuracy: 0.7471 - val_loss: 0.1854 - val_accuracy: 0.7383\n",
      "Epoch 537/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1740 - accuracy: 0.7463 - val_loss: 0.1874 - val_accuracy: 0.7362\n",
      "Epoch 538/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1737 - accuracy: 0.7481 - val_loss: 0.1862 - val_accuracy: 0.7389\n",
      "Epoch 539/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1764 - accuracy: 0.7407 - val_loss: 0.1862 - val_accuracy: 0.7391\n",
      "Epoch 540/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1727 - accuracy: 0.7520 - val_loss: 0.1861 - val_accuracy: 0.7388\n",
      "Epoch 541/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1765 - accuracy: 0.7408 - val_loss: 0.1864 - val_accuracy: 0.7388\n",
      "Epoch 542/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1798 - accuracy: 0.7373 - val_loss: 0.1855 - val_accuracy: 0.7392\n",
      "Epoch 543/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1738 - accuracy: 0.7475 - val_loss: 0.1859 - val_accuracy: 0.7389\n",
      "Epoch 544/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1753 - accuracy: 0.7436 - val_loss: 0.1857 - val_accuracy: 0.7388\n",
      "Epoch 545/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1766 - accuracy: 0.7414 - val_loss: 0.1861 - val_accuracy: 0.7388\n",
      "Epoch 546/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1761 - accuracy: 0.7394 - val_loss: 0.1862 - val_accuracy: 0.7380\n",
      "Epoch 547/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1776 - accuracy: 0.7423 - val_loss: 0.1866 - val_accuracy: 0.7323\n",
      "Epoch 548/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1747 - accuracy: 0.7432 - val_loss: 0.1869 - val_accuracy: 0.7375\n",
      "Epoch 549/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1781 - accuracy: 0.7368 - val_loss: 0.1867 - val_accuracy: 0.7326\n",
      "Epoch 550/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1773 - accuracy: 0.7406 - val_loss: 0.1864 - val_accuracy: 0.7380\n",
      "Epoch 551/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1752 - accuracy: 0.7454 - val_loss: 0.1869 - val_accuracy: 0.7376\n",
      "Epoch 552/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1756 - accuracy: 0.7442 - val_loss: 0.1866 - val_accuracy: 0.7317\n",
      "Epoch 553/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1752 - accuracy: 0.7423 - val_loss: 0.1869 - val_accuracy: 0.7381\n",
      "Epoch 554/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1778 - accuracy: 0.7383 - val_loss: 0.1872 - val_accuracy: 0.7376\n",
      "Epoch 555/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1728 - accuracy: 0.7460 - val_loss: 0.1870 - val_accuracy: 0.7375\n",
      "Epoch 556/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1759 - accuracy: 0.7443 - val_loss: 0.1865 - val_accuracy: 0.7396\n",
      "Epoch 557/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1764 - accuracy: 0.7437 - val_loss: 0.1874 - val_accuracy: 0.7383\n",
      "Epoch 558/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1756 - accuracy: 0.7455 - val_loss: 0.1867 - val_accuracy: 0.7368\n",
      "Epoch 559/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1747 - accuracy: 0.7439 - val_loss: 0.1868 - val_accuracy: 0.7379\n",
      "Epoch 560/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1769 - accuracy: 0.7425 - val_loss: 0.1870 - val_accuracy: 0.7374\n",
      "Epoch 561/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1771 - accuracy: 0.7411 - val_loss: 0.1869 - val_accuracy: 0.7372\n",
      "Epoch 562/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1788 - accuracy: 0.7366 - val_loss: 0.1874 - val_accuracy: 0.7368\n",
      "Epoch 563/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1767 - accuracy: 0.7408 - val_loss: 0.1871 - val_accuracy: 0.7376\n",
      "Epoch 564/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1763 - accuracy: 0.7424 - val_loss: 0.1868 - val_accuracy: 0.7371\n",
      "Epoch 565/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1773 - accuracy: 0.7418 - val_loss: 0.1870 - val_accuracy: 0.7375\n",
      "Epoch 566/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1749 - accuracy: 0.7451 - val_loss: 0.1875 - val_accuracy: 0.7372\n",
      "Epoch 567/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1758 - accuracy: 0.7431 - val_loss: 0.1871 - val_accuracy: 0.7372\n",
      "Epoch 568/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1758 - accuracy: 0.7413 - val_loss: 0.1865 - val_accuracy: 0.7391\n",
      "Epoch 569/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1755 - accuracy: 0.7433 - val_loss: 0.1882 - val_accuracy: 0.7372\n",
      "Epoch 570/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1763 - accuracy: 0.7422 - val_loss: 0.1868 - val_accuracy: 0.7378\n",
      "Epoch 571/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1776 - accuracy: 0.7409 - val_loss: 0.1871 - val_accuracy: 0.7371\n",
      "Epoch 572/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1776 - accuracy: 0.7409 - val_loss: 0.1871 - val_accuracy: 0.7374\n",
      "Epoch 573/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1769 - accuracy: 0.7420 - val_loss: 0.1873 - val_accuracy: 0.7376\n",
      "Epoch 574/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1824 - accuracy: 0.7298 - val_loss: 0.1871 - val_accuracy: 0.7379\n",
      "Epoch 575/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1769 - accuracy: 0.7421 - val_loss: 0.1865 - val_accuracy: 0.7379\n",
      "Epoch 576/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1755 - accuracy: 0.7434 - val_loss: 0.1873 - val_accuracy: 0.7379\n",
      "Epoch 577/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1773 - accuracy: 0.7381 - val_loss: 0.1897 - val_accuracy: 0.7348\n",
      "Epoch 578/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1774 - accuracy: 0.7410 - val_loss: 0.1880 - val_accuracy: 0.7367\n",
      "Epoch 579/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1774 - accuracy: 0.7397 - val_loss: 0.1879 - val_accuracy: 0.7362\n",
      "Epoch 580/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1805 - accuracy: 0.7364 - val_loss: 0.1883 - val_accuracy: 0.7353\n",
      "Epoch 581/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1784 - accuracy: 0.7362 - val_loss: 0.1882 - val_accuracy: 0.7359\n",
      "Epoch 582/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1758 - accuracy: 0.7436 - val_loss: 0.1881 - val_accuracy: 0.7362\n",
      "Epoch 583/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1764 - accuracy: 0.7439 - val_loss: 0.1880 - val_accuracy: 0.7357\n",
      "Epoch 584/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1744 - accuracy: 0.7455 - val_loss: 0.1886 - val_accuracy: 0.7357\n",
      "Epoch 585/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1769 - accuracy: 0.7415 - val_loss: 0.1885 - val_accuracy: 0.7356\n",
      "Epoch 586/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1767 - accuracy: 0.7409 - val_loss: 0.1883 - val_accuracy: 0.7354\n",
      "Epoch 587/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1799 - accuracy: 0.7383 - val_loss: 0.1877 - val_accuracy: 0.7331\n",
      "Epoch 588/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1743 - accuracy: 0.7506 - val_loss: 0.1895 - val_accuracy: 0.7366\n",
      "Epoch 589/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1785 - accuracy: 0.7414 - val_loss: 0.1883 - val_accuracy: 0.7337\n",
      "Epoch 590/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1780 - accuracy: 0.7391 - val_loss: 0.1881 - val_accuracy: 0.7354\n",
      "Epoch 591/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1775 - accuracy: 0.7391 - val_loss: 0.1884 - val_accuracy: 0.7359\n",
      "Epoch 592/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1751 - accuracy: 0.7452 - val_loss: 0.1880 - val_accuracy: 0.7357\n",
      "Epoch 593/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1774 - accuracy: 0.7405 - val_loss: 0.1871 - val_accuracy: 0.7372\n",
      "Epoch 594/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1752 - accuracy: 0.7451 - val_loss: 0.1871 - val_accuracy: 0.7361\n",
      "Epoch 595/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1743 - accuracy: 0.7458 - val_loss: 0.1881 - val_accuracy: 0.7356\n",
      "Epoch 596/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1761 - accuracy: 0.7434 - val_loss: 0.1874 - val_accuracy: 0.7378\n",
      "Epoch 597/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1771 - accuracy: 0.7435 - val_loss: 0.1868 - val_accuracy: 0.7379\n",
      "Epoch 598/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1751 - accuracy: 0.7417 - val_loss: 0.1873 - val_accuracy: 0.7380\n",
      "Epoch 599/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1772 - accuracy: 0.7431 - val_loss: 0.1873 - val_accuracy: 0.7370\n",
      "Epoch 600/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1773 - accuracy: 0.7405 - val_loss: 0.1877 - val_accuracy: 0.7368\n",
      "Epoch 601/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1760 - accuracy: 0.7390 - val_loss: 0.1874 - val_accuracy: 0.7370\n",
      "Epoch 602/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1781 - accuracy: 0.7378 - val_loss: 0.1862 - val_accuracy: 0.7391\n",
      "Epoch 603/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1783 - accuracy: 0.7386 - val_loss: 0.1877 - val_accuracy: 0.7380\n",
      "Epoch 604/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1778 - accuracy: 0.7403 - val_loss: 0.1879 - val_accuracy: 0.7378\n",
      "Epoch 605/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1764 - accuracy: 0.7416 - val_loss: 0.1881 - val_accuracy: 0.7380\n",
      "Epoch 606/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1777 - accuracy: 0.7393 - val_loss: 0.1872 - val_accuracy: 0.7375\n",
      "Epoch 607/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1752 - accuracy: 0.7447 - val_loss: 0.1879 - val_accuracy: 0.7368\n",
      "Epoch 608/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1779 - accuracy: 0.7385 - val_loss: 0.1885 - val_accuracy: 0.7363\n",
      "Epoch 609/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1766 - accuracy: 0.7419 - val_loss: 0.1911 - val_accuracy: 0.7334\n",
      "Epoch 610/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1754 - accuracy: 0.7492 - val_loss: 0.1881 - val_accuracy: 0.7372\n",
      "Epoch 611/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1782 - accuracy: 0.7387 - val_loss: 0.1882 - val_accuracy: 0.7363\n",
      "Epoch 612/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1798 - accuracy: 0.7355 - val_loss: 0.1888 - val_accuracy: 0.7362\n",
      "Epoch 613/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1791 - accuracy: 0.7377 - val_loss: 0.1887 - val_accuracy: 0.7361\n",
      "Epoch 614/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1768 - accuracy: 0.7439 - val_loss: 0.1884 - val_accuracy: 0.7341\n",
      "Epoch 615/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1773 - accuracy: 0.7414 - val_loss: 0.1891 - val_accuracy: 0.7341\n",
      "Epoch 616/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1775 - accuracy: 0.7398 - val_loss: 0.1886 - val_accuracy: 0.7361\n",
      "Epoch 617/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1785 - accuracy: 0.7370 - val_loss: 0.1884 - val_accuracy: 0.7359\n",
      "Epoch 618/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1765 - accuracy: 0.7422 - val_loss: 0.1892 - val_accuracy: 0.7352\n",
      "Epoch 619/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1764 - accuracy: 0.7416 - val_loss: 0.1880 - val_accuracy: 0.7365\n",
      "Epoch 620/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1763 - accuracy: 0.7436 - val_loss: 0.1885 - val_accuracy: 0.7368\n",
      "Epoch 621/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1778 - accuracy: 0.7384 - val_loss: 0.1884 - val_accuracy: 0.7313\n",
      "Epoch 622/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1754 - accuracy: 0.7422 - val_loss: 0.1890 - val_accuracy: 0.7359\n",
      "Epoch 623/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1772 - accuracy: 0.7380 - val_loss: 0.1891 - val_accuracy: 0.7349\n",
      "Epoch 624/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1780 - accuracy: 0.7390 - val_loss: 0.1890 - val_accuracy: 0.7353\n",
      "Epoch 625/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1754 - accuracy: 0.7449 - val_loss: 0.1888 - val_accuracy: 0.7350\n",
      "Epoch 626/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1787 - accuracy: 0.7401 - val_loss: 0.1898 - val_accuracy: 0.7358\n",
      "Epoch 627/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1764 - accuracy: 0.7443 - val_loss: 0.1889 - val_accuracy: 0.7359\n",
      "Epoch 628/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1744 - accuracy: 0.7493 - val_loss: 0.1890 - val_accuracy: 0.7322\n",
      "Epoch 629/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1757 - accuracy: 0.7443 - val_loss: 0.1888 - val_accuracy: 0.7344\n",
      "Epoch 630/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1749 - accuracy: 0.7458 - val_loss: 0.1887 - val_accuracy: 0.7357\n",
      "Epoch 631/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1758 - accuracy: 0.7452 - val_loss: 0.1881 - val_accuracy: 0.7362\n",
      "Epoch 632/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1754 - accuracy: 0.7449 - val_loss: 0.1896 - val_accuracy: 0.7359\n",
      "Epoch 633/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1785 - accuracy: 0.7394 - val_loss: 0.1891 - val_accuracy: 0.7365\n",
      "Epoch 634/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1771 - accuracy: 0.7409 - val_loss: 0.1897 - val_accuracy: 0.7324\n",
      "Epoch 635/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1785 - accuracy: 0.7373 - val_loss: 0.1887 - val_accuracy: 0.7363\n",
      "Epoch 636/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1772 - accuracy: 0.7430 - val_loss: 0.1886 - val_accuracy: 0.7368\n",
      "Epoch 637/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1768 - accuracy: 0.7423 - val_loss: 0.1885 - val_accuracy: 0.7367\n",
      "Epoch 638/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1778 - accuracy: 0.7412 - val_loss: 0.1885 - val_accuracy: 0.7366\n",
      "Epoch 639/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1773 - accuracy: 0.7399 - val_loss: 0.1885 - val_accuracy: 0.7299\n",
      "Epoch 640/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1759 - accuracy: 0.7419 - val_loss: 0.1878 - val_accuracy: 0.7370\n",
      "Epoch 641/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1777 - accuracy: 0.7415 - val_loss: 0.1885 - val_accuracy: 0.7361\n",
      "Epoch 642/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1761 - accuracy: 0.7438 - val_loss: 0.1892 - val_accuracy: 0.7302\n",
      "Epoch 643/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1776 - accuracy: 0.7374 - val_loss: 0.1887 - val_accuracy: 0.7367\n",
      "Epoch 644/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1806 - accuracy: 0.7381 - val_loss: 0.1892 - val_accuracy: 0.7357\n",
      "Epoch 645/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1777 - accuracy: 0.7416 - val_loss: 0.1897 - val_accuracy: 0.7291\n",
      "Epoch 646/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1769 - accuracy: 0.7434 - val_loss: 0.1893 - val_accuracy: 0.7324\n",
      "Epoch 647/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1787 - accuracy: 0.7396 - val_loss: 0.1892 - val_accuracy: 0.7353\n",
      "Epoch 648/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1770 - accuracy: 0.7418 - val_loss: 0.1891 - val_accuracy: 0.7363\n",
      "Epoch 649/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1775 - accuracy: 0.7406 - val_loss: 0.1884 - val_accuracy: 0.7372\n",
      "Epoch 650/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1760 - accuracy: 0.7428 - val_loss: 0.1886 - val_accuracy: 0.7374\n",
      "Epoch 651/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1803 - accuracy: 0.7357 - val_loss: 0.1881 - val_accuracy: 0.7367\n",
      "Epoch 652/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1776 - accuracy: 0.7412 - val_loss: 0.1891 - val_accuracy: 0.7309\n",
      "Epoch 653/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1781 - accuracy: 0.7402 - val_loss: 0.1885 - val_accuracy: 0.7371\n",
      "Epoch 654/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1799 - accuracy: 0.7347 - val_loss: 0.1884 - val_accuracy: 0.7366\n",
      "Epoch 655/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1765 - accuracy: 0.7420 - val_loss: 0.1880 - val_accuracy: 0.7348\n",
      "Epoch 656/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1809 - accuracy: 0.7347 - val_loss: 0.1866 - val_accuracy: 0.7372\n",
      "Epoch 657/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1761 - accuracy: 0.7435 - val_loss: 0.1872 - val_accuracy: 0.7363\n",
      "Epoch 658/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1762 - accuracy: 0.7439 - val_loss: 0.1929 - val_accuracy: 0.7289\n",
      "Epoch 659/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1844 - accuracy: 0.7333 - val_loss: 0.1913 - val_accuracy: 0.7343\n",
      "Epoch 660/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1850 - accuracy: 0.7303 - val_loss: 0.1902 - val_accuracy: 0.7349\n",
      "Epoch 661/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1826 - accuracy: 0.7366 - val_loss: 0.1912 - val_accuracy: 0.7311\n",
      "Epoch 662/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1803 - accuracy: 0.7387 - val_loss: 0.1862 - val_accuracy: 0.7375\n",
      "Epoch 663/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1788 - accuracy: 0.7400 - val_loss: 0.1866 - val_accuracy: 0.7366\n",
      "Epoch 664/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1789 - accuracy: 0.7380 - val_loss: 0.1862 - val_accuracy: 0.7348\n",
      "Epoch 665/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1768 - accuracy: 0.7409 - val_loss: 0.1869 - val_accuracy: 0.7359\n",
      "Epoch 666/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1786 - accuracy: 0.7402 - val_loss: 0.1869 - val_accuracy: 0.7372\n",
      "Epoch 667/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1786 - accuracy: 0.7414 - val_loss: 0.1871 - val_accuracy: 0.7372\n",
      "Epoch 668/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1746 - accuracy: 0.7442 - val_loss: 0.1880 - val_accuracy: 0.7374\n",
      "Epoch 669/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1773 - accuracy: 0.7411 - val_loss: 0.1874 - val_accuracy: 0.7375\n",
      "Epoch 670/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1775 - accuracy: 0.7411 - val_loss: 0.1874 - val_accuracy: 0.7367\n",
      "Epoch 671/1000\n",
      "563/563 [==============================] - ETA: 0s - loss: 0.1788 - accuracy: 0.73 - 1s 1ms/step - loss: 0.1788 - accuracy: 0.7373 - val_loss: 0.1868 - val_accuracy: 0.7383\n",
      "Epoch 672/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1765 - accuracy: 0.7442 - val_loss: 0.1881 - val_accuracy: 0.7359\n",
      "Epoch 673/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1789 - accuracy: 0.7368 - val_loss: 0.1867 - val_accuracy: 0.7376\n",
      "Epoch 674/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1757 - accuracy: 0.7448 - val_loss: 0.1869 - val_accuracy: 0.7376\n",
      "Epoch 675/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1743 - accuracy: 0.7465 - val_loss: 0.1875 - val_accuracy: 0.7383\n",
      "Epoch 676/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1782 - accuracy: 0.7391 - val_loss: 0.1870 - val_accuracy: 0.7378\n",
      "Epoch 677/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1759 - accuracy: 0.7430 - val_loss: 0.1867 - val_accuracy: 0.7370\n",
      "Epoch 678/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1752 - accuracy: 0.7471 - val_loss: 0.1871 - val_accuracy: 0.7376\n",
      "Epoch 679/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1768 - accuracy: 0.7423 - val_loss: 0.1875 - val_accuracy: 0.7367\n",
      "Epoch 680/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1751 - accuracy: 0.7447 - val_loss: 0.1868 - val_accuracy: 0.7374\n",
      "Epoch 681/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1768 - accuracy: 0.7413 - val_loss: 0.1871 - val_accuracy: 0.7350\n",
      "Epoch 682/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1769 - accuracy: 0.7428 - val_loss: 0.1869 - val_accuracy: 0.7363\n",
      "Epoch 683/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1756 - accuracy: 0.7447 - val_loss: 0.1871 - val_accuracy: 0.7380\n",
      "Epoch 684/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1747 - accuracy: 0.7450 - val_loss: 0.1879 - val_accuracy: 0.7368\n",
      "Epoch 685/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1761 - accuracy: 0.7463 - val_loss: 0.1872 - val_accuracy: 0.7376\n",
      "Epoch 686/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1758 - accuracy: 0.7444 - val_loss: 0.1865 - val_accuracy: 0.7379\n",
      "Epoch 687/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1777 - accuracy: 0.7433 - val_loss: 0.1874 - val_accuracy: 0.7381\n",
      "Epoch 688/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1772 - accuracy: 0.7402 - val_loss: 0.1871 - val_accuracy: 0.7379\n",
      "Epoch 689/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1754 - accuracy: 0.7447 - val_loss: 0.1862 - val_accuracy: 0.7383\n",
      "Epoch 690/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1776 - accuracy: 0.7403 - val_loss: 0.1862 - val_accuracy: 0.7381\n",
      "Epoch 691/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1782 - accuracy: 0.7402 - val_loss: 0.1867 - val_accuracy: 0.7383\n",
      "Epoch 692/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1752 - accuracy: 0.7470 - val_loss: 0.1865 - val_accuracy: 0.7385\n",
      "Epoch 693/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1787 - accuracy: 0.7355 - val_loss: 0.1862 - val_accuracy: 0.7372\n",
      "Epoch 694/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1777 - accuracy: 0.7394 - val_loss: 0.1866 - val_accuracy: 0.7378\n",
      "Epoch 695/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1785 - accuracy: 0.7385 - val_loss: 0.1864 - val_accuracy: 0.7385\n",
      "Epoch 696/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1765 - accuracy: 0.7424 - val_loss: 0.1865 - val_accuracy: 0.7383\n",
      "Epoch 697/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1748 - accuracy: 0.7452 - val_loss: 0.1867 - val_accuracy: 0.7380\n",
      "Epoch 698/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1767 - accuracy: 0.7416 - val_loss: 0.1867 - val_accuracy: 0.7381\n",
      "Epoch 699/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1741 - accuracy: 0.7457 - val_loss: 0.1863 - val_accuracy: 0.7387\n",
      "Epoch 700/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1734 - accuracy: 0.7475 - val_loss: 0.1862 - val_accuracy: 0.7385\n",
      "Epoch 701/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1787 - accuracy: 0.7376 - val_loss: 0.1856 - val_accuracy: 0.7387\n",
      "Epoch 702/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1743 - accuracy: 0.7426 - val_loss: 0.1854 - val_accuracy: 0.7380\n",
      "Epoch 703/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1760 - accuracy: 0.7414 - val_loss: 0.1868 - val_accuracy: 0.7376\n",
      "Epoch 704/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1760 - accuracy: 0.7439 - val_loss: 0.1869 - val_accuracy: 0.7379\n",
      "Epoch 705/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1804 - accuracy: 0.7362 - val_loss: 0.1874 - val_accuracy: 0.7357\n",
      "Epoch 706/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1761 - accuracy: 0.7404 - val_loss: 0.1869 - val_accuracy: 0.7301\n",
      "Epoch 707/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1763 - accuracy: 0.7396 - val_loss: 0.1870 - val_accuracy: 0.7314\n",
      "Epoch 708/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1754 - accuracy: 0.7438 - val_loss: 0.1882 - val_accuracy: 0.7308\n",
      "Epoch 709/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1786 - accuracy: 0.7356 - val_loss: 0.1870 - val_accuracy: 0.7381\n",
      "Epoch 710/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1763 - accuracy: 0.7417 - val_loss: 0.1878 - val_accuracy: 0.7345\n",
      "Epoch 711/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1776 - accuracy: 0.7405 - val_loss: 0.1870 - val_accuracy: 0.7343\n",
      "Epoch 712/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1755 - accuracy: 0.7417 - val_loss: 0.1874 - val_accuracy: 0.7356\n",
      "Epoch 713/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1805 - accuracy: 0.7350 - val_loss: 0.1856 - val_accuracy: 0.7384\n",
      "Epoch 714/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1776 - accuracy: 0.7388 - val_loss: 0.1880 - val_accuracy: 0.7371\n",
      "Epoch 715/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1757 - accuracy: 0.7454 - val_loss: 0.1870 - val_accuracy: 0.7380\n",
      "Epoch 716/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1766 - accuracy: 0.7428 - val_loss: 0.1865 - val_accuracy: 0.7378\n",
      "Epoch 717/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1784 - accuracy: 0.7372 - val_loss: 0.1879 - val_accuracy: 0.7368\n",
      "Epoch 718/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1771 - accuracy: 0.7402 - val_loss: 0.1888 - val_accuracy: 0.7362\n",
      "Epoch 719/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1749 - accuracy: 0.7460 - val_loss: 0.1888 - val_accuracy: 0.7344\n",
      "Epoch 720/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1766 - accuracy: 0.7407 - val_loss: 0.1879 - val_accuracy: 0.7367\n",
      "Epoch 721/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1752 - accuracy: 0.7479 - val_loss: 0.1882 - val_accuracy: 0.7371\n",
      "Epoch 722/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1759 - accuracy: 0.7431 - val_loss: 0.1883 - val_accuracy: 0.7350\n",
      "Epoch 723/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1788 - accuracy: 0.7382 - val_loss: 0.1863 - val_accuracy: 0.7368\n",
      "Epoch 724/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1759 - accuracy: 0.7442 - val_loss: 0.1860 - val_accuracy: 0.7387\n",
      "Epoch 725/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1737 - accuracy: 0.7451 - val_loss: 0.1869 - val_accuracy: 0.7376\n",
      "Epoch 726/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1759 - accuracy: 0.7431 - val_loss: 0.1881 - val_accuracy: 0.7350\n",
      "Epoch 727/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1757 - accuracy: 0.7407 - val_loss: 0.1884 - val_accuracy: 0.7348\n",
      "Epoch 728/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1768 - accuracy: 0.7419 - val_loss: 0.1909 - val_accuracy: 0.7323\n",
      "Epoch 729/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1815 - accuracy: 0.7346 - val_loss: 0.1873 - val_accuracy: 0.7365\n",
      "Epoch 730/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1785 - accuracy: 0.7412 - val_loss: 0.1874 - val_accuracy: 0.7375\n",
      "Epoch 731/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1741 - accuracy: 0.7468 - val_loss: 0.1874 - val_accuracy: 0.7365\n",
      "Epoch 732/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1757 - accuracy: 0.7431 - val_loss: 0.1872 - val_accuracy: 0.7374\n",
      "Epoch 733/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1762 - accuracy: 0.7439 - val_loss: 0.1868 - val_accuracy: 0.7378\n",
      "Epoch 734/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1762 - accuracy: 0.7429 - val_loss: 0.1868 - val_accuracy: 0.7379\n",
      "Epoch 735/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1728 - accuracy: 0.7499 - val_loss: 0.1882 - val_accuracy: 0.7383\n",
      "Epoch 736/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1776 - accuracy: 0.7390 - val_loss: 0.1858 - val_accuracy: 0.7387\n",
      "Epoch 737/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1749 - accuracy: 0.7445 - val_loss: 0.1857 - val_accuracy: 0.7389\n",
      "Epoch 738/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1758 - accuracy: 0.7434 - val_loss: 0.1866 - val_accuracy: 0.7378\n",
      "Epoch 739/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1753 - accuracy: 0.7429 - val_loss: 0.1874 - val_accuracy: 0.7388\n",
      "Epoch 740/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1761 - accuracy: 0.7428 - val_loss: 0.1864 - val_accuracy: 0.7392\n",
      "Epoch 741/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1750 - accuracy: 0.7461 - val_loss: 0.1863 - val_accuracy: 0.7380\n",
      "Epoch 742/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1734 - accuracy: 0.7478 - val_loss: 0.1863 - val_accuracy: 0.7388\n",
      "Epoch 743/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1753 - accuracy: 0.7430 - val_loss: 0.1866 - val_accuracy: 0.7385\n",
      "Epoch 744/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1735 - accuracy: 0.7499 - val_loss: 0.1866 - val_accuracy: 0.7380\n",
      "Epoch 745/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1750 - accuracy: 0.7464 - val_loss: 0.1879 - val_accuracy: 0.7363\n",
      "Epoch 746/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1736 - accuracy: 0.7463 - val_loss: 0.1880 - val_accuracy: 0.7368\n",
      "Epoch 747/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1747 - accuracy: 0.7449 - val_loss: 0.1879 - val_accuracy: 0.7361\n",
      "Epoch 748/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1765 - accuracy: 0.7448 - val_loss: 0.1873 - val_accuracy: 0.7367\n",
      "Epoch 749/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1758 - accuracy: 0.7444 - val_loss: 0.1869 - val_accuracy: 0.7376\n",
      "Epoch 750/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1776 - accuracy: 0.7428 - val_loss: 0.1870 - val_accuracy: 0.7374\n",
      "Epoch 751/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1760 - accuracy: 0.7408 - val_loss: 0.1866 - val_accuracy: 0.7374\n",
      "Epoch 752/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1743 - accuracy: 0.7475 - val_loss: 0.1865 - val_accuracy: 0.7378\n",
      "Epoch 753/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1754 - accuracy: 0.7424 - val_loss: 0.1867 - val_accuracy: 0.7362\n",
      "Epoch 754/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1744 - accuracy: 0.7450 - val_loss: 0.1863 - val_accuracy: 0.7374\n",
      "Epoch 755/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1752 - accuracy: 0.7475 - val_loss: 0.1863 - val_accuracy: 0.7374\n",
      "Epoch 756/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1773 - accuracy: 0.7365 - val_loss: 0.1868 - val_accuracy: 0.7370\n",
      "Epoch 757/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1765 - accuracy: 0.7391 - val_loss: 0.1875 - val_accuracy: 0.7362\n",
      "Epoch 758/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1779 - accuracy: 0.7381 - val_loss: 0.1862 - val_accuracy: 0.7384\n",
      "Epoch 759/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1765 - accuracy: 0.7404 - val_loss: 0.1858 - val_accuracy: 0.7380\n",
      "Epoch 760/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1741 - accuracy: 0.7461 - val_loss: 0.1867 - val_accuracy: 0.7380\n",
      "Epoch 761/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1759 - accuracy: 0.7435 - val_loss: 0.1875 - val_accuracy: 0.7372\n",
      "Epoch 762/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1726 - accuracy: 0.7480 - val_loss: 0.1869 - val_accuracy: 0.7374\n",
      "Epoch 763/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1778 - accuracy: 0.7399 - val_loss: 0.1864 - val_accuracy: 0.7381\n",
      "Epoch 764/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1761 - accuracy: 0.7441 - val_loss: 0.1863 - val_accuracy: 0.7384\n",
      "Epoch 765/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1769 - accuracy: 0.7414 - val_loss: 0.1878 - val_accuracy: 0.7361\n",
      "Epoch 766/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1750 - accuracy: 0.7469 - val_loss: 0.1868 - val_accuracy: 0.7370\n",
      "Epoch 767/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1756 - accuracy: 0.7431 - val_loss: 0.1863 - val_accuracy: 0.7375\n",
      "Epoch 768/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1779 - accuracy: 0.7389 - val_loss: 0.1863 - val_accuracy: 0.7378\n",
      "Epoch 769/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1757 - accuracy: 0.7444 - val_loss: 0.1871 - val_accuracy: 0.7372\n",
      "Epoch 770/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1735 - accuracy: 0.7493 - val_loss: 0.1873 - val_accuracy: 0.7374\n",
      "Epoch 771/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1750 - accuracy: 0.7444 - val_loss: 0.1876 - val_accuracy: 0.7375\n",
      "Epoch 772/1000\n",
      "563/563 [==============================] - ETA: 0s - loss: 0.1769 - accuracy: 0.73 - 1s 1ms/step - loss: 0.1769 - accuracy: 0.7378 - val_loss: 0.1868 - val_accuracy: 0.7376\n",
      "Epoch 773/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1757 - accuracy: 0.7428 - val_loss: 0.1881 - val_accuracy: 0.7372\n",
      "Epoch 774/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1743 - accuracy: 0.7460 - val_loss: 0.1876 - val_accuracy: 0.7370\n",
      "Epoch 775/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1761 - accuracy: 0.7418 - val_loss: 0.1867 - val_accuracy: 0.7370\n",
      "Epoch 776/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1748 - accuracy: 0.7409 - val_loss: 0.1850 - val_accuracy: 0.7389\n",
      "Epoch 777/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1723 - accuracy: 0.7519 - val_loss: 0.1866 - val_accuracy: 0.7374\n",
      "Epoch 778/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1744 - accuracy: 0.7487 - val_loss: 0.1870 - val_accuracy: 0.7376\n",
      "Epoch 779/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1754 - accuracy: 0.7448 - val_loss: 0.1873 - val_accuracy: 0.7381\n",
      "Epoch 780/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1747 - accuracy: 0.7486 - val_loss: 0.1870 - val_accuracy: 0.7372\n",
      "Epoch 781/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1775 - accuracy: 0.7422 - val_loss: 0.1878 - val_accuracy: 0.7379\n",
      "Epoch 782/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1755 - accuracy: 0.7443 - val_loss: 0.1865 - val_accuracy: 0.7379\n",
      "Epoch 783/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1781 - accuracy: 0.7377 - val_loss: 0.1882 - val_accuracy: 0.7383\n",
      "Epoch 784/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1740 - accuracy: 0.7464 - val_loss: 0.1873 - val_accuracy: 0.7384\n",
      "Epoch 785/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1755 - accuracy: 0.7470 - val_loss: 0.1856 - val_accuracy: 0.7401\n",
      "Epoch 786/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1771 - accuracy: 0.7407 - val_loss: 0.1898 - val_accuracy: 0.7362\n",
      "Epoch 787/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1757 - accuracy: 0.7439 - val_loss: 0.1888 - val_accuracy: 0.7375\n",
      "Epoch 788/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1779 - accuracy: 0.7421 - val_loss: 0.1894 - val_accuracy: 0.7370\n",
      "Epoch 789/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1780 - accuracy: 0.7415 - val_loss: 0.1889 - val_accuracy: 0.7383\n",
      "Epoch 790/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1771 - accuracy: 0.7409 - val_loss: 0.1876 - val_accuracy: 0.7387\n",
      "Epoch 791/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1783 - accuracy: 0.7404 - val_loss: 0.1881 - val_accuracy: 0.7376\n",
      "Epoch 792/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1765 - accuracy: 0.7416 - val_loss: 0.1876 - val_accuracy: 0.7389\n",
      "Epoch 793/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1754 - accuracy: 0.7467 - val_loss: 0.1881 - val_accuracy: 0.7383\n",
      "Epoch 794/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1759 - accuracy: 0.7448 - val_loss: 0.1871 - val_accuracy: 0.7387\n",
      "Epoch 795/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1755 - accuracy: 0.7459 - val_loss: 0.1872 - val_accuracy: 0.7387\n",
      "Epoch 796/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1767 - accuracy: 0.7416 - val_loss: 0.1871 - val_accuracy: 0.7375\n",
      "Epoch 797/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1770 - accuracy: 0.7441 - val_loss: 0.1872 - val_accuracy: 0.7392\n",
      "Epoch 798/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1776 - accuracy: 0.7416 - val_loss: 0.1871 - val_accuracy: 0.7388\n",
      "Epoch 799/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1794 - accuracy: 0.7374 - val_loss: 0.1867 - val_accuracy: 0.7380\n",
      "Epoch 800/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1777 - accuracy: 0.7432 - val_loss: 0.1863 - val_accuracy: 0.7394\n",
      "Epoch 801/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1758 - accuracy: 0.7476 - val_loss: 0.1862 - val_accuracy: 0.7392\n",
      "Epoch 802/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1751 - accuracy: 0.7444 - val_loss: 0.1864 - val_accuracy: 0.7396\n",
      "Epoch 803/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1740 - accuracy: 0.7511 - val_loss: 0.1867 - val_accuracy: 0.7394\n",
      "Epoch 804/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1759 - accuracy: 0.7452 - val_loss: 0.1863 - val_accuracy: 0.7388\n",
      "Epoch 805/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1772 - accuracy: 0.7401 - val_loss: 0.1862 - val_accuracy: 0.7388\n",
      "Epoch 806/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1775 - accuracy: 0.7426 - val_loss: 0.1869 - val_accuracy: 0.7394\n",
      "Epoch 807/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1750 - accuracy: 0.7432 - val_loss: 0.1871 - val_accuracy: 0.7384\n",
      "Epoch 808/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1765 - accuracy: 0.7409 - val_loss: 0.1878 - val_accuracy: 0.7371\n",
      "Epoch 809/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1777 - accuracy: 0.7399 - val_loss: 0.1878 - val_accuracy: 0.7358\n",
      "Epoch 810/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1771 - accuracy: 0.7428 - val_loss: 0.1878 - val_accuracy: 0.7363\n",
      "Epoch 811/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1777 - accuracy: 0.7402 - val_loss: 0.1872 - val_accuracy: 0.7366\n",
      "Epoch 812/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1761 - accuracy: 0.7460 - val_loss: 0.1878 - val_accuracy: 0.7367\n",
      "Epoch 813/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1762 - accuracy: 0.7435 - val_loss: 0.1880 - val_accuracy: 0.7367\n",
      "Epoch 814/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1774 - accuracy: 0.7438 - val_loss: 0.1900 - val_accuracy: 0.7365\n",
      "Epoch 815/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1746 - accuracy: 0.7489 - val_loss: 0.1866 - val_accuracy: 0.7370\n",
      "Epoch 816/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1786 - accuracy: 0.7397 - val_loss: 0.1868 - val_accuracy: 0.7374\n",
      "Epoch 817/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1768 - accuracy: 0.7409 - val_loss: 0.1875 - val_accuracy: 0.7366\n",
      "Epoch 818/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1754 - accuracy: 0.7450 - val_loss: 0.1876 - val_accuracy: 0.7348\n",
      "Epoch 819/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1768 - accuracy: 0.7424 - val_loss: 0.1875 - val_accuracy: 0.7370\n",
      "Epoch 820/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1737 - accuracy: 0.7454 - val_loss: 0.1860 - val_accuracy: 0.7381\n",
      "Epoch 821/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1775 - accuracy: 0.7426 - val_loss: 0.1873 - val_accuracy: 0.7363\n",
      "Epoch 822/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1791 - accuracy: 0.7378 - val_loss: 0.1877 - val_accuracy: 0.7362\n",
      "Epoch 823/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1757 - accuracy: 0.7424 - val_loss: 0.1876 - val_accuracy: 0.7379\n",
      "Epoch 824/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1778 - accuracy: 0.7421 - val_loss: 0.1856 - val_accuracy: 0.7393\n",
      "Epoch 825/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1746 - accuracy: 0.7459 - val_loss: 0.1859 - val_accuracy: 0.7384\n",
      "Epoch 826/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1746 - accuracy: 0.7447 - val_loss: 0.1871 - val_accuracy: 0.7372\n",
      "Epoch 827/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1761 - accuracy: 0.7455 - val_loss: 0.1884 - val_accuracy: 0.7366\n",
      "Epoch 828/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1745 - accuracy: 0.7457 - val_loss: 0.1878 - val_accuracy: 0.7376\n",
      "Epoch 829/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1748 - accuracy: 0.7447 - val_loss: 0.1861 - val_accuracy: 0.7393\n",
      "Epoch 830/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1756 - accuracy: 0.7410 - val_loss: 0.1874 - val_accuracy: 0.7378\n",
      "Epoch 831/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1765 - accuracy: 0.7430 - val_loss: 0.1876 - val_accuracy: 0.7385\n",
      "Epoch 832/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1769 - accuracy: 0.7401 - val_loss: 0.1870 - val_accuracy: 0.7381\n",
      "Epoch 833/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1781 - accuracy: 0.7392 - val_loss: 0.1867 - val_accuracy: 0.7368\n",
      "Epoch 834/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1733 - accuracy: 0.7495 - val_loss: 0.1861 - val_accuracy: 0.7383\n",
      "Epoch 835/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1758 - accuracy: 0.7422 - val_loss: 0.1868 - val_accuracy: 0.7374\n",
      "Epoch 836/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1783 - accuracy: 0.7401 - val_loss: 0.1869 - val_accuracy: 0.7379\n",
      "Epoch 837/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1735 - accuracy: 0.7498 - val_loss: 0.1878 - val_accuracy: 0.7362\n",
      "Epoch 838/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1744 - accuracy: 0.7451 - val_loss: 0.1872 - val_accuracy: 0.7381\n",
      "Epoch 839/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1767 - accuracy: 0.7415 - val_loss: 0.1865 - val_accuracy: 0.7394\n",
      "Epoch 840/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1795 - accuracy: 0.7376 - val_loss: 0.1868 - val_accuracy: 0.7384\n",
      "Epoch 841/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1748 - accuracy: 0.7453 - val_loss: 0.1871 - val_accuracy: 0.7379\n",
      "Epoch 842/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1742 - accuracy: 0.7452 - val_loss: 0.1869 - val_accuracy: 0.7375\n",
      "Epoch 843/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1746 - accuracy: 0.7444 - val_loss: 0.1879 - val_accuracy: 0.7370\n",
      "Epoch 844/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1757 - accuracy: 0.7442 - val_loss: 0.1877 - val_accuracy: 0.7370\n",
      "Epoch 845/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1750 - accuracy: 0.7493 - val_loss: 0.1879 - val_accuracy: 0.7378\n",
      "Epoch 846/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1739 - accuracy: 0.7477 - val_loss: 0.1875 - val_accuracy: 0.7367\n",
      "Epoch 847/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1756 - accuracy: 0.7450 - val_loss: 0.1867 - val_accuracy: 0.7378\n",
      "Epoch 848/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1773 - accuracy: 0.7401 - val_loss: 0.1902 - val_accuracy: 0.7350\n",
      "Epoch 849/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1775 - accuracy: 0.7409 - val_loss: 0.1900 - val_accuracy: 0.7346\n",
      "Epoch 850/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1777 - accuracy: 0.7440 - val_loss: 0.1891 - val_accuracy: 0.7357\n",
      "Epoch 851/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1785 - accuracy: 0.7393 - val_loss: 0.1887 - val_accuracy: 0.7365\n",
      "Epoch 852/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1742 - accuracy: 0.7452 - val_loss: 0.1895 - val_accuracy: 0.7361\n",
      "Epoch 853/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1754 - accuracy: 0.7467 - val_loss: 0.1884 - val_accuracy: 0.7361\n",
      "Epoch 854/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1781 - accuracy: 0.7397 - val_loss: 0.1885 - val_accuracy: 0.7368\n",
      "Epoch 855/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1763 - accuracy: 0.7448 - val_loss: 0.1883 - val_accuracy: 0.7367\n",
      "Epoch 856/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1768 - accuracy: 0.7432 - val_loss: 0.1891 - val_accuracy: 0.7352\n",
      "Epoch 857/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1777 - accuracy: 0.7410 - val_loss: 0.1878 - val_accuracy: 0.7371\n",
      "Epoch 858/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1760 - accuracy: 0.7445 - val_loss: 0.1886 - val_accuracy: 0.7366\n",
      "Epoch 859/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1795 - accuracy: 0.7389 - val_loss: 0.1883 - val_accuracy: 0.7365\n",
      "Epoch 860/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1779 - accuracy: 0.7410 - val_loss: 0.1880 - val_accuracy: 0.7366\n",
      "Epoch 861/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1761 - accuracy: 0.7440 - val_loss: 0.1882 - val_accuracy: 0.7365\n",
      "Epoch 862/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1758 - accuracy: 0.7451 - val_loss: 0.1892 - val_accuracy: 0.7352\n",
      "Epoch 863/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1771 - accuracy: 0.7418 - val_loss: 0.1882 - val_accuracy: 0.7371\n",
      "Epoch 864/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1771 - accuracy: 0.7439 - val_loss: 0.1884 - val_accuracy: 0.7367\n",
      "Epoch 865/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1764 - accuracy: 0.7438 - val_loss: 0.1896 - val_accuracy: 0.7362\n",
      "Epoch 866/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1744 - accuracy: 0.7469 - val_loss: 0.1882 - val_accuracy: 0.7368\n",
      "Epoch 867/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1793 - accuracy: 0.7390 - val_loss: 0.1886 - val_accuracy: 0.7361\n",
      "Epoch 868/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1771 - accuracy: 0.7437 - val_loss: 0.1887 - val_accuracy: 0.7372\n",
      "Epoch 869/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1776 - accuracy: 0.7418 - val_loss: 0.1883 - val_accuracy: 0.7372\n",
      "Epoch 870/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1757 - accuracy: 0.7471 - val_loss: 0.1902 - val_accuracy: 0.7344\n",
      "Epoch 871/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1809 - accuracy: 0.7368 - val_loss: 0.1888 - val_accuracy: 0.7361\n",
      "Epoch 872/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1763 - accuracy: 0.7434 - val_loss: 0.1876 - val_accuracy: 0.7383\n",
      "Epoch 873/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1810 - accuracy: 0.7375 - val_loss: 0.1884 - val_accuracy: 0.7366\n",
      "Epoch 874/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1786 - accuracy: 0.7402 - val_loss: 0.1896 - val_accuracy: 0.7340\n",
      "Epoch 875/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1825 - accuracy: 0.7377 - val_loss: 0.1870 - val_accuracy: 0.7308\n",
      "Epoch 876/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1755 - accuracy: 0.7407 - val_loss: 0.1863 - val_accuracy: 0.7381\n",
      "Epoch 877/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1764 - accuracy: 0.7406 - val_loss: 0.1864 - val_accuracy: 0.7379\n",
      "Epoch 878/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1762 - accuracy: 0.7456 - val_loss: 0.1862 - val_accuracy: 0.7383\n",
      "Epoch 879/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1734 - accuracy: 0.7478 - val_loss: 0.1865 - val_accuracy: 0.7384\n",
      "Epoch 880/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1754 - accuracy: 0.7423 - val_loss: 0.1868 - val_accuracy: 0.7379\n",
      "Epoch 881/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1758 - accuracy: 0.7434 - val_loss: 0.1864 - val_accuracy: 0.7381\n",
      "Epoch 882/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1780 - accuracy: 0.7413 - val_loss: 0.1876 - val_accuracy: 0.7361\n",
      "Epoch 883/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1783 - accuracy: 0.7402 - val_loss: 0.1875 - val_accuracy: 0.7366\n",
      "Epoch 884/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1743 - accuracy: 0.7474 - val_loss: 0.1863 - val_accuracy: 0.7381\n",
      "Epoch 885/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1750 - accuracy: 0.7424 - val_loss: 0.1865 - val_accuracy: 0.7378\n",
      "Epoch 886/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1760 - accuracy: 0.7384 - val_loss: 0.1861 - val_accuracy: 0.7378\n",
      "Epoch 887/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1767 - accuracy: 0.7424 - val_loss: 0.1864 - val_accuracy: 0.7380\n",
      "Epoch 888/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1751 - accuracy: 0.7450 - val_loss: 0.1864 - val_accuracy: 0.7383\n",
      "Epoch 889/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1777 - accuracy: 0.7407 - val_loss: 0.1864 - val_accuracy: 0.7393\n",
      "Epoch 890/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1780 - accuracy: 0.7399 - val_loss: 0.1860 - val_accuracy: 0.7383\n",
      "Epoch 891/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1785 - accuracy: 0.7368 - val_loss: 0.1858 - val_accuracy: 0.7392\n",
      "Epoch 892/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1751 - accuracy: 0.7454 - val_loss: 0.1855 - val_accuracy: 0.7393\n",
      "Epoch 893/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1755 - accuracy: 0.7411 - val_loss: 0.1853 - val_accuracy: 0.7411\n",
      "Epoch 894/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1760 - accuracy: 0.7451 - val_loss: 0.1865 - val_accuracy: 0.7389\n",
      "Epoch 895/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1778 - accuracy: 0.7397 - val_loss: 0.1862 - val_accuracy: 0.7384\n",
      "Epoch 896/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1754 - accuracy: 0.7452 - val_loss: 0.1868 - val_accuracy: 0.7384\n",
      "Epoch 897/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1787 - accuracy: 0.7377 - val_loss: 0.1919 - val_accuracy: 0.7332\n",
      "Epoch 898/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1805 - accuracy: 0.7384 - val_loss: 0.1911 - val_accuracy: 0.7358\n",
      "Epoch 899/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1802 - accuracy: 0.7399 - val_loss: 0.1871 - val_accuracy: 0.7378\n",
      "Epoch 900/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1780 - accuracy: 0.7415 - val_loss: 0.1868 - val_accuracy: 0.7371\n",
      "Epoch 901/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1765 - accuracy: 0.7409 - val_loss: 0.1870 - val_accuracy: 0.7378\n",
      "Epoch 902/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1775 - accuracy: 0.7404 - val_loss: 0.1880 - val_accuracy: 0.7348\n",
      "Epoch 903/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1756 - accuracy: 0.7430 - val_loss: 0.1860 - val_accuracy: 0.7387\n",
      "Epoch 904/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1748 - accuracy: 0.7486 - val_loss: 0.1861 - val_accuracy: 0.7367\n",
      "Epoch 905/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1769 - accuracy: 0.7395 - val_loss: 0.1870 - val_accuracy: 0.7371\n",
      "Epoch 906/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1780 - accuracy: 0.7416 - val_loss: 0.1867 - val_accuracy: 0.7370\n",
      "Epoch 907/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1770 - accuracy: 0.7409 - val_loss: 0.1868 - val_accuracy: 0.7366\n",
      "Epoch 908/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1759 - accuracy: 0.7396 - val_loss: 0.1869 - val_accuracy: 0.7380\n",
      "Epoch 909/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1745 - accuracy: 0.7443 - val_loss: 0.1867 - val_accuracy: 0.7379\n",
      "Epoch 910/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1794 - accuracy: 0.7374 - val_loss: 0.1871 - val_accuracy: 0.7384\n",
      "Epoch 911/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1785 - accuracy: 0.7387 - val_loss: 0.1866 - val_accuracy: 0.7380\n",
      "Epoch 912/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1741 - accuracy: 0.7474 - val_loss: 0.1872 - val_accuracy: 0.7375\n",
      "Epoch 913/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1735 - accuracy: 0.7477 - val_loss: 0.1867 - val_accuracy: 0.7381\n",
      "Epoch 914/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1768 - accuracy: 0.7428 - val_loss: 0.1873 - val_accuracy: 0.7379\n",
      "Epoch 915/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1744 - accuracy: 0.7452 - val_loss: 0.1867 - val_accuracy: 0.7379\n",
      "Epoch 916/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1778 - accuracy: 0.7389 - val_loss: 0.1888 - val_accuracy: 0.7363\n",
      "Epoch 917/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1787 - accuracy: 0.7387 - val_loss: 0.1880 - val_accuracy: 0.7368\n",
      "Epoch 918/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1747 - accuracy: 0.7458 - val_loss: 0.1878 - val_accuracy: 0.7367\n",
      "Epoch 919/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1784 - accuracy: 0.7391 - val_loss: 0.1901 - val_accuracy: 0.7304\n",
      "Epoch 920/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1767 - accuracy: 0.7420 - val_loss: 0.1857 - val_accuracy: 0.7389\n",
      "Epoch 921/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1763 - accuracy: 0.7396 - val_loss: 0.1863 - val_accuracy: 0.7372\n",
      "Epoch 922/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1763 - accuracy: 0.7462 - val_loss: 0.1854 - val_accuracy: 0.7383\n",
      "Epoch 923/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1795 - accuracy: 0.7361 - val_loss: 0.1863 - val_accuracy: 0.7324\n",
      "Epoch 924/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1785 - accuracy: 0.7383 - val_loss: 0.1871 - val_accuracy: 0.7297\n",
      "Epoch 925/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1793 - accuracy: 0.7342 - val_loss: 0.1857 - val_accuracy: 0.7371\n",
      "Epoch 926/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1745 - accuracy: 0.7452 - val_loss: 0.1860 - val_accuracy: 0.7375\n",
      "Epoch 927/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1757 - accuracy: 0.7433 - val_loss: 0.1852 - val_accuracy: 0.7393\n",
      "Epoch 928/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1789 - accuracy: 0.7378 - val_loss: 0.1856 - val_accuracy: 0.7396\n",
      "Epoch 929/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1752 - accuracy: 0.7468 - val_loss: 0.1857 - val_accuracy: 0.7402\n",
      "Epoch 930/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1800 - accuracy: 0.7366 - val_loss: 0.1885 - val_accuracy: 0.7356\n",
      "Epoch 931/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1795 - accuracy: 0.7352 - val_loss: 0.1891 - val_accuracy: 0.7367\n",
      "Epoch 932/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1781 - accuracy: 0.7394 - val_loss: 0.1878 - val_accuracy: 0.7372\n",
      "Epoch 933/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1776 - accuracy: 0.7407 - val_loss: 0.1884 - val_accuracy: 0.7372\n",
      "Epoch 934/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1765 - accuracy: 0.7427 - val_loss: 0.1882 - val_accuracy: 0.7357\n",
      "Epoch 935/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1773 - accuracy: 0.7383 - val_loss: 0.1882 - val_accuracy: 0.7357\n",
      "Epoch 936/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1762 - accuracy: 0.7440 - val_loss: 0.1863 - val_accuracy: 0.7375\n",
      "Epoch 937/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1804 - accuracy: 0.7344 - val_loss: 0.1872 - val_accuracy: 0.7388\n",
      "Epoch 938/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1799 - accuracy: 0.7349 - val_loss: 0.1870 - val_accuracy: 0.7372\n",
      "Epoch 939/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1781 - accuracy: 0.7389 - val_loss: 0.1873 - val_accuracy: 0.7370\n",
      "Epoch 940/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1765 - accuracy: 0.7443 - val_loss: 0.1873 - val_accuracy: 0.7372\n",
      "Epoch 941/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1735 - accuracy: 0.7464 - val_loss: 0.1880 - val_accuracy: 0.7372\n",
      "Epoch 942/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1766 - accuracy: 0.7426 - val_loss: 0.1891 - val_accuracy: 0.7359\n",
      "Epoch 943/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1780 - accuracy: 0.7397 - val_loss: 0.1883 - val_accuracy: 0.7353\n",
      "Epoch 944/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1780 - accuracy: 0.7418 - val_loss: 0.1882 - val_accuracy: 0.7361\n",
      "Epoch 945/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1757 - accuracy: 0.7437 - val_loss: 0.1877 - val_accuracy: 0.7341\n",
      "Epoch 946/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1756 - accuracy: 0.7443 - val_loss: 0.1882 - val_accuracy: 0.7365\n",
      "Epoch 947/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1750 - accuracy: 0.7459 - val_loss: 0.1882 - val_accuracy: 0.7379\n",
      "Epoch 948/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1755 - accuracy: 0.7453 - val_loss: 0.1889 - val_accuracy: 0.7365\n",
      "Epoch 949/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1769 - accuracy: 0.7429 - val_loss: 0.1875 - val_accuracy: 0.7372\n",
      "Epoch 950/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1773 - accuracy: 0.7432 - val_loss: 0.1894 - val_accuracy: 0.7358\n",
      "Epoch 951/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1766 - accuracy: 0.7431 - val_loss: 0.1879 - val_accuracy: 0.7358\n",
      "Epoch 952/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1760 - accuracy: 0.7428 - val_loss: 0.1872 - val_accuracy: 0.7368\n",
      "Epoch 953/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1785 - accuracy: 0.7391 - val_loss: 0.1877 - val_accuracy: 0.7368\n",
      "Epoch 954/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1765 - accuracy: 0.7415 - val_loss: 0.1871 - val_accuracy: 0.7374\n",
      "Epoch 955/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1754 - accuracy: 0.7431 - val_loss: 0.1872 - val_accuracy: 0.7375\n",
      "Epoch 956/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1768 - accuracy: 0.7421 - val_loss: 0.1916 - val_accuracy: 0.7331\n",
      "Epoch 957/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1803 - accuracy: 0.7390 - val_loss: 0.1857 - val_accuracy: 0.7400\n",
      "Epoch 958/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1770 - accuracy: 0.7432 - val_loss: 0.1852 - val_accuracy: 0.7398\n",
      "Epoch 959/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1753 - accuracy: 0.7440 - val_loss: 0.1869 - val_accuracy: 0.7380\n",
      "Epoch 960/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1762 - accuracy: 0.7424 - val_loss: 0.1872 - val_accuracy: 0.7394\n",
      "Epoch 961/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1789 - accuracy: 0.7416 - val_loss: 0.1864 - val_accuracy: 0.7384\n",
      "Epoch 962/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1761 - accuracy: 0.7447 - val_loss: 0.1859 - val_accuracy: 0.7385\n",
      "Epoch 963/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1753 - accuracy: 0.7439 - val_loss: 0.1870 - val_accuracy: 0.7378\n",
      "Epoch 964/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1764 - accuracy: 0.7385 - val_loss: 0.1871 - val_accuracy: 0.7380\n",
      "Epoch 965/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1753 - accuracy: 0.7438 - val_loss: 0.1871 - val_accuracy: 0.7380\n",
      "Epoch 966/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1780 - accuracy: 0.7380 - val_loss: 0.1860 - val_accuracy: 0.7383\n",
      "Epoch 967/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1775 - accuracy: 0.7383 - val_loss: 0.1872 - val_accuracy: 0.7372\n",
      "Epoch 968/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1760 - accuracy: 0.7428 - val_loss: 0.1876 - val_accuracy: 0.7378\n",
      "Epoch 969/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1775 - accuracy: 0.7385 - val_loss: 0.1874 - val_accuracy: 0.7378\n",
      "Epoch 970/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1766 - accuracy: 0.7439 - val_loss: 0.1859 - val_accuracy: 0.7392\n",
      "Epoch 971/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1763 - accuracy: 0.7437 - val_loss: 0.1853 - val_accuracy: 0.7394\n",
      "Epoch 972/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1767 - accuracy: 0.7422 - val_loss: 0.1864 - val_accuracy: 0.7401\n",
      "Epoch 973/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1779 - accuracy: 0.7387 - val_loss: 0.1862 - val_accuracy: 0.7388\n",
      "Epoch 974/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1754 - accuracy: 0.7443 - val_loss: 0.1860 - val_accuracy: 0.7396\n",
      "Epoch 975/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1775 - accuracy: 0.7401 - val_loss: 0.1868 - val_accuracy: 0.7379\n",
      "Epoch 976/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1784 - accuracy: 0.7360 - val_loss: 0.1867 - val_accuracy: 0.7388\n",
      "Epoch 977/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1765 - accuracy: 0.7453 - val_loss: 0.1867 - val_accuracy: 0.7345\n",
      "Epoch 978/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1783 - accuracy: 0.7370 - val_loss: 0.1872 - val_accuracy: 0.7346\n",
      "Epoch 979/1000\n",
      "563/563 [==============================] - ETA: 0s - loss: 0.1783 - accuracy: 0.73 - 1s 1ms/step - loss: 0.1783 - accuracy: 0.7395 - val_loss: 0.1864 - val_accuracy: 0.7367\n",
      "Epoch 980/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1764 - accuracy: 0.7426 - val_loss: 0.1867 - val_accuracy: 0.7393\n",
      "Epoch 981/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1793 - accuracy: 0.7372 - val_loss: 0.1858 - val_accuracy: 0.7400\n",
      "Epoch 982/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1761 - accuracy: 0.7461 - val_loss: 0.1855 - val_accuracy: 0.7400\n",
      "Epoch 983/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1757 - accuracy: 0.7471 - val_loss: 0.1854 - val_accuracy: 0.7406\n",
      "Epoch 984/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1773 - accuracy: 0.7429 - val_loss: 0.1859 - val_accuracy: 0.7400\n",
      "Epoch 985/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1761 - accuracy: 0.7441 - val_loss: 0.1864 - val_accuracy: 0.7397\n",
      "Epoch 986/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1778 - accuracy: 0.7425 - val_loss: 0.1854 - val_accuracy: 0.7403\n",
      "Epoch 987/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1787 - accuracy: 0.7390 - val_loss: 0.1847 - val_accuracy: 0.7407\n",
      "Epoch 988/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1765 - accuracy: 0.7417 - val_loss: 0.1858 - val_accuracy: 0.7415\n",
      "Epoch 989/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1757 - accuracy: 0.7422 - val_loss: 0.1843 - val_accuracy: 0.7419\n",
      "Epoch 990/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1768 - accuracy: 0.7418 - val_loss: 0.1857 - val_accuracy: 0.7420\n",
      "Epoch 991/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1756 - accuracy: 0.7436 - val_loss: 0.1851 - val_accuracy: 0.7413\n",
      "Epoch 992/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1775 - accuracy: 0.7405 - val_loss: 0.1866 - val_accuracy: 0.7394\n",
      "Epoch 993/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1725 - accuracy: 0.7515 - val_loss: 0.1889 - val_accuracy: 0.7389\n",
      "Epoch 994/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1755 - accuracy: 0.7453 - val_loss: 0.1888 - val_accuracy: 0.7385\n",
      "Epoch 995/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1783 - accuracy: 0.7392 - val_loss: 0.1850 - val_accuracy: 0.7393\n",
      "Epoch 996/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1781 - accuracy: 0.7410 - val_loss: 0.1853 - val_accuracy: 0.7413\n",
      "Epoch 997/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1764 - accuracy: 0.7411 - val_loss: 0.1852 - val_accuracy: 0.7397\n",
      "Epoch 998/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1779 - accuracy: 0.7414 - val_loss: 0.1846 - val_accuracy: 0.7406\n",
      "Epoch 999/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1759 - accuracy: 0.7460 - val_loss: 0.1852 - val_accuracy: 0.7401\n",
      "Epoch 1000/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1762 - accuracy: 0.7437 - val_loss: 0.1845 - val_accuracy: 0.7419\n"
     ]
    }
   ],
   "source": [
    "# Fit the model using 50 epochs and the training data\n",
    "fit_model_A41 = nn_A41.fit(X_train_scaled, y_train, validation_split=0.3, epochs=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alternative Model 41 Results\n",
      "Loss: 0.1888144314289093, Accuracy: 0.7306122183799744\n"
     ]
    }
   ],
   "source": [
    "print(\"Alternative Model 41 Results\")\n",
    "# Evaluate the model loss and accuracy metrics using the evaluate method and the test data\n",
    "model_loss, model_accuracy = nn_A41.evaluate(X_test_scaled,y_test,verbose=0)\n",
    "# Display the model loss and accuracy results\n",
    "print(f\"Loss: {model_loss}, Accuracy: {model_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alternative Model 25+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "114"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the the number of inputs (features) to the model\n",
    "number_input_features = len(X_train.iloc[0])\n",
    "# Review the number of features\n",
    "number_input_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the number of neurons in the output layer\n",
    "number_output_neurons_A25 = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the number of hidden nodes for the first hidden layer\n",
    "hidden_nodes_layer1_A25 =  (number_input_features + number_output_neurons_A25) // 2\n",
    "# Review the number of hidden nodes in the first layer\n",
    "hidden_nodes_layer1_A25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the number of hidden nodes for the second hidden layer\n",
    "hidden_nodes_layer2_A25 =  (hidden_nodes_layer1_A25 + number_output_neurons_A25) // 2\n",
    "# Review the number of hidden nodes in the second layer\n",
    "hidden_nodes_layer2_A25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the number of hidden nodes for the third hidden layer\n",
    "hidden_nodes_layer3_A25 =  (hidden_nodes_layer2_A25 + number_output_neurons_A25) // 2\n",
    "# Review the number of hidden nodes in the third layer\n",
    "hidden_nodes_layer3_A25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the number of hidden nodes for the fourth hidden layer\n",
    "hidden_nodes_layer4_A25 =  (hidden_nodes_layer3_A25 + number_output_neurons_A25) // 2\n",
    "# Review the number of hidden nodes in the fourth layer\n",
    "hidden_nodes_layer4_A25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the number of hidden nodes for the fifth hidden layer\n",
    "hidden_nodes_layer5_A25 =  (hidden_nodes_layer4_A25 + number_output_neurons_A25) // 2\n",
    "# Review the number of hidden nodes in the fifth layer\n",
    "hidden_nodes_layer5_A25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the number of hidden nodes for the Sixth hidden layer\n",
    "hidden_nodes_layer6_A25 =  (hidden_nodes_layer5_A25 + number_output_neurons_A25) // 2\n",
    "# Review the number of hidden nodes in the sixth layer\n",
    "hidden_nodes_layer6_A25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_10 (Dense)             (None, 57)                6555      \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 29)                1682      \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 15)                450       \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 8)                 128       \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 4)                 36        \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 2)                 10        \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 1)                 3         \n",
      "=================================================================\n",
      "Total params: 8,864\n",
      "Trainable params: 8,864\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Create and Display the Sequential Model Instance \n",
    "# for Model A25\n",
    "nn_A25 = Sequential() \n",
    "\n",
    "# Add the first hidden layer\n",
    "nn_A25.add(Dense(units=hidden_nodes_layer1_A25, input_dim=number_input_features, activation=\"relu\"))\n",
    "# Add the second hidden layer\n",
    "nn_A25.add(Dense(units=hidden_nodes_layer2_A25, input_dim=number_input_features, activation=\"relu\"))\n",
    "# Add the third hidden layer\n",
    "nn_A25.add(Dense(units=hidden_nodes_layer3_A25, input_dim=number_input_features, activation=\"relu\"))\n",
    "# Add the fourth hidden layer\n",
    "nn_A25.add(Dense(units=hidden_nodes_layer4_A25, input_dim=number_input_features, activation=\"relu\"))\n",
    "# Add the fifth hidden layer\n",
    "nn_A25.add(Dense(units=hidden_nodes_layer5_A25, input_dim=number_input_features, activation=\"relu\"))\n",
    "# Add the sixth hidden layer\n",
    "nn_A25.add(Dense(units=hidden_nodes_layer6_A25, activation=\"relu\"))\n",
    "\n",
    "# Add the output layer to the model specifying the number of output neurons and activation function\n",
    "nn_A25.add(Dense(units=number_output_neurons_A25, activation=\"sigmoid\"))\n",
    "\n",
    "# Display the Sequential model summary\n",
    "nn_A25.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the Sequential model\n",
    "nn_A25.compile(loss=\"mse\", optimizer=\"adam\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2239 - accuracy: 0.6623 - val_loss: 0.2041 - val_accuracy: 0.7321\n",
      "Epoch 2/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1994 - accuracy: 0.7293 - val_loss: 0.1943 - val_accuracy: 0.7317\n",
      "Epoch 3/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1933 - accuracy: 0.7324 - val_loss: 0.1920 - val_accuracy: 0.7315\n",
      "Epoch 4/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1909 - accuracy: 0.7294 - val_loss: 0.1876 - val_accuracy: 0.7366\n",
      "Epoch 5/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1918 - accuracy: 0.7244 - val_loss: 0.1876 - val_accuracy: 0.7335\n",
      "Epoch 6/1000\n",
      "563/563 [==============================] - 1s 993us/step - loss: 0.1872 - accuracy: 0.7304 - val_loss: 0.1864 - val_accuracy: 0.7359\n",
      "Epoch 7/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1886 - accuracy: 0.7269 - val_loss: 0.1863 - val_accuracy: 0.7339\n",
      "Epoch 8/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1874 - accuracy: 0.7297 - val_loss: 0.1852 - val_accuracy: 0.7352\n",
      "Epoch 9/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1856 - accuracy: 0.7329 - val_loss: 0.1846 - val_accuracy: 0.7367\n",
      "Epoch 10/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1868 - accuracy: 0.7317 - val_loss: 0.1839 - val_accuracy: 0.7376\n",
      "Epoch 11/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1859 - accuracy: 0.7290 - val_loss: 0.1838 - val_accuracy: 0.7370\n",
      "Epoch 12/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1831 - accuracy: 0.7375 - val_loss: 0.1857 - val_accuracy: 0.7334\n",
      "Epoch 13/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1826 - accuracy: 0.7361 - val_loss: 0.1851 - val_accuracy: 0.7323\n",
      "Epoch 14/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1823 - accuracy: 0.7368 - val_loss: 0.1845 - val_accuracy: 0.7345\n",
      "Epoch 15/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1821 - accuracy: 0.7351 - val_loss: 0.1851 - val_accuracy: 0.7321\n",
      "Epoch 16/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1832 - accuracy: 0.7283 - val_loss: 0.1848 - val_accuracy: 0.7368\n",
      "Epoch 17/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1808 - accuracy: 0.7438 - val_loss: 0.1844 - val_accuracy: 0.7349\n",
      "Epoch 18/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1830 - accuracy: 0.7398 - val_loss: 0.1854 - val_accuracy: 0.7309\n",
      "Epoch 19/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1799 - accuracy: 0.7424 - val_loss: 0.1849 - val_accuracy: 0.7344\n",
      "Epoch 20/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1815 - accuracy: 0.7354 - val_loss: 0.1858 - val_accuracy: 0.7349\n",
      "Epoch 21/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1844 - accuracy: 0.7344 - val_loss: 0.1838 - val_accuracy: 0.7372\n",
      "Epoch 22/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1813 - accuracy: 0.7374 - val_loss: 0.1846 - val_accuracy: 0.7370\n",
      "Epoch 23/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1800 - accuracy: 0.7395 - val_loss: 0.1856 - val_accuracy: 0.7358\n",
      "Epoch 24/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1835 - accuracy: 0.7324 - val_loss: 0.1853 - val_accuracy: 0.7335\n",
      "Epoch 25/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1819 - accuracy: 0.7360 - val_loss: 0.1839 - val_accuracy: 0.7348\n",
      "Epoch 26/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1826 - accuracy: 0.7375 - val_loss: 0.1870 - val_accuracy: 0.7299\n",
      "Epoch 27/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1812 - accuracy: 0.7382 - val_loss: 0.1846 - val_accuracy: 0.7359\n",
      "Epoch 28/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1811 - accuracy: 0.7363 - val_loss: 0.1847 - val_accuracy: 0.7358\n",
      "Epoch 29/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1800 - accuracy: 0.7416 - val_loss: 0.1855 - val_accuracy: 0.7337\n",
      "Epoch 30/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1802 - accuracy: 0.7382 - val_loss: 0.1854 - val_accuracy: 0.7323\n",
      "Epoch 31/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1804 - accuracy: 0.7414 - val_loss: 0.1847 - val_accuracy: 0.7352\n",
      "Epoch 32/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1776 - accuracy: 0.7485 - val_loss: 0.1860 - val_accuracy: 0.7322\n",
      "Epoch 33/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1818 - accuracy: 0.7368 - val_loss: 0.1845 - val_accuracy: 0.7367\n",
      "Epoch 34/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1796 - accuracy: 0.7372 - val_loss: 0.1842 - val_accuracy: 0.7372\n",
      "Epoch 35/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1785 - accuracy: 0.7405 - val_loss: 0.1855 - val_accuracy: 0.7339\n",
      "Epoch 36/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1820 - accuracy: 0.7381 - val_loss: 0.1847 - val_accuracy: 0.7340\n",
      "Epoch 37/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1819 - accuracy: 0.7351 - val_loss: 0.1857 - val_accuracy: 0.7311\n",
      "Epoch 38/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1780 - accuracy: 0.7433 - val_loss: 0.1848 - val_accuracy: 0.7367\n",
      "Epoch 39/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1811 - accuracy: 0.7365 - val_loss: 0.1850 - val_accuracy: 0.7359\n",
      "Epoch 40/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1803 - accuracy: 0.7368 - val_loss: 0.1854 - val_accuracy: 0.7337\n",
      "Epoch 41/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1764 - accuracy: 0.7439 - val_loss: 0.1854 - val_accuracy: 0.7357\n",
      "Epoch 42/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1791 - accuracy: 0.7379 - val_loss: 0.1853 - val_accuracy: 0.7350\n",
      "Epoch 43/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1819 - accuracy: 0.7346 - val_loss: 0.1843 - val_accuracy: 0.7356\n",
      "Epoch 44/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1790 - accuracy: 0.7402 - val_loss: 0.1859 - val_accuracy: 0.7339\n",
      "Epoch 45/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1784 - accuracy: 0.7433 - val_loss: 0.1866 - val_accuracy: 0.7292\n",
      "Epoch 46/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1784 - accuracy: 0.7392 - val_loss: 0.1849 - val_accuracy: 0.7348\n",
      "Epoch 47/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1808 - accuracy: 0.7375 - val_loss: 0.1866 - val_accuracy: 0.7292\n",
      "Epoch 48/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1808 - accuracy: 0.7343 - val_loss: 0.1848 - val_accuracy: 0.7354\n",
      "Epoch 49/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1791 - accuracy: 0.7392 - val_loss: 0.1855 - val_accuracy: 0.7352\n",
      "Epoch 50/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1804 - accuracy: 0.7379 - val_loss: 0.1852 - val_accuracy: 0.7363\n",
      "Epoch 51/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1800 - accuracy: 0.7393 - val_loss: 0.1847 - val_accuracy: 0.7359\n",
      "Epoch 52/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1806 - accuracy: 0.7335 - val_loss: 0.1852 - val_accuracy: 0.7340\n",
      "Epoch 53/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1805 - accuracy: 0.7362 - val_loss: 0.1848 - val_accuracy: 0.7363\n",
      "Epoch 54/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1779 - accuracy: 0.7428 - val_loss: 0.1857 - val_accuracy: 0.7313\n",
      "Epoch 55/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1793 - accuracy: 0.7395 - val_loss: 0.1853 - val_accuracy: 0.7331\n",
      "Epoch 56/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1808 - accuracy: 0.7349 - val_loss: 0.1861 - val_accuracy: 0.7328\n",
      "Epoch 57/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1798 - accuracy: 0.7360 - val_loss: 0.1851 - val_accuracy: 0.7363\n",
      "Epoch 58/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1793 - accuracy: 0.7404 - val_loss: 0.1859 - val_accuracy: 0.7327\n",
      "Epoch 59/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1798 - accuracy: 0.7392 - val_loss: 0.1850 - val_accuracy: 0.7352\n",
      "Epoch 60/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1788 - accuracy: 0.7421 - val_loss: 0.1860 - val_accuracy: 0.7366\n",
      "Epoch 61/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1784 - accuracy: 0.7379 - val_loss: 0.1856 - val_accuracy: 0.7327\n",
      "Epoch 62/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1794 - accuracy: 0.7398 - val_loss: 0.1858 - val_accuracy: 0.7350\n",
      "Epoch 63/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1800 - accuracy: 0.7387 - val_loss: 0.1855 - val_accuracy: 0.7341\n",
      "Epoch 64/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1791 - accuracy: 0.7430 - val_loss: 0.1858 - val_accuracy: 0.7324\n",
      "Epoch 65/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1816 - accuracy: 0.7368 - val_loss: 0.1859 - val_accuracy: 0.7368\n",
      "Epoch 66/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1744 - accuracy: 0.7454 - val_loss: 0.1848 - val_accuracy: 0.7344\n",
      "Epoch 67/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1787 - accuracy: 0.7414 - val_loss: 0.1870 - val_accuracy: 0.7287\n",
      "Epoch 68/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1788 - accuracy: 0.7395 - val_loss: 0.1855 - val_accuracy: 0.7328\n",
      "Epoch 69/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1801 - accuracy: 0.7403 - val_loss: 0.1852 - val_accuracy: 0.7343\n",
      "Epoch 70/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1773 - accuracy: 0.7412 - val_loss: 0.1858 - val_accuracy: 0.7344\n",
      "Epoch 71/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1780 - accuracy: 0.7393 - val_loss: 0.1850 - val_accuracy: 0.7358\n",
      "Epoch 72/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1788 - accuracy: 0.7393 - val_loss: 0.1864 - val_accuracy: 0.7327\n",
      "Epoch 73/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1774 - accuracy: 0.7449 - val_loss: 0.1872 - val_accuracy: 0.7288\n",
      "Epoch 74/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1778 - accuracy: 0.7433 - val_loss: 0.1881 - val_accuracy: 0.7323\n",
      "Epoch 75/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1808 - accuracy: 0.7356 - val_loss: 0.1857 - val_accuracy: 0.7353\n",
      "Epoch 76/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1788 - accuracy: 0.7385 - val_loss: 0.1857 - val_accuracy: 0.7334\n",
      "Epoch 77/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1772 - accuracy: 0.7439 - val_loss: 0.1858 - val_accuracy: 0.7357\n",
      "Epoch 78/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1795 - accuracy: 0.7406 - val_loss: 0.1858 - val_accuracy: 0.7321\n",
      "Epoch 79/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1792 - accuracy: 0.7395 - val_loss: 0.1850 - val_accuracy: 0.7366\n",
      "Epoch 80/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1801 - accuracy: 0.7367 - val_loss: 0.1863 - val_accuracy: 0.7319\n",
      "Epoch 81/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1777 - accuracy: 0.7422 - val_loss: 0.1865 - val_accuracy: 0.7348\n",
      "Epoch 82/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1782 - accuracy: 0.7370 - val_loss: 0.1862 - val_accuracy: 0.7326\n",
      "Epoch 83/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1761 - accuracy: 0.7465 - val_loss: 0.1859 - val_accuracy: 0.7341\n",
      "Epoch 84/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1771 - accuracy: 0.7405 - val_loss: 0.1865 - val_accuracy: 0.7352\n",
      "Epoch 85/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1795 - accuracy: 0.7390 - val_loss: 0.1878 - val_accuracy: 0.7296\n",
      "Epoch 86/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1771 - accuracy: 0.7455 - val_loss: 0.1871 - val_accuracy: 0.7352\n",
      "Epoch 87/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1789 - accuracy: 0.7385 - val_loss: 0.1870 - val_accuracy: 0.7311\n",
      "Epoch 88/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1781 - accuracy: 0.7413 - val_loss: 0.1855 - val_accuracy: 0.7357\n",
      "Epoch 89/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1767 - accuracy: 0.7475 - val_loss: 0.1859 - val_accuracy: 0.7344\n",
      "Epoch 90/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1786 - accuracy: 0.7400 - val_loss: 0.1870 - val_accuracy: 0.7348\n",
      "Epoch 91/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1805 - accuracy: 0.7371 - val_loss: 0.1858 - val_accuracy: 0.7352\n",
      "Epoch 92/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1788 - accuracy: 0.7405 - val_loss: 0.1861 - val_accuracy: 0.7340\n",
      "Epoch 93/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1781 - accuracy: 0.7382 - val_loss: 0.1863 - val_accuracy: 0.7327\n",
      "Epoch 94/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1791 - accuracy: 0.7361 - val_loss: 0.1865 - val_accuracy: 0.7340\n",
      "Epoch 95/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1770 - accuracy: 0.7447 - val_loss: 0.1865 - val_accuracy: 0.7353\n",
      "Epoch 96/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1770 - accuracy: 0.7406 - val_loss: 0.1865 - val_accuracy: 0.7357\n",
      "Epoch 97/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1783 - accuracy: 0.7394 - val_loss: 0.1857 - val_accuracy: 0.7350\n",
      "Epoch 98/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1795 - accuracy: 0.7385 - val_loss: 0.1855 - val_accuracy: 0.7366\n",
      "Epoch 99/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1778 - accuracy: 0.7417 - val_loss: 0.1886 - val_accuracy: 0.7321\n",
      "Epoch 100/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1788 - accuracy: 0.7372 - val_loss: 0.1865 - val_accuracy: 0.7362\n",
      "Epoch 101/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1788 - accuracy: 0.7376 - val_loss: 0.1860 - val_accuracy: 0.7343\n",
      "Epoch 102/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1768 - accuracy: 0.7442 - val_loss: 0.1859 - val_accuracy: 0.7356\n",
      "Epoch 103/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1773 - accuracy: 0.7436 - val_loss: 0.1863 - val_accuracy: 0.7311\n",
      "Epoch 104/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1757 - accuracy: 0.7450 - val_loss: 0.1873 - val_accuracy: 0.7302\n",
      "Epoch 105/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1774 - accuracy: 0.7442 - val_loss: 0.1859 - val_accuracy: 0.7328\n",
      "Epoch 106/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1774 - accuracy: 0.7454 - val_loss: 0.1854 - val_accuracy: 0.7357\n",
      "Epoch 107/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1805 - accuracy: 0.7359 - val_loss: 0.1860 - val_accuracy: 0.7352\n",
      "Epoch 108/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1782 - accuracy: 0.7433 - val_loss: 0.1855 - val_accuracy: 0.7357\n",
      "Epoch 109/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1775 - accuracy: 0.7453 - val_loss: 0.1856 - val_accuracy: 0.7328\n",
      "Epoch 110/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1784 - accuracy: 0.7437 - val_loss: 0.1865 - val_accuracy: 0.7362\n",
      "Epoch 111/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1776 - accuracy: 0.7427 - val_loss: 0.1865 - val_accuracy: 0.7317\n",
      "Epoch 112/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1759 - accuracy: 0.7456 - val_loss: 0.1864 - val_accuracy: 0.7352\n",
      "Epoch 113/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1783 - accuracy: 0.7411 - val_loss: 0.1867 - val_accuracy: 0.7350\n",
      "Epoch 114/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1793 - accuracy: 0.7388 - val_loss: 0.1868 - val_accuracy: 0.7331\n",
      "Epoch 115/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1776 - accuracy: 0.7405 - val_loss: 0.1846 - val_accuracy: 0.7368\n",
      "Epoch 116/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1777 - accuracy: 0.7439 - val_loss: 0.1855 - val_accuracy: 0.7331\n",
      "Epoch 117/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1776 - accuracy: 0.7389 - val_loss: 0.1860 - val_accuracy: 0.7362\n",
      "Epoch 118/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1775 - accuracy: 0.7421 - val_loss: 0.1858 - val_accuracy: 0.7336\n",
      "Epoch 119/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1768 - accuracy: 0.7436 - val_loss: 0.1863 - val_accuracy: 0.7372\n",
      "Epoch 120/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1768 - accuracy: 0.7415 - val_loss: 0.1863 - val_accuracy: 0.7358\n",
      "Epoch 121/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1752 - accuracy: 0.7456 - val_loss: 0.1872 - val_accuracy: 0.7352\n",
      "Epoch 122/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1776 - accuracy: 0.7406 - val_loss: 0.1858 - val_accuracy: 0.7362\n",
      "Epoch 123/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1773 - accuracy: 0.7410 - val_loss: 0.1855 - val_accuracy: 0.7358\n",
      "Epoch 124/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1784 - accuracy: 0.7409 - val_loss: 0.1862 - val_accuracy: 0.7354\n",
      "Epoch 125/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1767 - accuracy: 0.7430 - val_loss: 0.1864 - val_accuracy: 0.7361\n",
      "Epoch 126/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1771 - accuracy: 0.7404 - val_loss: 0.1858 - val_accuracy: 0.7353\n",
      "Epoch 127/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1795 - accuracy: 0.7370 - val_loss: 0.1851 - val_accuracy: 0.7376\n",
      "Epoch 128/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1763 - accuracy: 0.7442 - val_loss: 0.1851 - val_accuracy: 0.7370\n",
      "Epoch 129/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1769 - accuracy: 0.7396 - val_loss: 0.1848 - val_accuracy: 0.7379\n",
      "Epoch 130/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1786 - accuracy: 0.7389 - val_loss: 0.1854 - val_accuracy: 0.7357\n",
      "Epoch 131/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1761 - accuracy: 0.7431 - val_loss: 0.1864 - val_accuracy: 0.7344\n",
      "Epoch 132/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1785 - accuracy: 0.7407 - val_loss: 0.1859 - val_accuracy: 0.7335\n",
      "Epoch 133/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1802 - accuracy: 0.7361 - val_loss: 0.1855 - val_accuracy: 0.7366\n",
      "Epoch 134/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1773 - accuracy: 0.7394 - val_loss: 0.1851 - val_accuracy: 0.7336\n",
      "Epoch 135/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1788 - accuracy: 0.7376 - val_loss: 0.1862 - val_accuracy: 0.7339\n",
      "Epoch 136/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1779 - accuracy: 0.7394 - val_loss: 0.1850 - val_accuracy: 0.7328\n",
      "Epoch 137/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1764 - accuracy: 0.7451 - val_loss: 0.1858 - val_accuracy: 0.7372\n",
      "Epoch 138/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1786 - accuracy: 0.7382 - val_loss: 0.1847 - val_accuracy: 0.7375\n",
      "Epoch 139/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1756 - accuracy: 0.7448 - val_loss: 0.1862 - val_accuracy: 0.7334\n",
      "Epoch 140/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1756 - accuracy: 0.7432 - val_loss: 0.1861 - val_accuracy: 0.7340\n",
      "Epoch 141/1000\n",
      "563/563 [==============================] - 1s 3ms/step - loss: 0.1757 - accuracy: 0.7435 - val_loss: 0.1860 - val_accuracy: 0.7366\n",
      "Epoch 142/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1790 - accuracy: 0.7396 - val_loss: 0.1854 - val_accuracy: 0.7356\n",
      "Epoch 143/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1801 - accuracy: 0.7377 - val_loss: 0.1861 - val_accuracy: 0.7362\n",
      "Epoch 144/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1786 - accuracy: 0.7405 - val_loss: 0.1857 - val_accuracy: 0.7345\n",
      "Epoch 145/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1761 - accuracy: 0.7454 - val_loss: 0.1870 - val_accuracy: 0.7375\n",
      "Epoch 146/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1767 - accuracy: 0.7438 - val_loss: 0.1865 - val_accuracy: 0.7339\n",
      "Epoch 147/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1777 - accuracy: 0.7446 - val_loss: 0.1851 - val_accuracy: 0.7383\n",
      "Epoch 148/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1763 - accuracy: 0.7414 - val_loss: 0.1865 - val_accuracy: 0.7363\n",
      "Epoch 149/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1746 - accuracy: 0.7453 - val_loss: 0.1873 - val_accuracy: 0.7346\n",
      "Epoch 150/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1764 - accuracy: 0.7443 - val_loss: 0.1860 - val_accuracy: 0.7381\n",
      "Epoch 151/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1752 - accuracy: 0.7451 - val_loss: 0.1854 - val_accuracy: 0.7345\n",
      "Epoch 152/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1743 - accuracy: 0.7473 - val_loss: 0.1857 - val_accuracy: 0.7354\n",
      "Epoch 153/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1789 - accuracy: 0.7370 - val_loss: 0.1849 - val_accuracy: 0.7368\n",
      "Epoch 154/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1789 - accuracy: 0.7372 - val_loss: 0.1867 - val_accuracy: 0.7366\n",
      "Epoch 155/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1771 - accuracy: 0.7428 - val_loss: 0.1848 - val_accuracy: 0.7367\n",
      "Epoch 156/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1764 - accuracy: 0.7430 - val_loss: 0.1852 - val_accuracy: 0.7367\n",
      "Epoch 157/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1755 - accuracy: 0.7452 - val_loss: 0.1859 - val_accuracy: 0.7346\n",
      "Epoch 158/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1745 - accuracy: 0.7454 - val_loss: 0.1859 - val_accuracy: 0.7339\n",
      "Epoch 159/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1761 - accuracy: 0.7438 - val_loss: 0.1867 - val_accuracy: 0.7368\n",
      "Epoch 160/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1770 - accuracy: 0.7419 - val_loss: 0.1857 - val_accuracy: 0.7374\n",
      "Epoch 161/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1783 - accuracy: 0.7393 - val_loss: 0.1860 - val_accuracy: 0.7366\n",
      "Epoch 162/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1771 - accuracy: 0.7406 - val_loss: 0.1862 - val_accuracy: 0.7345\n",
      "Epoch 163/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1778 - accuracy: 0.7400 - val_loss: 0.1850 - val_accuracy: 0.7372\n",
      "Epoch 164/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1769 - accuracy: 0.7424 - val_loss: 0.1858 - val_accuracy: 0.7371\n",
      "Epoch 165/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1772 - accuracy: 0.7434 - val_loss: 0.1861 - val_accuracy: 0.7349\n",
      "Epoch 166/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1764 - accuracy: 0.7445 - val_loss: 0.1863 - val_accuracy: 0.7348\n",
      "Epoch 167/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1743 - accuracy: 0.7466 - val_loss: 0.1857 - val_accuracy: 0.7366\n",
      "Epoch 168/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1747 - accuracy: 0.7451 - val_loss: 0.1859 - val_accuracy: 0.7334\n",
      "Epoch 169/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1769 - accuracy: 0.7405 - val_loss: 0.1870 - val_accuracy: 0.7337\n",
      "Epoch 170/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1740 - accuracy: 0.7464 - val_loss: 0.1856 - val_accuracy: 0.7370\n",
      "Epoch 171/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1777 - accuracy: 0.7409 - val_loss: 0.1863 - val_accuracy: 0.7367\n",
      "Epoch 172/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1778 - accuracy: 0.7401 - val_loss: 0.1871 - val_accuracy: 0.7362\n",
      "Epoch 173/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1766 - accuracy: 0.7427 - val_loss: 0.1866 - val_accuracy: 0.7365\n",
      "Epoch 174/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1755 - accuracy: 0.7421 - val_loss: 0.1859 - val_accuracy: 0.7359\n",
      "Epoch 175/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1769 - accuracy: 0.7412 - val_loss: 0.1869 - val_accuracy: 0.7330\n",
      "Epoch 176/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1776 - accuracy: 0.7412 - val_loss: 0.1868 - val_accuracy: 0.7330\n",
      "Epoch 177/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1800 - accuracy: 0.7350 - val_loss: 0.1860 - val_accuracy: 0.7343\n",
      "Epoch 178/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1770 - accuracy: 0.7389 - val_loss: 0.1858 - val_accuracy: 0.7372\n",
      "Epoch 179/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1761 - accuracy: 0.7454 - val_loss: 0.1857 - val_accuracy: 0.7370\n",
      "Epoch 180/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1762 - accuracy: 0.7445 - val_loss: 0.1855 - val_accuracy: 0.7366\n",
      "Epoch 181/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1762 - accuracy: 0.7415 - val_loss: 0.1863 - val_accuracy: 0.7367\n",
      "Epoch 182/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1759 - accuracy: 0.7426 - val_loss: 0.1858 - val_accuracy: 0.7357\n",
      "Epoch 183/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1762 - accuracy: 0.7431 - val_loss: 0.1862 - val_accuracy: 0.7353\n",
      "Epoch 184/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1759 - accuracy: 0.7460 - val_loss: 0.1855 - val_accuracy: 0.7354\n",
      "Epoch 185/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1751 - accuracy: 0.7446 - val_loss: 0.1870 - val_accuracy: 0.7323\n",
      "Epoch 186/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1762 - accuracy: 0.7406 - val_loss: 0.1882 - val_accuracy: 0.7314\n",
      "Epoch 187/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1742 - accuracy: 0.7477 - val_loss: 0.1864 - val_accuracy: 0.7356\n",
      "Epoch 188/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1773 - accuracy: 0.7415 - val_loss: 0.1865 - val_accuracy: 0.7365\n",
      "Epoch 189/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1753 - accuracy: 0.7448 - val_loss: 0.1862 - val_accuracy: 0.7365\n",
      "Epoch 190/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1750 - accuracy: 0.7462 - val_loss: 0.1860 - val_accuracy: 0.7368\n",
      "Epoch 191/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1737 - accuracy: 0.7494 - val_loss: 0.1855 - val_accuracy: 0.7362\n",
      "Epoch 192/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1750 - accuracy: 0.7436 - val_loss: 0.1871 - val_accuracy: 0.7337\n",
      "Epoch 193/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1761 - accuracy: 0.7465 - val_loss: 0.1874 - val_accuracy: 0.7353\n",
      "Epoch 194/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1772 - accuracy: 0.7428 - val_loss: 0.1871 - val_accuracy: 0.7350\n",
      "Epoch 195/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1761 - accuracy: 0.7440 - val_loss: 0.1855 - val_accuracy: 0.7374\n",
      "Epoch 196/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1747 - accuracy: 0.7429 - val_loss: 0.1856 - val_accuracy: 0.7340\n",
      "Epoch 197/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1791 - accuracy: 0.7360 - val_loss: 0.1854 - val_accuracy: 0.7372\n",
      "Epoch 198/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1773 - accuracy: 0.7425 - val_loss: 0.1864 - val_accuracy: 0.7324\n",
      "Epoch 199/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1753 - accuracy: 0.7450 - val_loss: 0.1862 - val_accuracy: 0.7352\n",
      "Epoch 200/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1784 - accuracy: 0.7380 - val_loss: 0.1857 - val_accuracy: 0.7362\n",
      "Epoch 201/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1743 - accuracy: 0.7447 - val_loss: 0.1862 - val_accuracy: 0.7354\n",
      "Epoch 202/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1759 - accuracy: 0.7442 - val_loss: 0.1857 - val_accuracy: 0.7361\n",
      "Epoch 203/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1761 - accuracy: 0.7394 - val_loss: 0.1858 - val_accuracy: 0.7366\n",
      "Epoch 204/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1769 - accuracy: 0.7402 - val_loss: 0.1861 - val_accuracy: 0.7367\n",
      "Epoch 205/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1765 - accuracy: 0.7419 - val_loss: 0.1865 - val_accuracy: 0.7370\n",
      "Epoch 206/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1774 - accuracy: 0.7395 - val_loss: 0.1864 - val_accuracy: 0.7363\n",
      "Epoch 207/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1749 - accuracy: 0.7421 - val_loss: 0.1871 - val_accuracy: 0.7350\n",
      "Epoch 208/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1771 - accuracy: 0.7418 - val_loss: 0.1860 - val_accuracy: 0.7353\n",
      "Epoch 209/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1765 - accuracy: 0.7419 - val_loss: 0.1867 - val_accuracy: 0.7332\n",
      "Epoch 210/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1762 - accuracy: 0.7438 - val_loss: 0.1875 - val_accuracy: 0.7321\n",
      "Epoch 211/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1757 - accuracy: 0.7444 - val_loss: 0.1859 - val_accuracy: 0.7367\n",
      "Epoch 212/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1770 - accuracy: 0.7393 - val_loss: 0.1862 - val_accuracy: 0.7358\n",
      "Epoch 213/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1774 - accuracy: 0.7417 - val_loss: 0.1865 - val_accuracy: 0.7350\n",
      "Epoch 214/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1767 - accuracy: 0.7400 - val_loss: 0.1855 - val_accuracy: 0.7366\n",
      "Epoch 215/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1759 - accuracy: 0.7445 - val_loss: 0.1871 - val_accuracy: 0.7321\n",
      "Epoch 216/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1742 - accuracy: 0.7465 - val_loss: 0.1867 - val_accuracy: 0.7340\n",
      "Epoch 217/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1742 - accuracy: 0.7471 - val_loss: 0.1874 - val_accuracy: 0.7313\n",
      "Epoch 218/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1767 - accuracy: 0.7420 - val_loss: 0.1870 - val_accuracy: 0.7353\n",
      "Epoch 219/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1762 - accuracy: 0.7415 - val_loss: 0.1862 - val_accuracy: 0.7367\n",
      "Epoch 220/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1757 - accuracy: 0.7441 - val_loss: 0.1865 - val_accuracy: 0.7368\n",
      "Epoch 221/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1786 - accuracy: 0.7408 - val_loss: 0.1861 - val_accuracy: 0.7334\n",
      "Epoch 222/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1749 - accuracy: 0.7448 - val_loss: 0.1855 - val_accuracy: 0.7380\n",
      "Epoch 223/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1772 - accuracy: 0.7420 - val_loss: 0.1860 - val_accuracy: 0.7366\n",
      "Epoch 224/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1760 - accuracy: 0.7471 - val_loss: 0.1857 - val_accuracy: 0.7372\n",
      "Epoch 225/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1759 - accuracy: 0.7469 - val_loss: 0.1872 - val_accuracy: 0.7358\n",
      "Epoch 226/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1777 - accuracy: 0.7418 - val_loss: 0.1871 - val_accuracy: 0.7336\n",
      "Epoch 227/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1765 - accuracy: 0.7427 - val_loss: 0.1866 - val_accuracy: 0.7354\n",
      "Epoch 228/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1761 - accuracy: 0.7420 - val_loss: 0.1867 - val_accuracy: 0.7358\n",
      "Epoch 229/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1753 - accuracy: 0.7449 - val_loss: 0.1867 - val_accuracy: 0.7344\n",
      "Epoch 230/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1780 - accuracy: 0.7389 - val_loss: 0.1862 - val_accuracy: 0.7344\n",
      "Epoch 231/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1791 - accuracy: 0.7392 - val_loss: 0.1861 - val_accuracy: 0.7353\n",
      "Epoch 232/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1765 - accuracy: 0.7431 - val_loss: 0.1876 - val_accuracy: 0.7296\n",
      "Epoch 233/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1776 - accuracy: 0.7397 - val_loss: 0.1862 - val_accuracy: 0.7345\n",
      "Epoch 234/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1769 - accuracy: 0.7433 - val_loss: 0.1870 - val_accuracy: 0.7326\n",
      "Epoch 235/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1765 - accuracy: 0.7441 - val_loss: 0.1867 - val_accuracy: 0.7321\n",
      "Epoch 236/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1766 - accuracy: 0.7401 - val_loss: 0.1864 - val_accuracy: 0.7330\n",
      "Epoch 237/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1755 - accuracy: 0.7440 - val_loss: 0.1867 - val_accuracy: 0.7356\n",
      "Epoch 238/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1773 - accuracy: 0.7408 - val_loss: 0.1866 - val_accuracy: 0.7340\n",
      "Epoch 239/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1761 - accuracy: 0.7426 - val_loss: 0.1864 - val_accuracy: 0.7326\n",
      "Epoch 240/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1769 - accuracy: 0.7415 - val_loss: 0.1863 - val_accuracy: 0.7359\n",
      "Epoch 241/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1766 - accuracy: 0.7438 - val_loss: 0.1868 - val_accuracy: 0.7359\n",
      "Epoch 242/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1758 - accuracy: 0.7447 - val_loss: 0.1863 - val_accuracy: 0.7365\n",
      "Epoch 243/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1739 - accuracy: 0.7481 - val_loss: 0.1868 - val_accuracy: 0.7345\n",
      "Epoch 244/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1761 - accuracy: 0.7425 - val_loss: 0.1871 - val_accuracy: 0.7337\n",
      "Epoch 245/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1764 - accuracy: 0.7413 - val_loss: 0.1867 - val_accuracy: 0.7352\n",
      "Epoch 246/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1761 - accuracy: 0.7427 - val_loss: 0.1866 - val_accuracy: 0.7326\n",
      "Epoch 247/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1777 - accuracy: 0.7387 - val_loss: 0.1878 - val_accuracy: 0.7308\n",
      "Epoch 248/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1782 - accuracy: 0.7386 - val_loss: 0.1864 - val_accuracy: 0.7344\n",
      "Epoch 249/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1769 - accuracy: 0.7428 - val_loss: 0.1875 - val_accuracy: 0.7297\n",
      "Epoch 250/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1761 - accuracy: 0.7449 - val_loss: 0.1874 - val_accuracy: 0.7321\n",
      "Epoch 251/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1745 - accuracy: 0.7465 - val_loss: 0.1865 - val_accuracy: 0.7357\n",
      "Epoch 252/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1762 - accuracy: 0.7445 - val_loss: 0.1863 - val_accuracy: 0.7336\n",
      "Epoch 253/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1761 - accuracy: 0.7457 - val_loss: 0.1867 - val_accuracy: 0.7331\n",
      "Epoch 254/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1774 - accuracy: 0.7418 - val_loss: 0.1861 - val_accuracy: 0.7353\n",
      "Epoch 255/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1763 - accuracy: 0.7404 - val_loss: 0.1862 - val_accuracy: 0.7330\n",
      "Epoch 256/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1739 - accuracy: 0.7456 - val_loss: 0.1872 - val_accuracy: 0.7334\n",
      "Epoch 257/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1773 - accuracy: 0.7385 - val_loss: 0.1859 - val_accuracy: 0.7357\n",
      "Epoch 258/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1749 - accuracy: 0.7462 - val_loss: 0.1854 - val_accuracy: 0.7365\n",
      "Epoch 259/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1792 - accuracy: 0.7360 - val_loss: 0.1862 - val_accuracy: 0.7365\n",
      "Epoch 260/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1764 - accuracy: 0.7416 - val_loss: 0.1869 - val_accuracy: 0.7324\n",
      "Epoch 261/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1750 - accuracy: 0.7450 - val_loss: 0.1861 - val_accuracy: 0.7341\n",
      "Epoch 262/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1778 - accuracy: 0.7371 - val_loss: 0.1858 - val_accuracy: 0.7363\n",
      "Epoch 263/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1749 - accuracy: 0.7455 - val_loss: 0.1853 - val_accuracy: 0.7356\n",
      "Epoch 264/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1757 - accuracy: 0.7414 - val_loss: 0.1863 - val_accuracy: 0.7341\n",
      "Epoch 265/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1770 - accuracy: 0.7403 - val_loss: 0.1863 - val_accuracy: 0.7327\n",
      "Epoch 266/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1746 - accuracy: 0.7461 - val_loss: 0.1879 - val_accuracy: 0.7328\n",
      "Epoch 267/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1767 - accuracy: 0.7392 - val_loss: 0.1859 - val_accuracy: 0.7337\n",
      "Epoch 268/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1773 - accuracy: 0.7377 - val_loss: 0.1852 - val_accuracy: 0.7367\n",
      "Epoch 269/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1764 - accuracy: 0.7425 - val_loss: 0.1856 - val_accuracy: 0.7366\n",
      "Epoch 270/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1752 - accuracy: 0.7469 - val_loss: 0.1856 - val_accuracy: 0.7362\n",
      "Epoch 271/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1778 - accuracy: 0.7403 - val_loss: 0.1861 - val_accuracy: 0.7357\n",
      "Epoch 272/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1768 - accuracy: 0.7415 - val_loss: 0.1860 - val_accuracy: 0.7344\n",
      "Epoch 273/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1761 - accuracy: 0.7426 - val_loss: 0.1869 - val_accuracy: 0.7353\n",
      "Epoch 274/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1776 - accuracy: 0.7415 - val_loss: 0.1868 - val_accuracy: 0.7331\n",
      "Epoch 275/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1767 - accuracy: 0.7404 - val_loss: 0.1866 - val_accuracy: 0.7370\n",
      "Epoch 276/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1789 - accuracy: 0.7392 - val_loss: 0.1865 - val_accuracy: 0.7358\n",
      "Epoch 277/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1757 - accuracy: 0.7420 - val_loss: 0.1869 - val_accuracy: 0.7359\n",
      "Epoch 278/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1788 - accuracy: 0.7348 - val_loss: 0.1868 - val_accuracy: 0.7370\n",
      "Epoch 279/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1764 - accuracy: 0.7425 - val_loss: 0.1861 - val_accuracy: 0.7368\n",
      "Epoch 280/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1776 - accuracy: 0.7401 - val_loss: 0.1860 - val_accuracy: 0.7371\n",
      "Epoch 281/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1752 - accuracy: 0.7435 - val_loss: 0.1860 - val_accuracy: 0.7368\n",
      "Epoch 282/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1736 - accuracy: 0.7479 - val_loss: 0.1857 - val_accuracy: 0.7372\n",
      "Epoch 283/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1762 - accuracy: 0.7415 - val_loss: 0.1870 - val_accuracy: 0.7362\n",
      "Epoch 284/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1770 - accuracy: 0.7417 - val_loss: 0.1863 - val_accuracy: 0.7349\n",
      "Epoch 285/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1774 - accuracy: 0.7397 - val_loss: 0.1861 - val_accuracy: 0.7357\n",
      "Epoch 286/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1764 - accuracy: 0.7449 - val_loss: 0.1856 - val_accuracy: 0.7361\n",
      "Epoch 287/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1741 - accuracy: 0.7448 - val_loss: 0.1855 - val_accuracy: 0.7359\n",
      "Epoch 288/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1762 - accuracy: 0.7427 - val_loss: 0.1859 - val_accuracy: 0.7361\n",
      "Epoch 289/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1774 - accuracy: 0.7391 - val_loss: 0.1854 - val_accuracy: 0.7365\n",
      "Epoch 290/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1747 - accuracy: 0.7467 - val_loss: 0.1868 - val_accuracy: 0.7323\n",
      "Epoch 291/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1754 - accuracy: 0.7418 - val_loss: 0.1872 - val_accuracy: 0.7319\n",
      "Epoch 292/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1786 - accuracy: 0.7389 - val_loss: 0.1859 - val_accuracy: 0.7337\n",
      "Epoch 293/1000\n",
      "563/563 [==============================] - ETA: 0s - loss: 0.1762 - accuracy: 0.74 - 1s 1ms/step - loss: 0.1762 - accuracy: 0.7428 - val_loss: 0.1859 - val_accuracy: 0.7358\n",
      "Epoch 294/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1772 - accuracy: 0.7428 - val_loss: 0.1861 - val_accuracy: 0.7366\n",
      "Epoch 295/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1744 - accuracy: 0.7453 - val_loss: 0.1861 - val_accuracy: 0.7345\n",
      "Epoch 296/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1751 - accuracy: 0.7455 - val_loss: 0.1867 - val_accuracy: 0.7310\n",
      "Epoch 297/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1765 - accuracy: 0.7434 - val_loss: 0.1858 - val_accuracy: 0.7359\n",
      "Epoch 298/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1746 - accuracy: 0.7469 - val_loss: 0.1868 - val_accuracy: 0.7315\n",
      "Epoch 299/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1776 - accuracy: 0.7377 - val_loss: 0.1866 - val_accuracy: 0.7317\n",
      "Epoch 300/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1738 - accuracy: 0.7487 - val_loss: 0.1859 - val_accuracy: 0.7350\n",
      "Epoch 301/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1800 - accuracy: 0.7364 - val_loss: 0.1869 - val_accuracy: 0.7354\n",
      "Epoch 302/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1777 - accuracy: 0.7379 - val_loss: 0.1859 - val_accuracy: 0.7358\n",
      "Epoch 303/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1790 - accuracy: 0.7384 - val_loss: 0.1857 - val_accuracy: 0.7371\n",
      "Epoch 304/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1770 - accuracy: 0.7421 - val_loss: 0.1862 - val_accuracy: 0.7362\n",
      "Epoch 305/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1750 - accuracy: 0.7443 - val_loss: 0.1876 - val_accuracy: 0.7324\n",
      "Epoch 306/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1758 - accuracy: 0.7411 - val_loss: 0.1871 - val_accuracy: 0.7362\n",
      "Epoch 307/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1759 - accuracy: 0.7424 - val_loss: 0.1864 - val_accuracy: 0.7366\n",
      "Epoch 308/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1770 - accuracy: 0.7398 - val_loss: 0.1869 - val_accuracy: 0.7324\n",
      "Epoch 309/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1764 - accuracy: 0.7426 - val_loss: 0.1864 - val_accuracy: 0.7354\n",
      "Epoch 310/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1756 - accuracy: 0.7453 - val_loss: 0.1862 - val_accuracy: 0.7354\n",
      "Epoch 311/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1769 - accuracy: 0.7416 - val_loss: 0.1863 - val_accuracy: 0.7363\n",
      "Epoch 312/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1753 - accuracy: 0.7426 - val_loss: 0.1869 - val_accuracy: 0.7330\n",
      "Epoch 313/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1768 - accuracy: 0.7393 - val_loss: 0.1864 - val_accuracy: 0.7361\n",
      "Epoch 314/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1758 - accuracy: 0.7432 - val_loss: 0.1864 - val_accuracy: 0.7361\n",
      "Epoch 315/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1758 - accuracy: 0.7435 - val_loss: 0.1872 - val_accuracy: 0.7353\n",
      "Epoch 316/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1764 - accuracy: 0.7418 - val_loss: 0.1871 - val_accuracy: 0.7357\n",
      "Epoch 317/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1736 - accuracy: 0.7465 - val_loss: 0.1865 - val_accuracy: 0.7354\n",
      "Epoch 318/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1753 - accuracy: 0.7425 - val_loss: 0.1867 - val_accuracy: 0.7357\n",
      "Epoch 319/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1772 - accuracy: 0.7401 - val_loss: 0.1868 - val_accuracy: 0.7331\n",
      "Epoch 320/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1760 - accuracy: 0.7438 - val_loss: 0.1862 - val_accuracy: 0.7358\n",
      "Epoch 321/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1755 - accuracy: 0.7434 - val_loss: 0.1862 - val_accuracy: 0.7362\n",
      "Epoch 322/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1767 - accuracy: 0.7423 - val_loss: 0.1864 - val_accuracy: 0.7359\n",
      "Epoch 323/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1784 - accuracy: 0.7372 - val_loss: 0.1865 - val_accuracy: 0.7367\n",
      "Epoch 324/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1745 - accuracy: 0.7452 - val_loss: 0.1864 - val_accuracy: 0.7356\n",
      "Epoch 325/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1770 - accuracy: 0.7403 - val_loss: 0.1870 - val_accuracy: 0.7345\n",
      "Epoch 326/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1743 - accuracy: 0.7474 - val_loss: 0.1866 - val_accuracy: 0.7334\n",
      "Epoch 327/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1751 - accuracy: 0.7446 - val_loss: 0.1877 - val_accuracy: 0.7330\n",
      "Epoch 328/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1742 - accuracy: 0.7458 - val_loss: 0.1865 - val_accuracy: 0.7349\n",
      "Epoch 329/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1762 - accuracy: 0.7447 - val_loss: 0.1867 - val_accuracy: 0.7350\n",
      "Epoch 330/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1785 - accuracy: 0.7405 - val_loss: 0.1862 - val_accuracy: 0.7356\n",
      "Epoch 331/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1750 - accuracy: 0.7443 - val_loss: 0.1875 - val_accuracy: 0.7348\n",
      "Epoch 332/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1767 - accuracy: 0.7408 - val_loss: 0.1861 - val_accuracy: 0.7367\n",
      "Epoch 333/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1796 - accuracy: 0.7350 - val_loss: 0.1866 - val_accuracy: 0.7361\n",
      "Epoch 334/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1772 - accuracy: 0.7407 - val_loss: 0.1868 - val_accuracy: 0.7339\n",
      "Epoch 335/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1769 - accuracy: 0.7402 - val_loss: 0.1876 - val_accuracy: 0.7328\n",
      "Epoch 336/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1760 - accuracy: 0.7421 - val_loss: 0.1867 - val_accuracy: 0.7350\n",
      "Epoch 337/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1755 - accuracy: 0.7451 - val_loss: 0.1865 - val_accuracy: 0.7352\n",
      "Epoch 338/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1752 - accuracy: 0.7456 - val_loss: 0.1862 - val_accuracy: 0.7350\n",
      "Epoch 339/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1778 - accuracy: 0.7411 - val_loss: 0.1858 - val_accuracy: 0.7356\n",
      "Epoch 340/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1774 - accuracy: 0.7392 - val_loss: 0.1863 - val_accuracy: 0.7358\n",
      "Epoch 341/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1765 - accuracy: 0.7419 - val_loss: 0.1864 - val_accuracy: 0.7358\n",
      "Epoch 342/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1790 - accuracy: 0.7363 - val_loss: 0.1864 - val_accuracy: 0.7356\n",
      "Epoch 343/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1732 - accuracy: 0.7483 - val_loss: 0.1873 - val_accuracy: 0.7356\n",
      "Epoch 344/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1728 - accuracy: 0.7504 - val_loss: 0.1862 - val_accuracy: 0.7361\n",
      "Epoch 345/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1739 - accuracy: 0.7494 - val_loss: 0.1872 - val_accuracy: 0.7356\n",
      "Epoch 346/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1776 - accuracy: 0.7398 - val_loss: 0.1859 - val_accuracy: 0.7370\n",
      "Epoch 347/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1737 - accuracy: 0.7478 - val_loss: 0.1855 - val_accuracy: 0.7367\n",
      "Epoch 348/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1765 - accuracy: 0.7415 - val_loss: 0.1865 - val_accuracy: 0.7367\n",
      "Epoch 349/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1737 - accuracy: 0.7486 - val_loss: 0.1857 - val_accuracy: 0.7371\n",
      "Epoch 350/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1786 - accuracy: 0.7370 - val_loss: 0.1864 - val_accuracy: 0.7361\n",
      "Epoch 351/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1774 - accuracy: 0.7394 - val_loss: 0.1862 - val_accuracy: 0.7350\n",
      "Epoch 352/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1774 - accuracy: 0.7394 - val_loss: 0.1875 - val_accuracy: 0.7353\n",
      "Epoch 353/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1763 - accuracy: 0.7454 - val_loss: 0.1861 - val_accuracy: 0.7357\n",
      "Epoch 354/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1780 - accuracy: 0.7377 - val_loss: 0.1859 - val_accuracy: 0.7367\n",
      "Epoch 355/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1749 - accuracy: 0.7429 - val_loss: 0.1867 - val_accuracy: 0.7348\n",
      "Epoch 356/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1773 - accuracy: 0.7399 - val_loss: 0.1862 - val_accuracy: 0.7363\n",
      "Epoch 357/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1763 - accuracy: 0.7401 - val_loss: 0.1866 - val_accuracy: 0.7362\n",
      "Epoch 358/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1762 - accuracy: 0.7455 - val_loss: 0.1864 - val_accuracy: 0.7380\n",
      "Epoch 359/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1804 - accuracy: 0.7346 - val_loss: 0.1860 - val_accuracy: 0.7376\n",
      "Epoch 360/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1761 - accuracy: 0.7433 - val_loss: 0.1862 - val_accuracy: 0.7366\n",
      "Epoch 361/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1774 - accuracy: 0.7409 - val_loss: 0.1866 - val_accuracy: 0.7363\n",
      "Epoch 362/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1768 - accuracy: 0.7393 - val_loss: 0.1873 - val_accuracy: 0.7362\n",
      "Epoch 363/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1768 - accuracy: 0.7402 - val_loss: 0.1864 - val_accuracy: 0.7367\n",
      "Epoch 364/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1753 - accuracy: 0.7450 - val_loss: 0.1873 - val_accuracy: 0.7359\n",
      "Epoch 365/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1743 - accuracy: 0.7485 - val_loss: 0.1872 - val_accuracy: 0.7353\n",
      "Epoch 366/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1765 - accuracy: 0.7438 - val_loss: 0.1889 - val_accuracy: 0.7317\n",
      "Epoch 367/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1761 - accuracy: 0.7414 - val_loss: 0.1902 - val_accuracy: 0.7185\n",
      "Epoch 368/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1749 - accuracy: 0.7415 - val_loss: 0.1873 - val_accuracy: 0.7361\n",
      "Epoch 369/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1759 - accuracy: 0.7423 - val_loss: 0.1859 - val_accuracy: 0.7380\n",
      "Epoch 370/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1766 - accuracy: 0.7435 - val_loss: 0.1864 - val_accuracy: 0.7358\n",
      "Epoch 371/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1739 - accuracy: 0.7477 - val_loss: 0.1864 - val_accuracy: 0.7359\n",
      "Epoch 372/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1776 - accuracy: 0.7408 - val_loss: 0.1866 - val_accuracy: 0.7361\n",
      "Epoch 373/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1771 - accuracy: 0.7419 - val_loss: 0.1868 - val_accuracy: 0.7361\n",
      "Epoch 374/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1756 - accuracy: 0.7445 - val_loss: 0.1875 - val_accuracy: 0.7375\n",
      "Epoch 375/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1765 - accuracy: 0.7452 - val_loss: 0.1873 - val_accuracy: 0.7372\n",
      "Epoch 376/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1754 - accuracy: 0.7453 - val_loss: 0.1863 - val_accuracy: 0.7363\n",
      "Epoch 377/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1770 - accuracy: 0.7396 - val_loss: 0.1863 - val_accuracy: 0.7362\n",
      "Epoch 378/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1745 - accuracy: 0.7437 - val_loss: 0.1861 - val_accuracy: 0.7363\n",
      "Epoch 379/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1748 - accuracy: 0.7469 - val_loss: 0.1869 - val_accuracy: 0.7354\n",
      "Epoch 380/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1761 - accuracy: 0.7426 - val_loss: 0.1874 - val_accuracy: 0.7356\n",
      "Epoch 381/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1734 - accuracy: 0.7499 - val_loss: 0.1865 - val_accuracy: 0.7374\n",
      "Epoch 382/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1748 - accuracy: 0.7453 - val_loss: 0.1862 - val_accuracy: 0.7367\n",
      "Epoch 383/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1736 - accuracy: 0.7485 - val_loss: 0.1876 - val_accuracy: 0.7354\n",
      "Epoch 384/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1752 - accuracy: 0.7460 - val_loss: 0.1872 - val_accuracy: 0.7368\n",
      "Epoch 385/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1757 - accuracy: 0.7423 - val_loss: 0.1866 - val_accuracy: 0.7363\n",
      "Epoch 386/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1792 - accuracy: 0.7375 - val_loss: 0.1862 - val_accuracy: 0.7374\n",
      "Epoch 387/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1761 - accuracy: 0.7440 - val_loss: 0.1866 - val_accuracy: 0.7366\n",
      "Epoch 388/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1751 - accuracy: 0.7428 - val_loss: 0.1875 - val_accuracy: 0.7376\n",
      "Epoch 389/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1760 - accuracy: 0.7424 - val_loss: 0.1862 - val_accuracy: 0.7372\n",
      "Epoch 390/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1760 - accuracy: 0.7440 - val_loss: 0.1865 - val_accuracy: 0.7368\n",
      "Epoch 391/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1781 - accuracy: 0.7382 - val_loss: 0.1861 - val_accuracy: 0.7376\n",
      "Epoch 392/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1762 - accuracy: 0.7434 - val_loss: 0.1880 - val_accuracy: 0.7323\n",
      "Epoch 393/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1767 - accuracy: 0.7403 - val_loss: 0.1869 - val_accuracy: 0.7370\n",
      "Epoch 394/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1752 - accuracy: 0.7463 - val_loss: 0.1872 - val_accuracy: 0.7357\n",
      "Epoch 395/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1752 - accuracy: 0.7457 - val_loss: 0.1871 - val_accuracy: 0.7353\n",
      "Epoch 396/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1748 - accuracy: 0.7427 - val_loss: 0.1876 - val_accuracy: 0.7349\n",
      "Epoch 397/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1774 - accuracy: 0.7429 - val_loss: 0.1878 - val_accuracy: 0.7359\n",
      "Epoch 398/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1772 - accuracy: 0.7377 - val_loss: 0.1877 - val_accuracy: 0.7362\n",
      "Epoch 399/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1759 - accuracy: 0.7436 - val_loss: 0.1878 - val_accuracy: 0.7336\n",
      "Epoch 400/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1746 - accuracy: 0.7457 - val_loss: 0.1889 - val_accuracy: 0.7326\n",
      "Epoch 401/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1766 - accuracy: 0.7383 - val_loss: 0.1877 - val_accuracy: 0.7359\n",
      "Epoch 402/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1761 - accuracy: 0.7445 - val_loss: 0.1889 - val_accuracy: 0.7362\n",
      "Epoch 403/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1731 - accuracy: 0.7502 - val_loss: 0.1879 - val_accuracy: 0.7349\n",
      "Epoch 404/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1763 - accuracy: 0.7413 - val_loss: 0.1885 - val_accuracy: 0.7332\n",
      "Epoch 405/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1759 - accuracy: 0.7437 - val_loss: 0.1870 - val_accuracy: 0.7362\n",
      "Epoch 406/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1800 - accuracy: 0.7367 - val_loss: 0.1875 - val_accuracy: 0.7365\n",
      "Epoch 407/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1749 - accuracy: 0.7438 - val_loss: 0.1869 - val_accuracy: 0.7350\n",
      "Epoch 408/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1747 - accuracy: 0.7429 - val_loss: 0.1866 - val_accuracy: 0.7374\n",
      "Epoch 409/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1762 - accuracy: 0.7431 - val_loss: 0.1882 - val_accuracy: 0.7334\n",
      "Epoch 410/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1765 - accuracy: 0.7423 - val_loss: 0.1874 - val_accuracy: 0.7362\n",
      "Epoch 411/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1757 - accuracy: 0.7429 - val_loss: 0.1883 - val_accuracy: 0.7335\n",
      "Epoch 412/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1775 - accuracy: 0.7371 - val_loss: 0.1881 - val_accuracy: 0.7328\n",
      "Epoch 413/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1748 - accuracy: 0.7454 - val_loss: 0.1878 - val_accuracy: 0.7322\n",
      "Epoch 414/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1769 - accuracy: 0.7410 - val_loss: 0.1873 - val_accuracy: 0.7366\n",
      "Epoch 415/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1755 - accuracy: 0.7436 - val_loss: 0.1883 - val_accuracy: 0.7328\n",
      "Epoch 416/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1772 - accuracy: 0.7406 - val_loss: 0.1882 - val_accuracy: 0.7331\n",
      "Epoch 417/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1754 - accuracy: 0.7420 - val_loss: 0.1878 - val_accuracy: 0.7315\n",
      "Epoch 418/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1772 - accuracy: 0.7391 - val_loss: 0.1868 - val_accuracy: 0.7337\n",
      "Epoch 419/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1766 - accuracy: 0.7408 - val_loss: 0.1867 - val_accuracy: 0.7341\n",
      "Epoch 420/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1768 - accuracy: 0.7422 - val_loss: 0.1868 - val_accuracy: 0.7345\n",
      "Epoch 421/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1786 - accuracy: 0.7368 - val_loss: 0.1870 - val_accuracy: 0.7336\n",
      "Epoch 422/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1760 - accuracy: 0.7459 - val_loss: 0.1873 - val_accuracy: 0.7362\n",
      "Epoch 423/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1751 - accuracy: 0.7460 - val_loss: 0.1875 - val_accuracy: 0.7327\n",
      "Epoch 424/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1775 - accuracy: 0.7388 - val_loss: 0.1874 - val_accuracy: 0.7319\n",
      "Epoch 425/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1769 - accuracy: 0.7406 - val_loss: 0.1869 - val_accuracy: 0.7362\n",
      "Epoch 426/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1769 - accuracy: 0.7418 - val_loss: 0.1872 - val_accuracy: 0.7330\n",
      "Epoch 427/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1754 - accuracy: 0.7446 - val_loss: 0.1876 - val_accuracy: 0.7334\n",
      "Epoch 428/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1759 - accuracy: 0.7414 - val_loss: 0.1871 - val_accuracy: 0.7383\n",
      "Epoch 429/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1752 - accuracy: 0.7461 - val_loss: 0.1869 - val_accuracy: 0.7370\n",
      "Epoch 430/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1744 - accuracy: 0.7468 - val_loss: 0.1867 - val_accuracy: 0.7367\n",
      "Epoch 431/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1728 - accuracy: 0.7492 - val_loss: 0.1875 - val_accuracy: 0.7359\n",
      "Epoch 432/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1755 - accuracy: 0.7433 - val_loss: 0.1878 - val_accuracy: 0.7370\n",
      "Epoch 433/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1764 - accuracy: 0.7433 - val_loss: 0.1875 - val_accuracy: 0.7322\n",
      "Epoch 434/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1750 - accuracy: 0.7447 - val_loss: 0.1869 - val_accuracy: 0.7362\n",
      "Epoch 435/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1761 - accuracy: 0.7426 - val_loss: 0.1874 - val_accuracy: 0.7358\n",
      "Epoch 436/1000\n",
      "563/563 [==============================] - 1s 997us/step - loss: 0.1781 - accuracy: 0.7410 - val_loss: 0.1882 - val_accuracy: 0.7317\n",
      "Epoch 437/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1759 - accuracy: 0.7461 - val_loss: 0.1879 - val_accuracy: 0.7362\n",
      "Epoch 438/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1765 - accuracy: 0.7451 - val_loss: 0.1886 - val_accuracy: 0.7349\n",
      "Epoch 439/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1772 - accuracy: 0.7422 - val_loss: 0.1876 - val_accuracy: 0.7368\n",
      "Epoch 440/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1751 - accuracy: 0.7446 - val_loss: 0.1880 - val_accuracy: 0.7323\n",
      "Epoch 441/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1769 - accuracy: 0.7383 - val_loss: 0.1872 - val_accuracy: 0.7352\n",
      "Epoch 442/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1750 - accuracy: 0.7440 - val_loss: 0.1876 - val_accuracy: 0.7348\n",
      "Epoch 443/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1760 - accuracy: 0.7430 - val_loss: 0.1876 - val_accuracy: 0.7363\n",
      "Epoch 444/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1732 - accuracy: 0.7479 - val_loss: 0.1879 - val_accuracy: 0.7350\n",
      "Epoch 445/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1764 - accuracy: 0.7416 - val_loss: 0.1881 - val_accuracy: 0.7317\n",
      "Epoch 446/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1733 - accuracy: 0.7461 - val_loss: 0.1879 - val_accuracy: 0.7348\n",
      "Epoch 447/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1755 - accuracy: 0.7423 - val_loss: 0.1883 - val_accuracy: 0.7331\n",
      "Epoch 448/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1759 - accuracy: 0.7420 - val_loss: 0.1886 - val_accuracy: 0.7340\n",
      "Epoch 449/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1760 - accuracy: 0.7446 - val_loss: 0.1882 - val_accuracy: 0.7349\n",
      "Epoch 450/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1742 - accuracy: 0.7442 - val_loss: 0.1888 - val_accuracy: 0.7352\n",
      "Epoch 451/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1768 - accuracy: 0.7408 - val_loss: 0.1879 - val_accuracy: 0.7353\n",
      "Epoch 452/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1753 - accuracy: 0.7437 - val_loss: 0.1896 - val_accuracy: 0.7287\n",
      "Epoch 453/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1768 - accuracy: 0.7447 - val_loss: 0.1880 - val_accuracy: 0.7353\n",
      "Epoch 454/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1751 - accuracy: 0.7436 - val_loss: 0.1878 - val_accuracy: 0.7349\n",
      "Epoch 455/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1791 - accuracy: 0.7378 - val_loss: 0.1878 - val_accuracy: 0.7362\n",
      "Epoch 456/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1774 - accuracy: 0.7391 - val_loss: 0.1875 - val_accuracy: 0.7362\n",
      "Epoch 457/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1773 - accuracy: 0.7421 - val_loss: 0.1884 - val_accuracy: 0.7362\n",
      "Epoch 458/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1747 - accuracy: 0.7452 - val_loss: 0.1880 - val_accuracy: 0.7358\n",
      "Epoch 459/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1776 - accuracy: 0.7410 - val_loss: 0.1880 - val_accuracy: 0.7361\n",
      "Epoch 460/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1742 - accuracy: 0.7451 - val_loss: 0.1893 - val_accuracy: 0.7337\n",
      "Epoch 461/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1753 - accuracy: 0.7429 - val_loss: 0.1871 - val_accuracy: 0.7354\n",
      "Epoch 462/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1764 - accuracy: 0.7438 - val_loss: 0.1875 - val_accuracy: 0.7358\n",
      "Epoch 463/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1760 - accuracy: 0.7439 - val_loss: 0.1885 - val_accuracy: 0.7335\n",
      "Epoch 464/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1738 - accuracy: 0.7468 - val_loss: 0.1882 - val_accuracy: 0.7327\n",
      "Epoch 465/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1757 - accuracy: 0.7437 - val_loss: 0.1866 - val_accuracy: 0.7350\n",
      "Epoch 466/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1751 - accuracy: 0.7470 - val_loss: 0.1870 - val_accuracy: 0.7358\n",
      "Epoch 467/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1788 - accuracy: 0.7381 - val_loss: 0.1871 - val_accuracy: 0.7359\n",
      "Epoch 468/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1761 - accuracy: 0.7426 - val_loss: 0.1884 - val_accuracy: 0.7346\n",
      "Epoch 469/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1744 - accuracy: 0.7436 - val_loss: 0.1871 - val_accuracy: 0.7359\n",
      "Epoch 470/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1747 - accuracy: 0.7461 - val_loss: 0.1870 - val_accuracy: 0.7359\n",
      "Epoch 471/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1765 - accuracy: 0.7424 - val_loss: 0.1877 - val_accuracy: 0.7359\n",
      "Epoch 472/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1769 - accuracy: 0.7440 - val_loss: 0.1899 - val_accuracy: 0.7296\n",
      "Epoch 473/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1756 - accuracy: 0.7408 - val_loss: 0.1881 - val_accuracy: 0.7353\n",
      "Epoch 474/1000\n",
      "563/563 [==============================] - 1s 991us/step - loss: 0.1781 - accuracy: 0.7396 - val_loss: 0.1870 - val_accuracy: 0.7362\n",
      "Epoch 475/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1764 - accuracy: 0.7418 - val_loss: 0.1880 - val_accuracy: 0.7345\n",
      "Epoch 476/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1772 - accuracy: 0.7405 - val_loss: 0.1881 - val_accuracy: 0.7361\n",
      "Epoch 477/1000\n",
      "563/563 [==============================] - 1s 979us/step - loss: 0.1742 - accuracy: 0.7453 - val_loss: 0.1883 - val_accuracy: 0.7348\n",
      "Epoch 478/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1749 - accuracy: 0.7462 - val_loss: 0.1885 - val_accuracy: 0.7353\n",
      "Epoch 479/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1745 - accuracy: 0.7478 - val_loss: 0.1883 - val_accuracy: 0.7368\n",
      "Epoch 480/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1739 - accuracy: 0.7478 - val_loss: 0.1885 - val_accuracy: 0.7332\n",
      "Epoch 481/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1749 - accuracy: 0.7454 - val_loss: 0.1881 - val_accuracy: 0.7359\n",
      "Epoch 482/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1758 - accuracy: 0.7458 - val_loss: 0.1875 - val_accuracy: 0.7359\n",
      "Epoch 483/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1754 - accuracy: 0.7416 - val_loss: 0.1877 - val_accuracy: 0.7350\n",
      "Epoch 484/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1761 - accuracy: 0.7430 - val_loss: 0.1887 - val_accuracy: 0.7345\n",
      "Epoch 485/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1735 - accuracy: 0.7492 - val_loss: 0.1882 - val_accuracy: 0.7334\n",
      "Epoch 486/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1783 - accuracy: 0.7418 - val_loss: 0.1875 - val_accuracy: 0.7345\n",
      "Epoch 487/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1758 - accuracy: 0.7433 - val_loss: 0.1884 - val_accuracy: 0.7336\n",
      "Epoch 488/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1751 - accuracy: 0.7425 - val_loss: 0.1873 - val_accuracy: 0.7349\n",
      "Epoch 489/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1756 - accuracy: 0.7441 - val_loss: 0.1879 - val_accuracy: 0.7332\n",
      "Epoch 490/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1753 - accuracy: 0.7462 - val_loss: 0.1879 - val_accuracy: 0.7324\n",
      "Epoch 491/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1748 - accuracy: 0.7449 - val_loss: 0.1875 - val_accuracy: 0.7362\n",
      "Epoch 492/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1764 - accuracy: 0.7436 - val_loss: 0.1875 - val_accuracy: 0.7371\n",
      "Epoch 493/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1740 - accuracy: 0.7454 - val_loss: 0.1872 - val_accuracy: 0.7363\n",
      "Epoch 494/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1743 - accuracy: 0.7475 - val_loss: 0.1887 - val_accuracy: 0.7349\n",
      "Epoch 495/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1758 - accuracy: 0.7411 - val_loss: 0.1874 - val_accuracy: 0.7368\n",
      "Epoch 496/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1743 - accuracy: 0.7469 - val_loss: 0.1874 - val_accuracy: 0.7332\n",
      "Epoch 497/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1741 - accuracy: 0.7460 - val_loss: 0.1885 - val_accuracy: 0.7359\n",
      "Epoch 498/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1737 - accuracy: 0.7476 - val_loss: 0.1885 - val_accuracy: 0.7292\n",
      "Epoch 499/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1744 - accuracy: 0.7442 - val_loss: 0.1882 - val_accuracy: 0.7354\n",
      "Epoch 500/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1748 - accuracy: 0.7458 - val_loss: 0.1883 - val_accuracy: 0.7305\n",
      "Epoch 501/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1766 - accuracy: 0.7417 - val_loss: 0.1874 - val_accuracy: 0.7362\n",
      "Epoch 502/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1759 - accuracy: 0.7430 - val_loss: 0.1878 - val_accuracy: 0.7352\n",
      "Epoch 503/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1747 - accuracy: 0.7470 - val_loss: 0.1897 - val_accuracy: 0.7322\n",
      "Epoch 504/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1757 - accuracy: 0.7441 - val_loss: 0.1878 - val_accuracy: 0.7337\n",
      "Epoch 505/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1776 - accuracy: 0.7395 - val_loss: 0.1874 - val_accuracy: 0.7350\n",
      "Epoch 506/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1761 - accuracy: 0.7403 - val_loss: 0.1873 - val_accuracy: 0.7365\n",
      "Epoch 507/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1785 - accuracy: 0.7397 - val_loss: 0.1869 - val_accuracy: 0.7371\n",
      "Epoch 508/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1782 - accuracy: 0.7385 - val_loss: 0.1880 - val_accuracy: 0.7327\n",
      "Epoch 509/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1742 - accuracy: 0.7439 - val_loss: 0.1873 - val_accuracy: 0.7356\n",
      "Epoch 510/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1737 - accuracy: 0.7498 - val_loss: 0.1871 - val_accuracy: 0.7343\n",
      "Epoch 511/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1740 - accuracy: 0.7489 - val_loss: 0.1869 - val_accuracy: 0.7352\n",
      "Epoch 512/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1763 - accuracy: 0.7410 - val_loss: 0.1872 - val_accuracy: 0.7359\n",
      "Epoch 513/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1757 - accuracy: 0.7407 - val_loss: 0.1870 - val_accuracy: 0.7363\n",
      "Epoch 514/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1752 - accuracy: 0.7451 - val_loss: 0.1874 - val_accuracy: 0.7337\n",
      "Epoch 515/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1757 - accuracy: 0.7414 - val_loss: 0.1881 - val_accuracy: 0.7346\n",
      "Epoch 516/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1737 - accuracy: 0.7483 - val_loss: 0.1871 - val_accuracy: 0.7359\n",
      "Epoch 517/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1772 - accuracy: 0.7413 - val_loss: 0.1884 - val_accuracy: 0.7363\n",
      "Epoch 518/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1766 - accuracy: 0.7428 - val_loss: 0.1876 - val_accuracy: 0.7345\n",
      "Epoch 519/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1782 - accuracy: 0.7403 - val_loss: 0.1879 - val_accuracy: 0.7362\n",
      "Epoch 520/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1753 - accuracy: 0.7436 - val_loss: 0.1877 - val_accuracy: 0.7358\n",
      "Epoch 521/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1753 - accuracy: 0.7426 - val_loss: 0.1878 - val_accuracy: 0.7337\n",
      "Epoch 522/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1763 - accuracy: 0.7449 - val_loss: 0.1866 - val_accuracy: 0.7365\n",
      "Epoch 523/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1768 - accuracy: 0.7410 - val_loss: 0.1870 - val_accuracy: 0.7366\n",
      "Epoch 524/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1771 - accuracy: 0.7409 - val_loss: 0.1876 - val_accuracy: 0.7359\n",
      "Epoch 525/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1767 - accuracy: 0.7415 - val_loss: 0.1869 - val_accuracy: 0.7375\n",
      "Epoch 526/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1751 - accuracy: 0.7438 - val_loss: 0.1869 - val_accuracy: 0.7366\n",
      "Epoch 527/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1765 - accuracy: 0.7404 - val_loss: 0.1871 - val_accuracy: 0.7363\n",
      "Epoch 528/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1773 - accuracy: 0.7394 - val_loss: 0.1879 - val_accuracy: 0.7350\n",
      "Epoch 529/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1754 - accuracy: 0.7456 - val_loss: 0.1880 - val_accuracy: 0.7326\n",
      "Epoch 530/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1738 - accuracy: 0.7484 - val_loss: 0.1876 - val_accuracy: 0.7323\n",
      "Epoch 531/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1749 - accuracy: 0.7435 - val_loss: 0.1880 - val_accuracy: 0.7350\n",
      "Epoch 532/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1760 - accuracy: 0.7429 - val_loss: 0.1874 - val_accuracy: 0.7354\n",
      "Epoch 533/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1732 - accuracy: 0.7464 - val_loss: 0.1878 - val_accuracy: 0.7368\n",
      "Epoch 534/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1754 - accuracy: 0.7441 - val_loss: 0.1878 - val_accuracy: 0.7346\n",
      "Epoch 535/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1760 - accuracy: 0.7445 - val_loss: 0.1883 - val_accuracy: 0.7336\n",
      "Epoch 536/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1754 - accuracy: 0.7441 - val_loss: 0.1874 - val_accuracy: 0.7359\n",
      "Epoch 537/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1763 - accuracy: 0.7383 - val_loss: 0.1876 - val_accuracy: 0.7356\n",
      "Epoch 538/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1764 - accuracy: 0.7423 - val_loss: 0.1877 - val_accuracy: 0.7368\n",
      "Epoch 539/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1770 - accuracy: 0.7391 - val_loss: 0.1878 - val_accuracy: 0.7368\n",
      "Epoch 540/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1763 - accuracy: 0.7457 - val_loss: 0.1872 - val_accuracy: 0.7365\n",
      "Epoch 541/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1749 - accuracy: 0.7433 - val_loss: 0.1876 - val_accuracy: 0.7336\n",
      "Epoch 542/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1725 - accuracy: 0.7485 - val_loss: 0.1868 - val_accuracy: 0.7367\n",
      "Epoch 543/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1756 - accuracy: 0.7448 - val_loss: 0.1880 - val_accuracy: 0.7365\n",
      "Epoch 544/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1760 - accuracy: 0.7408 - val_loss: 0.1869 - val_accuracy: 0.7349\n",
      "Epoch 545/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1765 - accuracy: 0.7418 - val_loss: 0.1881 - val_accuracy: 0.7357\n",
      "Epoch 546/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1771 - accuracy: 0.7388 - val_loss: 0.1873 - val_accuracy: 0.7353\n",
      "Epoch 547/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1743 - accuracy: 0.7462 - val_loss: 0.1873 - val_accuracy: 0.7339\n",
      "Epoch 548/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1741 - accuracy: 0.7463 - val_loss: 0.1878 - val_accuracy: 0.7348\n",
      "Epoch 549/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1754 - accuracy: 0.7420 - val_loss: 0.1883 - val_accuracy: 0.7336\n",
      "Epoch 550/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1771 - accuracy: 0.7389 - val_loss: 0.1875 - val_accuracy: 0.7353\n",
      "Epoch 551/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1759 - accuracy: 0.7416 - val_loss: 0.1877 - val_accuracy: 0.7346\n",
      "Epoch 552/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1776 - accuracy: 0.7370 - val_loss: 0.1881 - val_accuracy: 0.7341\n",
      "Epoch 553/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1770 - accuracy: 0.7410 - val_loss: 0.1873 - val_accuracy: 0.7357\n",
      "Epoch 554/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1749 - accuracy: 0.7434 - val_loss: 0.1875 - val_accuracy: 0.7319\n",
      "Epoch 555/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1730 - accuracy: 0.7495 - val_loss: 0.1869 - val_accuracy: 0.7367\n",
      "Epoch 556/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1736 - accuracy: 0.7486 - val_loss: 0.1873 - val_accuracy: 0.7348\n",
      "Epoch 557/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1749 - accuracy: 0.7427 - val_loss: 0.1866 - val_accuracy: 0.7354\n",
      "Epoch 558/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1755 - accuracy: 0.7420 - val_loss: 0.1880 - val_accuracy: 0.7370\n",
      "Epoch 559/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1776 - accuracy: 0.7391 - val_loss: 0.1874 - val_accuracy: 0.7370\n",
      "Epoch 560/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1775 - accuracy: 0.7395 - val_loss: 0.1867 - val_accuracy: 0.7383\n",
      "Epoch 561/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1758 - accuracy: 0.7416 - val_loss: 0.1867 - val_accuracy: 0.7372\n",
      "Epoch 562/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1756 - accuracy: 0.7430 - val_loss: 0.1865 - val_accuracy: 0.7359\n",
      "Epoch 563/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1770 - accuracy: 0.7393 - val_loss: 0.1869 - val_accuracy: 0.7359\n",
      "Epoch 564/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1767 - accuracy: 0.7374 - val_loss: 0.1883 - val_accuracy: 0.7328\n",
      "Epoch 565/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1743 - accuracy: 0.7444 - val_loss: 0.1877 - val_accuracy: 0.7366\n",
      "Epoch 566/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1740 - accuracy: 0.7490 - val_loss: 0.1873 - val_accuracy: 0.7340\n",
      "Epoch 567/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1758 - accuracy: 0.7398 - val_loss: 0.1870 - val_accuracy: 0.7375\n",
      "Epoch 568/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1764 - accuracy: 0.7411 - val_loss: 0.1876 - val_accuracy: 0.7361\n",
      "Epoch 569/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1756 - accuracy: 0.7415 - val_loss: 0.1880 - val_accuracy: 0.7315\n",
      "Epoch 570/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1766 - accuracy: 0.7416 - val_loss: 0.1874 - val_accuracy: 0.7359\n",
      "Epoch 571/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1753 - accuracy: 0.7469 - val_loss: 0.1878 - val_accuracy: 0.7343\n",
      "Epoch 572/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1749 - accuracy: 0.7437 - val_loss: 0.1877 - val_accuracy: 0.7383\n",
      "Epoch 573/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1763 - accuracy: 0.7406 - val_loss: 0.1873 - val_accuracy: 0.7368\n",
      "Epoch 574/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1773 - accuracy: 0.7432 - val_loss: 0.1890 - val_accuracy: 0.7331\n",
      "Epoch 575/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1740 - accuracy: 0.7479 - val_loss: 0.1871 - val_accuracy: 0.7365\n",
      "Epoch 576/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1761 - accuracy: 0.7425 - val_loss: 0.1877 - val_accuracy: 0.7361\n",
      "Epoch 577/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1751 - accuracy: 0.7449 - val_loss: 0.1871 - val_accuracy: 0.7367\n",
      "Epoch 578/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1756 - accuracy: 0.7454 - val_loss: 0.1872 - val_accuracy: 0.7363\n",
      "Epoch 579/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1734 - accuracy: 0.7462 - val_loss: 0.1882 - val_accuracy: 0.7304\n",
      "Epoch 580/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1760 - accuracy: 0.7415 - val_loss: 0.1874 - val_accuracy: 0.7358\n",
      "Epoch 581/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1772 - accuracy: 0.7412 - val_loss: 0.1882 - val_accuracy: 0.7319\n",
      "Epoch 582/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1727 - accuracy: 0.7463 - val_loss: 0.1868 - val_accuracy: 0.7359\n",
      "Epoch 583/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1759 - accuracy: 0.7428 - val_loss: 0.1866 - val_accuracy: 0.7334\n",
      "Epoch 584/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1750 - accuracy: 0.7430 - val_loss: 0.1869 - val_accuracy: 0.7361\n",
      "Epoch 585/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1764 - accuracy: 0.7395 - val_loss: 0.1865 - val_accuracy: 0.7349\n",
      "Epoch 586/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1747 - accuracy: 0.7453 - val_loss: 0.1867 - val_accuracy: 0.7371\n",
      "Epoch 587/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1742 - accuracy: 0.7440 - val_loss: 0.1872 - val_accuracy: 0.7339\n",
      "Epoch 588/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1760 - accuracy: 0.7426 - val_loss: 0.1872 - val_accuracy: 0.7370\n",
      "Epoch 589/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1772 - accuracy: 0.7395 - val_loss: 0.1864 - val_accuracy: 0.7370\n",
      "Epoch 590/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1754 - accuracy: 0.7436 - val_loss: 0.1861 - val_accuracy: 0.7371\n",
      "Epoch 591/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1718 - accuracy: 0.7500 - val_loss: 0.1873 - val_accuracy: 0.7358\n",
      "Epoch 592/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1777 - accuracy: 0.7392 - val_loss: 0.1870 - val_accuracy: 0.7341\n",
      "Epoch 593/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1784 - accuracy: 0.7395 - val_loss: 0.1881 - val_accuracy: 0.7354\n",
      "Epoch 594/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1788 - accuracy: 0.7379 - val_loss: 0.1862 - val_accuracy: 0.7378\n",
      "Epoch 595/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1773 - accuracy: 0.7373 - val_loss: 0.1866 - val_accuracy: 0.7345\n",
      "Epoch 596/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1754 - accuracy: 0.7430 - val_loss: 0.1865 - val_accuracy: 0.7368\n",
      "Epoch 597/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1744 - accuracy: 0.7470 - val_loss: 0.1860 - val_accuracy: 0.7363\n",
      "Epoch 598/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1774 - accuracy: 0.7371 - val_loss: 0.1864 - val_accuracy: 0.7363\n",
      "Epoch 599/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1766 - accuracy: 0.7414 - val_loss: 0.1866 - val_accuracy: 0.7331\n",
      "Epoch 600/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1737 - accuracy: 0.7454 - val_loss: 0.1862 - val_accuracy: 0.7356\n",
      "Epoch 601/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1766 - accuracy: 0.7423 - val_loss: 0.1861 - val_accuracy: 0.7378\n",
      "Epoch 602/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1757 - accuracy: 0.7420 - val_loss: 0.1872 - val_accuracy: 0.7343\n",
      "Epoch 603/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1756 - accuracy: 0.7427 - val_loss: 0.1861 - val_accuracy: 0.7359\n",
      "Epoch 604/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1757 - accuracy: 0.7418 - val_loss: 0.1851 - val_accuracy: 0.7361\n",
      "Epoch 605/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1752 - accuracy: 0.7432 - val_loss: 0.1860 - val_accuracy: 0.7359\n",
      "Epoch 606/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1777 - accuracy: 0.7383 - val_loss: 0.1862 - val_accuracy: 0.7341\n",
      "Epoch 607/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1739 - accuracy: 0.7452 - val_loss: 0.1859 - val_accuracy: 0.7356\n",
      "Epoch 608/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1730 - accuracy: 0.7496 - val_loss: 0.1859 - val_accuracy: 0.7323\n",
      "Epoch 609/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1747 - accuracy: 0.7429 - val_loss: 0.1857 - val_accuracy: 0.7371\n",
      "Epoch 610/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1763 - accuracy: 0.7449 - val_loss: 0.1863 - val_accuracy: 0.7361\n",
      "Epoch 611/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1757 - accuracy: 0.7404 - val_loss: 0.1863 - val_accuracy: 0.7352\n",
      "Epoch 612/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1769 - accuracy: 0.7404 - val_loss: 0.1861 - val_accuracy: 0.7345\n",
      "Epoch 613/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1750 - accuracy: 0.7452 - val_loss: 0.1863 - val_accuracy: 0.7370\n",
      "Epoch 614/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1759 - accuracy: 0.7414 - val_loss: 0.1866 - val_accuracy: 0.7349\n",
      "Epoch 615/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1739 - accuracy: 0.7471 - val_loss: 0.1862 - val_accuracy: 0.7332\n",
      "Epoch 616/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1772 - accuracy: 0.7399 - val_loss: 0.1882 - val_accuracy: 0.7327\n",
      "Epoch 617/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1748 - accuracy: 0.7452 - val_loss: 0.1876 - val_accuracy: 0.7354\n",
      "Epoch 618/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1767 - accuracy: 0.7407 - val_loss: 0.1873 - val_accuracy: 0.7366\n",
      "Epoch 619/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1753 - accuracy: 0.7440 - val_loss: 0.1866 - val_accuracy: 0.7363\n",
      "Epoch 620/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1752 - accuracy: 0.7466 - val_loss: 0.1867 - val_accuracy: 0.7358\n",
      "Epoch 621/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1780 - accuracy: 0.7390 - val_loss: 0.1868 - val_accuracy: 0.7322\n",
      "Epoch 622/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1753 - accuracy: 0.7425 - val_loss: 0.1873 - val_accuracy: 0.7318\n",
      "Epoch 623/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1740 - accuracy: 0.7426 - val_loss: 0.1874 - val_accuracy: 0.7345\n",
      "Epoch 624/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1741 - accuracy: 0.7456 - val_loss: 0.1866 - val_accuracy: 0.7340\n",
      "Epoch 625/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1754 - accuracy: 0.7445 - val_loss: 0.1865 - val_accuracy: 0.7362\n",
      "Epoch 626/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1751 - accuracy: 0.7435 - val_loss: 0.1882 - val_accuracy: 0.7322\n",
      "Epoch 627/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1750 - accuracy: 0.7468 - val_loss: 0.1865 - val_accuracy: 0.7367\n",
      "Epoch 628/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1790 - accuracy: 0.7347 - val_loss: 0.1869 - val_accuracy: 0.7368\n",
      "Epoch 629/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1772 - accuracy: 0.7387 - val_loss: 0.1877 - val_accuracy: 0.7357\n",
      "Epoch 630/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1760 - accuracy: 0.7409 - val_loss: 0.1864 - val_accuracy: 0.7371\n",
      "Epoch 631/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1749 - accuracy: 0.7438 - val_loss: 0.1875 - val_accuracy: 0.7362\n",
      "Epoch 632/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1764 - accuracy: 0.7403 - val_loss: 0.1885 - val_accuracy: 0.7306\n",
      "Epoch 633/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1745 - accuracy: 0.7457 - val_loss: 0.1874 - val_accuracy: 0.7323\n",
      "Epoch 634/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1753 - accuracy: 0.7446 - val_loss: 0.1874 - val_accuracy: 0.7361\n",
      "Epoch 635/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1763 - accuracy: 0.7435 - val_loss: 0.1869 - val_accuracy: 0.7367\n",
      "Epoch 636/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1765 - accuracy: 0.7392 - val_loss: 0.1872 - val_accuracy: 0.7368\n",
      "Epoch 637/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1746 - accuracy: 0.7453 - val_loss: 0.1873 - val_accuracy: 0.7346\n",
      "Epoch 638/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1761 - accuracy: 0.7410 - val_loss: 0.1870 - val_accuracy: 0.7361\n",
      "Epoch 639/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1750 - accuracy: 0.7459 - val_loss: 0.1878 - val_accuracy: 0.7292\n",
      "Epoch 640/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1736 - accuracy: 0.7467 - val_loss: 0.1872 - val_accuracy: 0.7375\n",
      "Epoch 641/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1772 - accuracy: 0.7377 - val_loss: 0.1863 - val_accuracy: 0.7362\n",
      "Epoch 642/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1766 - accuracy: 0.7386 - val_loss: 0.1882 - val_accuracy: 0.7349\n",
      "Epoch 643/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1766 - accuracy: 0.7399 - val_loss: 0.1869 - val_accuracy: 0.7340\n",
      "Epoch 644/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1750 - accuracy: 0.7459 - val_loss: 0.1869 - val_accuracy: 0.7370\n",
      "Epoch 645/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1756 - accuracy: 0.7406 - val_loss: 0.1874 - val_accuracy: 0.7368\n",
      "Epoch 646/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1760 - accuracy: 0.7445 - val_loss: 0.1868 - val_accuracy: 0.7375\n",
      "Epoch 647/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1750 - accuracy: 0.7433 - val_loss: 0.1879 - val_accuracy: 0.7336\n",
      "Epoch 648/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1756 - accuracy: 0.7443 - val_loss: 0.1867 - val_accuracy: 0.7381\n",
      "Epoch 649/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1749 - accuracy: 0.7458 - val_loss: 0.1876 - val_accuracy: 0.7354\n",
      "Epoch 650/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1743 - accuracy: 0.7467 - val_loss: 0.1873 - val_accuracy: 0.7371\n",
      "Epoch 651/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1763 - accuracy: 0.7425 - val_loss: 0.1866 - val_accuracy: 0.7376\n",
      "Epoch 652/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1747 - accuracy: 0.7458 - val_loss: 0.1869 - val_accuracy: 0.7370\n",
      "Epoch 653/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1755 - accuracy: 0.7439 - val_loss: 0.1869 - val_accuracy: 0.7370\n",
      "Epoch 654/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1739 - accuracy: 0.7437 - val_loss: 0.1872 - val_accuracy: 0.7366\n",
      "Epoch 655/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1757 - accuracy: 0.7419 - val_loss: 0.1871 - val_accuracy: 0.7371\n",
      "Epoch 656/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1770 - accuracy: 0.7396 - val_loss: 0.1876 - val_accuracy: 0.7361\n",
      "Epoch 657/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1780 - accuracy: 0.7388 - val_loss: 0.1872 - val_accuracy: 0.7365\n",
      "Epoch 658/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1755 - accuracy: 0.7428 - val_loss: 0.1877 - val_accuracy: 0.7358\n",
      "Epoch 659/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1757 - accuracy: 0.7444 - val_loss: 0.1873 - val_accuracy: 0.7371\n",
      "Epoch 660/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1746 - accuracy: 0.7450 - val_loss: 0.1881 - val_accuracy: 0.7335\n",
      "Epoch 661/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1749 - accuracy: 0.7446 - val_loss: 0.1878 - val_accuracy: 0.7367\n",
      "Epoch 662/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1773 - accuracy: 0.7399 - val_loss: 0.1870 - val_accuracy: 0.7368\n",
      "Epoch 663/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1752 - accuracy: 0.7444 - val_loss: 0.1869 - val_accuracy: 0.7366\n",
      "Epoch 664/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1738 - accuracy: 0.7488 - val_loss: 0.1874 - val_accuracy: 0.7368\n",
      "Epoch 665/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1731 - accuracy: 0.7493 - val_loss: 0.1875 - val_accuracy: 0.7331\n",
      "Epoch 666/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1736 - accuracy: 0.7448 - val_loss: 0.1873 - val_accuracy: 0.7380\n",
      "Epoch 667/1000\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1783 - accuracy: 0.7389 - val_loss: 0.1869 - val_accuracy: 0.7372\n",
      "Epoch 668/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1764 - accuracy: 0.7417 - val_loss: 0.1881 - val_accuracy: 0.7345\n",
      "Epoch 669/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1758 - accuracy: 0.7426 - val_loss: 0.1876 - val_accuracy: 0.7359\n",
      "Epoch 670/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1751 - accuracy: 0.7412 - val_loss: 0.1877 - val_accuracy: 0.7362\n",
      "Epoch 671/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1738 - accuracy: 0.7434 - val_loss: 0.1876 - val_accuracy: 0.7354\n",
      "Epoch 672/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1750 - accuracy: 0.7419 - val_loss: 0.1861 - val_accuracy: 0.7374\n",
      "Epoch 673/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1730 - accuracy: 0.7506 - val_loss: 0.1875 - val_accuracy: 0.7339\n",
      "Epoch 674/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1748 - accuracy: 0.7429 - val_loss: 0.1868 - val_accuracy: 0.7365\n",
      "Epoch 675/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1724 - accuracy: 0.7498 - val_loss: 0.1869 - val_accuracy: 0.7362\n",
      "Epoch 676/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1756 - accuracy: 0.7428 - val_loss: 0.1877 - val_accuracy: 0.7354\n",
      "Epoch 677/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1745 - accuracy: 0.7478 - val_loss: 0.1884 - val_accuracy: 0.7300\n",
      "Epoch 678/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1752 - accuracy: 0.7443 - val_loss: 0.1864 - val_accuracy: 0.7353\n",
      "Epoch 679/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1738 - accuracy: 0.7432 - val_loss: 0.1867 - val_accuracy: 0.7363\n",
      "Epoch 680/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1742 - accuracy: 0.7473 - val_loss: 0.1871 - val_accuracy: 0.7375\n",
      "Epoch 681/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1747 - accuracy: 0.7436 - val_loss: 0.1884 - val_accuracy: 0.7370\n",
      "Epoch 682/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1742 - accuracy: 0.7454 - val_loss: 0.1866 - val_accuracy: 0.7376\n",
      "Epoch 683/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1763 - accuracy: 0.7440 - val_loss: 0.1869 - val_accuracy: 0.7381\n",
      "Epoch 684/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1762 - accuracy: 0.7416 - val_loss: 0.1883 - val_accuracy: 0.7343\n",
      "Epoch 685/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1728 - accuracy: 0.7500 - val_loss: 0.1887 - val_accuracy: 0.7345\n",
      "Epoch 686/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1752 - accuracy: 0.7444 - val_loss: 0.1865 - val_accuracy: 0.7378\n",
      "Epoch 687/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1751 - accuracy: 0.7450 - val_loss: 0.1865 - val_accuracy: 0.7330\n",
      "Epoch 688/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1760 - accuracy: 0.7446 - val_loss: 0.1865 - val_accuracy: 0.7365\n",
      "Epoch 689/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1743 - accuracy: 0.7469 - val_loss: 0.1876 - val_accuracy: 0.7337\n",
      "Epoch 690/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1723 - accuracy: 0.7513 - val_loss: 0.1862 - val_accuracy: 0.7365\n",
      "Epoch 691/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1748 - accuracy: 0.7467 - val_loss: 0.1876 - val_accuracy: 0.7350\n",
      "Epoch 692/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1762 - accuracy: 0.7413 - val_loss: 0.1882 - val_accuracy: 0.7328\n",
      "Epoch 693/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1733 - accuracy: 0.7457 - val_loss: 0.1872 - val_accuracy: 0.7367\n",
      "Epoch 694/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1755 - accuracy: 0.7459 - val_loss: 0.1876 - val_accuracy: 0.7352\n",
      "Epoch 695/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1761 - accuracy: 0.7422 - val_loss: 0.1879 - val_accuracy: 0.7326\n",
      "Epoch 696/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1748 - accuracy: 0.7464 - val_loss: 0.1869 - val_accuracy: 0.7356\n",
      "Epoch 697/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1776 - accuracy: 0.7408 - val_loss: 0.1875 - val_accuracy: 0.7356\n",
      "Epoch 698/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1748 - accuracy: 0.7426 - val_loss: 0.1870 - val_accuracy: 0.7361\n",
      "Epoch 699/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1745 - accuracy: 0.7460 - val_loss: 0.1864 - val_accuracy: 0.7363\n",
      "Epoch 700/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1773 - accuracy: 0.7386 - val_loss: 0.1879 - val_accuracy: 0.7315\n",
      "Epoch 701/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1773 - accuracy: 0.7388 - val_loss: 0.1868 - val_accuracy: 0.7340\n",
      "Epoch 702/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1750 - accuracy: 0.7421 - val_loss: 0.1882 - val_accuracy: 0.7324\n",
      "Epoch 703/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1782 - accuracy: 0.7372 - val_loss: 0.1873 - val_accuracy: 0.7344\n",
      "Epoch 704/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1746 - accuracy: 0.7424 - val_loss: 0.1880 - val_accuracy: 0.7344\n",
      "Epoch 705/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1731 - accuracy: 0.7481 - val_loss: 0.1865 - val_accuracy: 0.7362\n",
      "Epoch 706/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1759 - accuracy: 0.7407 - val_loss: 0.1866 - val_accuracy: 0.7372\n",
      "Epoch 707/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1778 - accuracy: 0.7412 - val_loss: 0.1874 - val_accuracy: 0.7371\n",
      "Epoch 708/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1730 - accuracy: 0.7473 - val_loss: 0.1861 - val_accuracy: 0.7371\n",
      "Epoch 709/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1745 - accuracy: 0.7453 - val_loss: 0.1869 - val_accuracy: 0.7365\n",
      "Epoch 710/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1738 - accuracy: 0.7455 - val_loss: 0.1869 - val_accuracy: 0.7370\n",
      "Epoch 711/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1735 - accuracy: 0.7473 - val_loss: 0.1863 - val_accuracy: 0.7367\n",
      "Epoch 712/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1753 - accuracy: 0.7474 - val_loss: 0.1871 - val_accuracy: 0.7367\n",
      "Epoch 713/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1771 - accuracy: 0.7395 - val_loss: 0.1872 - val_accuracy: 0.7366\n",
      "Epoch 714/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1751 - accuracy: 0.7433 - val_loss: 0.1877 - val_accuracy: 0.7359\n",
      "Epoch 715/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1765 - accuracy: 0.7410 - val_loss: 0.1883 - val_accuracy: 0.7358\n",
      "Epoch 716/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1747 - accuracy: 0.7461 - val_loss: 0.1881 - val_accuracy: 0.7344\n",
      "Epoch 717/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1767 - accuracy: 0.7395 - val_loss: 0.1873 - val_accuracy: 0.7341\n",
      "Epoch 718/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1758 - accuracy: 0.7445 - val_loss: 0.1876 - val_accuracy: 0.7358\n",
      "Epoch 719/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1749 - accuracy: 0.7430 - val_loss: 0.1859 - val_accuracy: 0.7344\n",
      "Epoch 720/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1743 - accuracy: 0.7477 - val_loss: 0.1902 - val_accuracy: 0.7274\n",
      "Epoch 721/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1776 - accuracy: 0.7394 - val_loss: 0.1879 - val_accuracy: 0.7359\n",
      "Epoch 722/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1760 - accuracy: 0.7426 - val_loss: 0.1885 - val_accuracy: 0.7319\n",
      "Epoch 723/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1742 - accuracy: 0.7467 - val_loss: 0.1872 - val_accuracy: 0.7356\n",
      "Epoch 724/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1749 - accuracy: 0.7457 - val_loss: 0.1870 - val_accuracy: 0.7334\n",
      "Epoch 725/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1742 - accuracy: 0.7445 - val_loss: 0.1869 - val_accuracy: 0.7339\n",
      "Epoch 726/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1776 - accuracy: 0.7405 - val_loss: 0.1870 - val_accuracy: 0.7352\n",
      "Epoch 727/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1745 - accuracy: 0.7461 - val_loss: 0.1868 - val_accuracy: 0.7361\n",
      "Epoch 728/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1749 - accuracy: 0.7417 - val_loss: 0.1869 - val_accuracy: 0.7353\n",
      "Epoch 729/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1729 - accuracy: 0.7502 - val_loss: 0.1876 - val_accuracy: 0.7315\n",
      "Epoch 730/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1751 - accuracy: 0.7445 - val_loss: 0.1874 - val_accuracy: 0.7331\n",
      "Epoch 731/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1743 - accuracy: 0.7441 - val_loss: 0.1868 - val_accuracy: 0.7359\n",
      "Epoch 732/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1752 - accuracy: 0.7417 - val_loss: 0.1860 - val_accuracy: 0.7354\n",
      "Epoch 733/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1743 - accuracy: 0.7434 - val_loss: 0.1868 - val_accuracy: 0.7353\n",
      "Epoch 734/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1741 - accuracy: 0.7478 - val_loss: 0.1874 - val_accuracy: 0.7334\n",
      "Epoch 735/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1742 - accuracy: 0.7476 - val_loss: 0.1867 - val_accuracy: 0.7343\n",
      "Epoch 736/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1774 - accuracy: 0.7410 - val_loss: 0.1873 - val_accuracy: 0.7367\n",
      "Epoch 737/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1727 - accuracy: 0.7468 - val_loss: 0.1875 - val_accuracy: 0.7331\n",
      "Epoch 738/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1770 - accuracy: 0.7407 - val_loss: 0.1876 - val_accuracy: 0.7337\n",
      "Epoch 739/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1751 - accuracy: 0.7434 - val_loss: 0.1872 - val_accuracy: 0.7361\n",
      "Epoch 740/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1748 - accuracy: 0.7467 - val_loss: 0.1883 - val_accuracy: 0.7322\n",
      "Epoch 741/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1746 - accuracy: 0.7439 - val_loss: 0.1867 - val_accuracy: 0.7368\n",
      "Epoch 742/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1743 - accuracy: 0.7466 - val_loss: 0.1866 - val_accuracy: 0.7371\n",
      "Epoch 743/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1735 - accuracy: 0.7493 - val_loss: 0.1867 - val_accuracy: 0.7345\n",
      "Epoch 744/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1763 - accuracy: 0.7403 - val_loss: 0.1873 - val_accuracy: 0.7354\n",
      "Epoch 745/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1749 - accuracy: 0.7452 - val_loss: 0.1872 - val_accuracy: 0.7328\n",
      "Epoch 746/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1747 - accuracy: 0.7453 - val_loss: 0.1864 - val_accuracy: 0.7368\n",
      "Epoch 747/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1726 - accuracy: 0.7478 - val_loss: 0.1887 - val_accuracy: 0.7309\n",
      "Epoch 748/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1763 - accuracy: 0.7421 - val_loss: 0.1873 - val_accuracy: 0.7359\n",
      "Epoch 749/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1747 - accuracy: 0.7478 - val_loss: 0.1874 - val_accuracy: 0.7366\n",
      "Epoch 750/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1748 - accuracy: 0.7437 - val_loss: 0.1870 - val_accuracy: 0.7354\n",
      "Epoch 751/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1742 - accuracy: 0.7450 - val_loss: 0.1879 - val_accuracy: 0.7345\n",
      "Epoch 752/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1739 - accuracy: 0.7438 - val_loss: 0.1872 - val_accuracy: 0.7330\n",
      "Epoch 753/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1762 - accuracy: 0.7446 - val_loss: 0.1878 - val_accuracy: 0.7366\n",
      "Epoch 754/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1732 - accuracy: 0.7474 - val_loss: 0.1871 - val_accuracy: 0.7334\n",
      "Epoch 755/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1765 - accuracy: 0.7420 - val_loss: 0.1874 - val_accuracy: 0.7368\n",
      "Epoch 756/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1768 - accuracy: 0.7403 - val_loss: 0.1883 - val_accuracy: 0.7340\n",
      "Epoch 757/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1775 - accuracy: 0.7409 - val_loss: 0.1895 - val_accuracy: 0.7305\n",
      "Epoch 758/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1770 - accuracy: 0.7409 - val_loss: 0.1873 - val_accuracy: 0.7350\n",
      "Epoch 759/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1742 - accuracy: 0.7454 - val_loss: 0.1877 - val_accuracy: 0.7330\n",
      "Epoch 760/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1752 - accuracy: 0.7414 - val_loss: 0.1876 - val_accuracy: 0.7366\n",
      "Epoch 761/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1717 - accuracy: 0.7512 - val_loss: 0.1888 - val_accuracy: 0.7296\n",
      "Epoch 762/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1723 - accuracy: 0.7497 - val_loss: 0.1876 - val_accuracy: 0.7335\n",
      "Epoch 763/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1767 - accuracy: 0.7449 - val_loss: 0.1870 - val_accuracy: 0.7326\n",
      "Epoch 764/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1772 - accuracy: 0.7403 - val_loss: 0.1879 - val_accuracy: 0.7322\n",
      "Epoch 765/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1762 - accuracy: 0.7393 - val_loss: 0.1881 - val_accuracy: 0.7315\n",
      "Epoch 766/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1745 - accuracy: 0.7433 - val_loss: 0.1870 - val_accuracy: 0.7371\n",
      "Epoch 767/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1748 - accuracy: 0.7439 - val_loss: 0.1867 - val_accuracy: 0.7357\n",
      "Epoch 768/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1759 - accuracy: 0.7425 - val_loss: 0.1876 - val_accuracy: 0.7361\n",
      "Epoch 769/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1738 - accuracy: 0.7474 - val_loss: 0.1870 - val_accuracy: 0.7366\n",
      "Epoch 770/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1728 - accuracy: 0.7519 - val_loss: 0.1881 - val_accuracy: 0.7352\n",
      "Epoch 771/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1783 - accuracy: 0.7399 - val_loss: 0.1879 - val_accuracy: 0.7362\n",
      "Epoch 772/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1730 - accuracy: 0.7495 - val_loss: 0.1876 - val_accuracy: 0.7353\n",
      "Epoch 773/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1758 - accuracy: 0.7437 - val_loss: 0.1886 - val_accuracy: 0.7353\n",
      "Epoch 774/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1795 - accuracy: 0.7429 - val_loss: 0.1887 - val_accuracy: 0.7345\n",
      "Epoch 775/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1761 - accuracy: 0.7466 - val_loss: 0.1890 - val_accuracy: 0.7354\n",
      "Epoch 776/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1781 - accuracy: 0.7385 - val_loss: 0.1885 - val_accuracy: 0.7354\n",
      "Epoch 777/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1755 - accuracy: 0.7433 - val_loss: 0.1889 - val_accuracy: 0.7359\n",
      "Epoch 778/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1765 - accuracy: 0.7395 - val_loss: 0.1882 - val_accuracy: 0.7341\n",
      "Epoch 779/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1766 - accuracy: 0.7437 - val_loss: 0.1882 - val_accuracy: 0.7344\n",
      "Epoch 780/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1763 - accuracy: 0.7419 - val_loss: 0.1872 - val_accuracy: 0.7354\n",
      "Epoch 781/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1781 - accuracy: 0.7389 - val_loss: 0.1881 - val_accuracy: 0.7336\n",
      "Epoch 782/1000\n",
      "563/563 [==============================] - 1s 968us/step - loss: 0.1740 - accuracy: 0.7495 - val_loss: 0.1868 - val_accuracy: 0.7371\n",
      "Epoch 783/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1758 - accuracy: 0.7421 - val_loss: 0.1882 - val_accuracy: 0.7357\n",
      "Epoch 784/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1739 - accuracy: 0.7471 - val_loss: 0.1869 - val_accuracy: 0.7363\n",
      "Epoch 785/1000\n",
      "563/563 [==============================] - 1s 998us/step - loss: 0.1734 - accuracy: 0.7486 - val_loss: 0.1875 - val_accuracy: 0.7356\n",
      "Epoch 786/1000\n",
      "563/563 [==============================] - 1s 986us/step - loss: 0.1744 - accuracy: 0.7437 - val_loss: 0.1872 - val_accuracy: 0.7365\n",
      "Epoch 787/1000\n",
      "563/563 [==============================] - 1s 995us/step - loss: 0.1751 - accuracy: 0.7442 - val_loss: 0.1872 - val_accuracy: 0.7352\n",
      "Epoch 788/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1752 - accuracy: 0.7426 - val_loss: 0.1870 - val_accuracy: 0.7380\n",
      "Epoch 789/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1745 - accuracy: 0.7451 - val_loss: 0.1869 - val_accuracy: 0.7375\n",
      "Epoch 790/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1744 - accuracy: 0.7460 - val_loss: 0.1874 - val_accuracy: 0.7378\n",
      "Epoch 791/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1755 - accuracy: 0.7438 - val_loss: 0.1875 - val_accuracy: 0.7371\n",
      "Epoch 792/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1742 - accuracy: 0.7463 - val_loss: 0.1878 - val_accuracy: 0.7353\n",
      "Epoch 793/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1768 - accuracy: 0.7406 - val_loss: 0.1875 - val_accuracy: 0.7361\n",
      "Epoch 794/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1766 - accuracy: 0.7401 - val_loss: 0.1871 - val_accuracy: 0.7362\n",
      "Epoch 795/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1733 - accuracy: 0.7433 - val_loss: 0.1877 - val_accuracy: 0.7346\n",
      "Epoch 796/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1747 - accuracy: 0.7456 - val_loss: 0.1895 - val_accuracy: 0.7308\n",
      "Epoch 797/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1764 - accuracy: 0.7422 - val_loss: 0.1871 - val_accuracy: 0.7361\n",
      "Epoch 798/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1761 - accuracy: 0.7409 - val_loss: 0.1876 - val_accuracy: 0.7345\n",
      "Epoch 799/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1746 - accuracy: 0.7464 - val_loss: 0.1871 - val_accuracy: 0.7372\n",
      "Epoch 800/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1720 - accuracy: 0.7502 - val_loss: 0.1877 - val_accuracy: 0.7359\n",
      "Epoch 801/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1739 - accuracy: 0.7455 - val_loss: 0.1865 - val_accuracy: 0.7368\n",
      "Epoch 802/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1750 - accuracy: 0.7431 - val_loss: 0.1867 - val_accuracy: 0.7367\n",
      "Epoch 803/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1732 - accuracy: 0.7485 - val_loss: 0.1868 - val_accuracy: 0.7375\n",
      "Epoch 804/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1749 - accuracy: 0.7428 - val_loss: 0.1879 - val_accuracy: 0.7341\n",
      "Epoch 805/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1794 - accuracy: 0.7405 - val_loss: 0.1878 - val_accuracy: 0.7363\n",
      "Epoch 806/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1762 - accuracy: 0.7436 - val_loss: 0.1878 - val_accuracy: 0.7330\n",
      "Epoch 807/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1728 - accuracy: 0.7475 - val_loss: 0.1875 - val_accuracy: 0.7363\n",
      "Epoch 808/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1759 - accuracy: 0.7415 - val_loss: 0.1878 - val_accuracy: 0.7363\n",
      "Epoch 809/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1724 - accuracy: 0.7526 - val_loss: 0.1879 - val_accuracy: 0.7361\n",
      "Epoch 810/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1743 - accuracy: 0.7464 - val_loss: 0.1875 - val_accuracy: 0.7363\n",
      "Epoch 811/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1770 - accuracy: 0.7425 - val_loss: 0.1881 - val_accuracy: 0.7327\n",
      "Epoch 812/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1743 - accuracy: 0.7462 - val_loss: 0.1878 - val_accuracy: 0.7366\n",
      "Epoch 813/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1739 - accuracy: 0.7471 - val_loss: 0.1871 - val_accuracy: 0.7375\n",
      "Epoch 814/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1760 - accuracy: 0.7436 - val_loss: 0.1887 - val_accuracy: 0.7282\n",
      "Epoch 815/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1760 - accuracy: 0.7434 - val_loss: 0.1872 - val_accuracy: 0.7372\n",
      "Epoch 816/1000\n",
      "563/563 [==============================] - 1s 998us/step - loss: 0.1741 - accuracy: 0.7440 - val_loss: 0.1879 - val_accuracy: 0.7326\n",
      "Epoch 817/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1753 - accuracy: 0.7411 - val_loss: 0.1881 - val_accuracy: 0.7362\n",
      "Epoch 818/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1743 - accuracy: 0.7451 - val_loss: 0.1869 - val_accuracy: 0.7371\n",
      "Epoch 819/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1737 - accuracy: 0.7465 - val_loss: 0.1864 - val_accuracy: 0.7371\n",
      "Epoch 820/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1759 - accuracy: 0.7426 - val_loss: 0.1870 - val_accuracy: 0.7380\n",
      "Epoch 821/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1752 - accuracy: 0.7432 - val_loss: 0.1877 - val_accuracy: 0.7356\n",
      "Epoch 822/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1747 - accuracy: 0.7443 - val_loss: 0.1874 - val_accuracy: 0.7374\n",
      "Epoch 823/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1756 - accuracy: 0.7428 - val_loss: 0.1890 - val_accuracy: 0.7297\n",
      "Epoch 824/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1747 - accuracy: 0.7436 - val_loss: 0.1885 - val_accuracy: 0.7321\n",
      "Epoch 825/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1739 - accuracy: 0.7437 - val_loss: 0.1881 - val_accuracy: 0.7339\n",
      "Epoch 826/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1751 - accuracy: 0.7438 - val_loss: 0.1875 - val_accuracy: 0.7339\n",
      "Epoch 827/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1751 - accuracy: 0.7443 - val_loss: 0.1875 - val_accuracy: 0.7341\n",
      "Epoch 828/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1752 - accuracy: 0.7443 - val_loss: 0.1881 - val_accuracy: 0.7344\n",
      "Epoch 829/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1756 - accuracy: 0.7460 - val_loss: 0.1881 - val_accuracy: 0.7374\n",
      "Epoch 830/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1742 - accuracy: 0.7462 - val_loss: 0.1881 - val_accuracy: 0.7327\n",
      "Epoch 831/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1737 - accuracy: 0.7468 - val_loss: 0.1869 - val_accuracy: 0.7372\n",
      "Epoch 832/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1722 - accuracy: 0.7498 - val_loss: 0.1873 - val_accuracy: 0.7363\n",
      "Epoch 833/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1726 - accuracy: 0.7481 - val_loss: 0.1886 - val_accuracy: 0.7358\n",
      "Epoch 834/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1769 - accuracy: 0.7409 - val_loss: 0.1878 - val_accuracy: 0.7357\n",
      "Epoch 835/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1755 - accuracy: 0.7426 - val_loss: 0.1886 - val_accuracy: 0.7346\n",
      "Epoch 836/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1751 - accuracy: 0.7424 - val_loss: 0.1871 - val_accuracy: 0.7359\n",
      "Epoch 837/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1759 - accuracy: 0.7436 - val_loss: 0.1883 - val_accuracy: 0.7317\n",
      "Epoch 838/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1776 - accuracy: 0.7416 - val_loss: 0.1881 - val_accuracy: 0.7359\n",
      "Epoch 839/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1748 - accuracy: 0.7476 - val_loss: 0.1882 - val_accuracy: 0.7361\n",
      "Epoch 840/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1742 - accuracy: 0.7450 - val_loss: 0.1880 - val_accuracy: 0.7359\n",
      "Epoch 841/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1750 - accuracy: 0.7461 - val_loss: 0.1875 - val_accuracy: 0.7350\n",
      "Epoch 842/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1761 - accuracy: 0.7398 - val_loss: 0.1879 - val_accuracy: 0.7337\n",
      "Epoch 843/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1772 - accuracy: 0.7391 - val_loss: 0.1884 - val_accuracy: 0.7356\n",
      "Epoch 844/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1757 - accuracy: 0.7422 - val_loss: 0.1871 - val_accuracy: 0.7376\n",
      "Epoch 845/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1732 - accuracy: 0.7491 - val_loss: 0.1873 - val_accuracy: 0.7368\n",
      "Epoch 846/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1759 - accuracy: 0.7453 - val_loss: 0.1882 - val_accuracy: 0.7344\n",
      "Epoch 847/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1766 - accuracy: 0.7421 - val_loss: 0.1883 - val_accuracy: 0.7334\n",
      "Epoch 848/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1776 - accuracy: 0.7421 - val_loss: 0.1878 - val_accuracy: 0.7346\n",
      "Epoch 849/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1763 - accuracy: 0.7424 - val_loss: 0.1881 - val_accuracy: 0.7321\n",
      "Epoch 850/1000\n",
      "563/563 [==============================] - 1s 998us/step - loss: 0.1749 - accuracy: 0.7449 - val_loss: 0.1880 - val_accuracy: 0.7327\n",
      "Epoch 851/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1750 - accuracy: 0.7462 - val_loss: 0.1880 - val_accuracy: 0.7326\n",
      "Epoch 852/1000\n",
      "563/563 [==============================] - 1s 986us/step - loss: 0.1750 - accuracy: 0.7451 - val_loss: 0.1882 - val_accuracy: 0.7365\n",
      "Epoch 853/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1753 - accuracy: 0.7428 - val_loss: 0.1878 - val_accuracy: 0.7359\n",
      "Epoch 854/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1751 - accuracy: 0.7457 - val_loss: 0.1881 - val_accuracy: 0.7358\n",
      "Epoch 855/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1734 - accuracy: 0.7463 - val_loss: 0.1889 - val_accuracy: 0.7334\n",
      "Epoch 856/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1781 - accuracy: 0.7399 - val_loss: 0.1887 - val_accuracy: 0.7293\n",
      "Epoch 857/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1757 - accuracy: 0.7416 - val_loss: 0.1874 - val_accuracy: 0.7365\n",
      "Epoch 858/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1739 - accuracy: 0.7461 - val_loss: 0.1886 - val_accuracy: 0.7317\n",
      "Epoch 859/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1752 - accuracy: 0.7420 - val_loss: 0.1894 - val_accuracy: 0.7302\n",
      "Epoch 860/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1735 - accuracy: 0.7459 - val_loss: 0.1880 - val_accuracy: 0.7352\n",
      "Epoch 861/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1739 - accuracy: 0.7457 - val_loss: 0.1887 - val_accuracy: 0.7349\n",
      "Epoch 862/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1752 - accuracy: 0.7425 - val_loss: 0.1884 - val_accuracy: 0.7365\n",
      "Epoch 863/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1791 - accuracy: 0.7379 - val_loss: 0.1885 - val_accuracy: 0.7349\n",
      "Epoch 864/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1748 - accuracy: 0.7442 - val_loss: 0.1889 - val_accuracy: 0.7359\n",
      "Epoch 865/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1746 - accuracy: 0.7459 - val_loss: 0.1887 - val_accuracy: 0.7363\n",
      "Epoch 866/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1781 - accuracy: 0.7375 - val_loss: 0.1882 - val_accuracy: 0.7370\n",
      "Epoch 867/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1756 - accuracy: 0.7452 - val_loss: 0.1886 - val_accuracy: 0.7331\n",
      "Epoch 868/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1747 - accuracy: 0.7447 - val_loss: 0.1893 - val_accuracy: 0.7319\n",
      "Epoch 869/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1734 - accuracy: 0.7476 - val_loss: 0.1888 - val_accuracy: 0.7358\n",
      "Epoch 870/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1779 - accuracy: 0.7421 - val_loss: 0.1885 - val_accuracy: 0.7346\n",
      "Epoch 871/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1781 - accuracy: 0.7394 - val_loss: 0.1889 - val_accuracy: 0.7319\n",
      "Epoch 872/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1757 - accuracy: 0.7451 - val_loss: 0.1874 - val_accuracy: 0.7367\n",
      "Epoch 873/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1744 - accuracy: 0.7477 - val_loss: 0.1888 - val_accuracy: 0.7330\n",
      "Epoch 874/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1765 - accuracy: 0.7404 - val_loss: 0.1876 - val_accuracy: 0.7353\n",
      "Epoch 875/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1755 - accuracy: 0.7462 - val_loss: 0.1881 - val_accuracy: 0.7337\n",
      "Epoch 876/1000\n",
      "563/563 [==============================] - 1s 991us/step - loss: 0.1712 - accuracy: 0.7510 - val_loss: 0.1882 - val_accuracy: 0.7361\n",
      "Epoch 877/1000\n",
      "563/563 [==============================] - 1s 998us/step - loss: 0.1760 - accuracy: 0.7413 - val_loss: 0.1883 - val_accuracy: 0.7330\n",
      "Epoch 878/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1732 - accuracy: 0.7486 - val_loss: 0.1876 - val_accuracy: 0.7371\n",
      "Epoch 879/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1760 - accuracy: 0.7436 - val_loss: 0.1889 - val_accuracy: 0.7327\n",
      "Epoch 880/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1753 - accuracy: 0.7433 - val_loss: 0.1885 - val_accuracy: 0.7335\n",
      "Epoch 881/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1757 - accuracy: 0.7446 - val_loss: 0.1888 - val_accuracy: 0.7340\n",
      "Epoch 882/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1726 - accuracy: 0.7514 - val_loss: 0.1878 - val_accuracy: 0.7367\n",
      "Epoch 883/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1751 - accuracy: 0.7449 - val_loss: 0.1884 - val_accuracy: 0.7363\n",
      "Epoch 884/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1761 - accuracy: 0.7417 - val_loss: 0.1880 - val_accuracy: 0.7362\n",
      "Epoch 885/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1747 - accuracy: 0.7441 - val_loss: 0.1883 - val_accuracy: 0.7361\n",
      "Epoch 886/1000\n",
      "563/563 [==============================] - 1s 993us/step - loss: 0.1749 - accuracy: 0.7496 - val_loss: 0.1886 - val_accuracy: 0.7359\n",
      "Epoch 887/1000\n",
      "563/563 [==============================] - 1s 991us/step - loss: 0.1755 - accuracy: 0.7456 - val_loss: 0.1881 - val_accuracy: 0.7354\n",
      "Epoch 888/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1729 - accuracy: 0.7502 - val_loss: 0.1881 - val_accuracy: 0.7363\n",
      "Epoch 889/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1739 - accuracy: 0.7476 - val_loss: 0.1885 - val_accuracy: 0.7358\n",
      "Epoch 890/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1740 - accuracy: 0.7485 - val_loss: 0.1896 - val_accuracy: 0.7324\n",
      "Epoch 891/1000\n",
      "563/563 [==============================] - 1s 998us/step - loss: 0.1743 - accuracy: 0.7465 - val_loss: 0.1883 - val_accuracy: 0.7358\n",
      "Epoch 892/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1766 - accuracy: 0.7425 - val_loss: 0.1885 - val_accuracy: 0.7353\n",
      "Epoch 893/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1744 - accuracy: 0.7443 - val_loss: 0.1877 - val_accuracy: 0.7363\n",
      "Epoch 894/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1746 - accuracy: 0.7459 - val_loss: 0.1893 - val_accuracy: 0.7319\n",
      "Epoch 895/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1760 - accuracy: 0.7397 - val_loss: 0.1888 - val_accuracy: 0.7344\n",
      "Epoch 896/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1766 - accuracy: 0.7400 - val_loss: 0.1886 - val_accuracy: 0.7361\n",
      "Epoch 897/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1753 - accuracy: 0.7451 - val_loss: 0.1884 - val_accuracy: 0.7358\n",
      "Epoch 898/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1762 - accuracy: 0.7429 - val_loss: 0.1885 - val_accuracy: 0.7354\n",
      "Epoch 899/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1749 - accuracy: 0.7453 - val_loss: 0.1884 - val_accuracy: 0.7353\n",
      "Epoch 900/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1754 - accuracy: 0.7440 - val_loss: 0.1887 - val_accuracy: 0.7349\n",
      "Epoch 901/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1735 - accuracy: 0.7481 - val_loss: 0.1879 - val_accuracy: 0.7359\n",
      "Epoch 902/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1752 - accuracy: 0.7458 - val_loss: 0.1887 - val_accuracy: 0.7319\n",
      "Epoch 903/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1750 - accuracy: 0.7406 - val_loss: 0.1879 - val_accuracy: 0.7361\n",
      "Epoch 904/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1767 - accuracy: 0.7427 - val_loss: 0.1880 - val_accuracy: 0.7363\n",
      "Epoch 905/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1762 - accuracy: 0.7427 - val_loss: 0.1884 - val_accuracy: 0.7354\n",
      "Epoch 906/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1774 - accuracy: 0.7394 - val_loss: 0.1882 - val_accuracy: 0.7356\n",
      "Epoch 907/1000\n",
      "563/563 [==============================] - 1s 998us/step - loss: 0.1754 - accuracy: 0.7441 - val_loss: 0.1878 - val_accuracy: 0.7356\n",
      "Epoch 908/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1733 - accuracy: 0.7484 - val_loss: 0.1880 - val_accuracy: 0.7348\n",
      "Epoch 909/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1764 - accuracy: 0.7440 - val_loss: 0.1889 - val_accuracy: 0.7332\n",
      "Epoch 910/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1741 - accuracy: 0.7467 - val_loss: 0.1879 - val_accuracy: 0.7381\n",
      "Epoch 911/1000\n",
      "563/563 [==============================] - 1s 998us/step - loss: 0.1732 - accuracy: 0.7481 - val_loss: 0.1880 - val_accuracy: 0.7374\n",
      "Epoch 912/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1747 - accuracy: 0.7443 - val_loss: 0.1874 - val_accuracy: 0.7372\n",
      "Epoch 913/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1756 - accuracy: 0.7440 - val_loss: 0.1890 - val_accuracy: 0.7336\n",
      "Epoch 914/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1771 - accuracy: 0.7448 - val_loss: 0.1885 - val_accuracy: 0.7363\n",
      "Epoch 915/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1758 - accuracy: 0.7437 - val_loss: 0.1892 - val_accuracy: 0.7354\n",
      "Epoch 916/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1763 - accuracy: 0.7419 - val_loss: 0.1894 - val_accuracy: 0.7350\n",
      "Epoch 917/1000\n",
      "563/563 [==============================] - 1s 995us/step - loss: 0.1750 - accuracy: 0.7439 - val_loss: 0.1884 - val_accuracy: 0.7356\n",
      "Epoch 918/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1740 - accuracy: 0.7488 - val_loss: 0.1880 - val_accuracy: 0.7367\n",
      "Epoch 919/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1720 - accuracy: 0.7494 - val_loss: 0.1879 - val_accuracy: 0.7350\n",
      "Epoch 920/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1750 - accuracy: 0.7468 - val_loss: 0.1878 - val_accuracy: 0.7345\n",
      "Epoch 921/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1749 - accuracy: 0.7450 - val_loss: 0.1870 - val_accuracy: 0.7375\n",
      "Epoch 922/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1774 - accuracy: 0.7422 - val_loss: 0.1881 - val_accuracy: 0.7340\n",
      "Epoch 923/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1741 - accuracy: 0.7488 - val_loss: 0.1872 - val_accuracy: 0.7380\n",
      "Epoch 924/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1747 - accuracy: 0.7455 - val_loss: 0.1878 - val_accuracy: 0.7353\n",
      "Epoch 925/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1758 - accuracy: 0.7404 - val_loss: 0.1853 - val_accuracy: 0.7389\n",
      "Epoch 926/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1788 - accuracy: 0.7367 - val_loss: 0.1881 - val_accuracy: 0.7345\n",
      "Epoch 927/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1742 - accuracy: 0.7459 - val_loss: 0.1881 - val_accuracy: 0.7368\n",
      "Epoch 928/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1767 - accuracy: 0.7409 - val_loss: 0.1862 - val_accuracy: 0.7384\n",
      "Epoch 929/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1737 - accuracy: 0.7481 - val_loss: 0.1875 - val_accuracy: 0.7372\n",
      "Epoch 930/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1748 - accuracy: 0.7447 - val_loss: 0.1869 - val_accuracy: 0.7370\n",
      "Epoch 931/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1752 - accuracy: 0.7469 - val_loss: 0.1874 - val_accuracy: 0.7378\n",
      "Epoch 932/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1743 - accuracy: 0.7446 - val_loss: 0.1884 - val_accuracy: 0.7356\n",
      "Epoch 933/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1743 - accuracy: 0.7456 - val_loss: 0.1879 - val_accuracy: 0.7363\n",
      "Epoch 934/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1751 - accuracy: 0.7448 - val_loss: 0.1885 - val_accuracy: 0.7357\n",
      "Epoch 935/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1768 - accuracy: 0.7430 - val_loss: 0.1875 - val_accuracy: 0.7359\n",
      "Epoch 936/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1729 - accuracy: 0.7502 - val_loss: 0.1891 - val_accuracy: 0.7354\n",
      "Epoch 937/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1743 - accuracy: 0.7456 - val_loss: 0.1866 - val_accuracy: 0.7376\n",
      "Epoch 938/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1744 - accuracy: 0.7479 - val_loss: 0.1884 - val_accuracy: 0.7356\n",
      "Epoch 939/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1726 - accuracy: 0.7494 - val_loss: 0.1874 - val_accuracy: 0.7368\n",
      "Epoch 940/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1735 - accuracy: 0.7461 - val_loss: 0.1872 - val_accuracy: 0.7356\n",
      "Epoch 941/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1762 - accuracy: 0.7427 - val_loss: 0.1872 - val_accuracy: 0.7365\n",
      "Epoch 942/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1753 - accuracy: 0.7457 - val_loss: 0.1880 - val_accuracy: 0.7331\n",
      "Epoch 943/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1784 - accuracy: 0.7410 - val_loss: 0.1870 - val_accuracy: 0.7365\n",
      "Epoch 944/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1760 - accuracy: 0.7428 - val_loss: 0.1884 - val_accuracy: 0.7353\n",
      "Epoch 945/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1749 - accuracy: 0.7436 - val_loss: 0.1885 - val_accuracy: 0.7340\n",
      "Epoch 946/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1754 - accuracy: 0.7458 - val_loss: 0.1879 - val_accuracy: 0.7357\n",
      "Epoch 947/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1730 - accuracy: 0.7485 - val_loss: 0.1880 - val_accuracy: 0.7324\n",
      "Epoch 948/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1736 - accuracy: 0.7474 - val_loss: 0.1879 - val_accuracy: 0.7323\n",
      "Epoch 949/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1740 - accuracy: 0.7485 - val_loss: 0.1883 - val_accuracy: 0.7358\n",
      "Epoch 950/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1741 - accuracy: 0.7460 - val_loss: 0.1885 - val_accuracy: 0.7349\n",
      "Epoch 951/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1714 - accuracy: 0.7511 - val_loss: 0.1875 - val_accuracy: 0.7367\n",
      "Epoch 952/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1793 - accuracy: 0.7359 - val_loss: 0.1878 - val_accuracy: 0.7363\n",
      "Epoch 953/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1735 - accuracy: 0.7496 - val_loss: 0.1882 - val_accuracy: 0.7365\n",
      "Epoch 954/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1763 - accuracy: 0.7432 - val_loss: 0.1880 - val_accuracy: 0.7359\n",
      "Epoch 955/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1746 - accuracy: 0.7442 - val_loss: 0.1874 - val_accuracy: 0.7367\n",
      "Epoch 956/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1762 - accuracy: 0.7416 - val_loss: 0.1873 - val_accuracy: 0.7362\n",
      "Epoch 957/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1778 - accuracy: 0.7408 - val_loss: 0.1875 - val_accuracy: 0.7353\n",
      "Epoch 958/1000\n",
      "563/563 [==============================] - 1s 998us/step - loss: 0.1750 - accuracy: 0.7480 - val_loss: 0.1884 - val_accuracy: 0.7348\n",
      "Epoch 959/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1755 - accuracy: 0.7431 - val_loss: 0.1885 - val_accuracy: 0.7346\n",
      "Epoch 960/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1759 - accuracy: 0.7440 - val_loss: 0.1874 - val_accuracy: 0.7358\n",
      "Epoch 961/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1742 - accuracy: 0.7474 - val_loss: 0.1877 - val_accuracy: 0.7365\n",
      "Epoch 962/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1741 - accuracy: 0.7462 - val_loss: 0.1881 - val_accuracy: 0.7358\n",
      "Epoch 963/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1740 - accuracy: 0.7453 - val_loss: 0.1881 - val_accuracy: 0.7358\n",
      "Epoch 964/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1762 - accuracy: 0.7419 - val_loss: 0.1878 - val_accuracy: 0.7353\n",
      "Epoch 965/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1748 - accuracy: 0.7453 - val_loss: 0.1878 - val_accuracy: 0.7356\n",
      "Epoch 966/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1714 - accuracy: 0.7506 - val_loss: 0.1868 - val_accuracy: 0.7365\n",
      "Epoch 967/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1761 - accuracy: 0.7417 - val_loss: 0.1881 - val_accuracy: 0.7344\n",
      "Epoch 968/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1730 - accuracy: 0.7479 - val_loss: 0.1875 - val_accuracy: 0.7336\n",
      "Epoch 969/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1744 - accuracy: 0.7460 - val_loss: 0.1870 - val_accuracy: 0.7371\n",
      "Epoch 970/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1748 - accuracy: 0.7453 - val_loss: 0.1879 - val_accuracy: 0.7348\n",
      "Epoch 971/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1766 - accuracy: 0.7424 - val_loss: 0.1871 - val_accuracy: 0.7350\n",
      "Epoch 972/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1786 - accuracy: 0.7393 - val_loss: 0.1878 - val_accuracy: 0.7363\n",
      "Epoch 973/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1756 - accuracy: 0.7451 - val_loss: 0.1881 - val_accuracy: 0.7349\n",
      "Epoch 974/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1731 - accuracy: 0.7478 - val_loss: 0.1905 - val_accuracy: 0.7301\n",
      "Epoch 975/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1752 - accuracy: 0.7445 - val_loss: 0.1877 - val_accuracy: 0.7349\n",
      "Epoch 976/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1762 - accuracy: 0.7405 - val_loss: 0.1876 - val_accuracy: 0.7352\n",
      "Epoch 977/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1757 - accuracy: 0.7417 - val_loss: 0.1885 - val_accuracy: 0.7331\n",
      "Epoch 978/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1765 - accuracy: 0.7422 - val_loss: 0.1878 - val_accuracy: 0.7356\n",
      "Epoch 979/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1739 - accuracy: 0.7457 - val_loss: 0.1906 - val_accuracy: 0.7292\n",
      "Epoch 980/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1736 - accuracy: 0.7465 - val_loss: 0.1887 - val_accuracy: 0.7340\n",
      "Epoch 981/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1740 - accuracy: 0.7446 - val_loss: 0.1887 - val_accuracy: 0.7328\n",
      "Epoch 982/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1760 - accuracy: 0.7420 - val_loss: 0.1881 - val_accuracy: 0.7363\n",
      "Epoch 983/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1765 - accuracy: 0.7431 - val_loss: 0.1875 - val_accuracy: 0.7357\n",
      "Epoch 984/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1766 - accuracy: 0.7416 - val_loss: 0.1876 - val_accuracy: 0.7370\n",
      "Epoch 985/1000\n",
      "563/563 [==============================] - 1s 998us/step - loss: 0.1760 - accuracy: 0.7433 - val_loss: 0.1870 - val_accuracy: 0.7368\n",
      "Epoch 986/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1738 - accuracy: 0.7451 - val_loss: 0.1875 - val_accuracy: 0.7352\n",
      "Epoch 987/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1754 - accuracy: 0.7449 - val_loss: 0.1878 - val_accuracy: 0.7356\n",
      "Epoch 988/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1776 - accuracy: 0.7413 - val_loss: 0.1876 - val_accuracy: 0.7344\n",
      "Epoch 989/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1731 - accuracy: 0.7495 - val_loss: 0.1877 - val_accuracy: 0.7318\n",
      "Epoch 990/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1765 - accuracy: 0.7445 - val_loss: 0.1886 - val_accuracy: 0.7330\n",
      "Epoch 991/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1755 - accuracy: 0.7448 - val_loss: 0.1888 - val_accuracy: 0.7343\n",
      "Epoch 992/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1757 - accuracy: 0.7454 - val_loss: 0.1880 - val_accuracy: 0.7356\n",
      "Epoch 993/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1777 - accuracy: 0.7403 - val_loss: 0.1877 - val_accuracy: 0.7358\n",
      "Epoch 994/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1757 - accuracy: 0.7425 - val_loss: 0.1877 - val_accuracy: 0.7361\n",
      "Epoch 995/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1783 - accuracy: 0.7392 - val_loss: 0.1881 - val_accuracy: 0.7356\n",
      "Epoch 996/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1737 - accuracy: 0.7476 - val_loss: 0.1868 - val_accuracy: 0.7348\n",
      "Epoch 997/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1762 - accuracy: 0.7414 - val_loss: 0.1872 - val_accuracy: 0.7362\n",
      "Epoch 998/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1733 - accuracy: 0.7470 - val_loss: 0.1892 - val_accuracy: 0.7315\n",
      "Epoch 999/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1745 - accuracy: 0.7463 - val_loss: 0.1881 - val_accuracy: 0.7319\n",
      "Epoch 1000/1000\n",
      "563/563 [==============================] - 1s 998us/step - loss: 0.1766 - accuracy: 0.7423 - val_loss: 0.1874 - val_accuracy: 0.7350\n"
     ]
    }
   ],
   "source": [
    "# Fit the model using 50 epochs and the training data\n",
    "fit_model_A25 = nn_A25.fit(X_train_scaled, y_train, validation_split=0.3, epochs=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alternative Model 25 Results\n",
      "Loss: 0.18960236012935638, Accuracy: 0.7255976796150208\n"
     ]
    }
   ],
   "source": [
    "print(\"Alternative Model 25 Results\")\n",
    "# Evaluate the model loss and accuracy metrics using the evaluate method and the test data\n",
    "model_loss, model_accuracy = nn_A25.evaluate(X_test_scaled,y_test,verbose=0)\n",
    "# Display the model loss and accuracy results\n",
    "print(f\"Loss: {model_loss}, Accuracy: {model_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alternative Model 26+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "114"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the the number of inputs (features) to the model\n",
    "number_input_features = len(X_train.iloc[0])\n",
    "# Review the number of features\n",
    "number_input_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the number of neurons in the output layer\n",
    "number_output_neurons_A26 = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the number of hidden nodes for the first hidden layer\n",
    "hidden_nodes_layer1_A26 =  (number_input_features + number_output_neurons_A26) // 2\n",
    "# Review the number of hidden nodes in the first layer\n",
    "hidden_nodes_layer1_A26"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the number of hidden nodes for the second hidden layer\n",
    "hidden_nodes_layer2_A26 =  (hidden_nodes_layer1_A26 + number_output_neurons_A26) // 2\n",
    "# Review the number of hidden nodes in the second layer\n",
    "hidden_nodes_layer2_A26"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the number of hidden nodes for the third hidden layer\n",
    "hidden_nodes_layer3_A26 =  (hidden_nodes_layer2_A26 + number_output_neurons_A26) // 2\n",
    "# Review the number of hidden nodes in the third layer\n",
    "hidden_nodes_layer3_A26"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the number of hidden nodes for the fourth hidden layer\n",
    "hidden_nodes_layer4_A26 =  (hidden_nodes_layer3_A26 + number_output_neurons_A26) // 2\n",
    "# Review the number of hidden nodes in the fourth layer\n",
    "hidden_nodes_layer4_A26"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the number of hidden nodes for the fifth hidden layer\n",
    "hidden_nodes_layer5_A26 =  (hidden_nodes_layer4_A26 + number_output_neurons_A26) // 2\n",
    "# Review the number of hidden nodes in the fifth layer\n",
    "hidden_nodes_layer5_A26"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the number of hidden nodes for the Sixth hidden layer\n",
    "hidden_nodes_layer6_A26 =  (hidden_nodes_layer5_A26 + number_output_neurons_A26) // 2\n",
    "# Review the number of hidden nodes in the sixth layer\n",
    "hidden_nodes_layer6_A26"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_17 (Dense)             (None, 57)                6555      \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 29)                1682      \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 15)                450       \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 8)                 128       \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 4)                 36        \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 2)                 10        \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 1)                 3         \n",
      "=================================================================\n",
      "Total params: 8,864\n",
      "Trainable params: 8,864\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Create and Display the Sequential Model Instance \n",
    "# for Model A26\n",
    "nn_A26 = Sequential() \n",
    "\n",
    "# Add the first hidden layer\n",
    "nn_A26.add(Dense(units=hidden_nodes_layer1_A26, input_dim=number_input_features, activation=\"relu\"))\n",
    "# Add the second hidden layer\n",
    "nn_A26.add(Dense(units=hidden_nodes_layer2_A26, input_dim=number_input_features, activation=\"relu\"))\n",
    "# Add the third hidden layer\n",
    "nn_A26.add(Dense(units=hidden_nodes_layer3_A26, input_dim=number_input_features, activation=\"relu\"))\n",
    "# Add the fourth hidden layer\n",
    "nn_A26.add(Dense(units=hidden_nodes_layer4_A26, input_dim=number_input_features, activation=\"relu\"))\n",
    "# Add the fifth hidden layer\n",
    "nn_A26.add(Dense(units=hidden_nodes_layer5_A26, input_dim=number_input_features, activation=\"relu\"))\n",
    "# Add the sixth hidden layer\n",
    "nn_A26.add(Dense(units=hidden_nodes_layer6_A26, activation=\"relu\"))\n",
    "\n",
    "# Add the output layer to the model specifying the number of output neurons and activation function\n",
    "nn_A26.add(Dense(units=number_output_neurons_A26, activation=\"sigmoid\"))\n",
    "\n",
    "# Display the Sequential model summary\n",
    "nn_A26.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the Sequential model\n",
    "nn_A26.compile(loss=\"mse\", optimizer=\"adadelta\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2516 - accuracy: 0.4358 - val_loss: 0.2517 - val_accuracy: 0.4255\n",
      "Epoch 2/1000\n",
      "563/563 [==============================] - 1s 997us/step - loss: 0.2515 - accuracy: 0.4380 - val_loss: 0.2516 - val_accuracy: 0.4259\n",
      "Epoch 3/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2511 - accuracy: 0.4377 - val_loss: 0.2515 - val_accuracy: 0.4280\n",
      "Epoch 4/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2511 - accuracy: 0.4424 - val_loss: 0.2514 - val_accuracy: 0.4282\n",
      "Epoch 5/1000\n",
      "563/563 [==============================] - 1s 940us/step - loss: 0.2511 - accuracy: 0.4378 - val_loss: 0.2512 - val_accuracy: 0.4283\n",
      "Epoch 6/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2512 - accuracy: 0.4383 - val_loss: 0.2511 - val_accuracy: 0.4298\n",
      "Epoch 7/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2513 - accuracy: 0.4419 - val_loss: 0.2510 - val_accuracy: 0.4339\n",
      "Epoch 8/1000\n",
      "563/563 [==============================] - 1s 988us/step - loss: 0.2510 - accuracy: 0.4361 - val_loss: 0.2509 - val_accuracy: 0.4374\n",
      "Epoch 9/1000\n",
      "563/563 [==============================] - 1s 997us/step - loss: 0.2510 - accuracy: 0.4622 - val_loss: 0.2509 - val_accuracy: 0.5133\n",
      "Epoch 10/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2507 - accuracy: 0.5235 - val_loss: 0.2508 - val_accuracy: 0.5165\n",
      "Epoch 11/1000\n",
      "563/563 [==============================] - 1s 938us/step - loss: 0.2510 - accuracy: 0.5208 - val_loss: 0.2507 - val_accuracy: 0.5167\n",
      "Epoch 12/1000\n",
      "563/563 [==============================] - 1s 941us/step - loss: 0.2504 - accuracy: 0.5241 - val_loss: 0.2507 - val_accuracy: 0.5165\n",
      "Epoch 13/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2508 - accuracy: 0.5257 - val_loss: 0.2506 - val_accuracy: 0.5210\n",
      "Epoch 14/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2505 - accuracy: 0.5254 - val_loss: 0.2506 - val_accuracy: 0.5219\n",
      "Epoch 15/1000\n",
      "563/563 [==============================] - 1s 972us/step - loss: 0.2505 - accuracy: 0.5237 - val_loss: 0.2506 - val_accuracy: 0.5075\n",
      "Epoch 16/1000\n",
      "563/563 [==============================] - 1s 972us/step - loss: 0.2505 - accuracy: 0.5029 - val_loss: 0.2505 - val_accuracy: 0.4975\n",
      "Epoch 17/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2506 - accuracy: 0.5007 - val_loss: 0.2505 - val_accuracy: 0.5006\n",
      "Epoch 18/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2502 - accuracy: 0.5058 - val_loss: 0.2504 - val_accuracy: 0.5092\n",
      "Epoch 19/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2504 - accuracy: 0.5124 - val_loss: 0.2504 - val_accuracy: 0.5093\n",
      "Epoch 20/1000\n",
      "563/563 [==============================] - 1s 974us/step - loss: 0.2502 - accuracy: 0.5151 - val_loss: 0.2504 - val_accuracy: 0.5092\n",
      "Epoch 21/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2502 - accuracy: 0.5100 - val_loss: 0.2503 - val_accuracy: 0.5092\n",
      "Epoch 22/1000\n",
      "563/563 [==============================] - 1s 988us/step - loss: 0.2505 - accuracy: 0.5137 - val_loss: 0.2503 - val_accuracy: 0.5100\n",
      "Epoch 23/1000\n",
      "563/563 [==============================] - 1s 995us/step - loss: 0.2503 - accuracy: 0.5136 - val_loss: 0.2503 - val_accuracy: 0.5106\n",
      "Epoch 24/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2501 - accuracy: 0.5096 - val_loss: 0.2502 - val_accuracy: 0.5108\n",
      "Epoch 25/1000\n",
      "563/563 [==============================] - 1s 988us/step - loss: 0.2503 - accuracy: 0.5123 - val_loss: 0.2502 - val_accuracy: 0.5341\n",
      "Epoch 26/1000\n",
      "563/563 [==============================] - 1s 998us/step - loss: 0.2502 - accuracy: 0.5387 - val_loss: 0.2502 - val_accuracy: 0.5354\n",
      "Epoch 27/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2501 - accuracy: 0.5339 - val_loss: 0.2501 - val_accuracy: 0.5355\n",
      "Epoch 28/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2501 - accuracy: 0.5399 - val_loss: 0.2501 - val_accuracy: 0.5354\n",
      "Epoch 29/1000\n",
      "563/563 [==============================] - 1s 966us/step - loss: 0.2501 - accuracy: 0.5277 - val_loss: 0.2501 - val_accuracy: 0.5365\n",
      "Epoch 30/1000\n",
      "563/563 [==============================] - 1s 963us/step - loss: 0.2501 - accuracy: 0.5365 - val_loss: 0.2501 - val_accuracy: 0.5381\n",
      "Epoch 31/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2499 - accuracy: 0.5443 - val_loss: 0.2500 - val_accuracy: 0.5385\n",
      "Epoch 32/1000\n",
      "563/563 [==============================] - 1s 995us/step - loss: 0.2500 - accuracy: 0.5352 - val_loss: 0.2500 - val_accuracy: 0.5384\n",
      "Epoch 33/1000\n",
      "563/563 [==============================] - 1s 975us/step - loss: 0.2503 - accuracy: 0.5342 - val_loss: 0.2500 - val_accuracy: 0.5412\n",
      "Epoch 34/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2500 - accuracy: 0.5441 - val_loss: 0.2499 - val_accuracy: 0.5398\n",
      "Epoch 35/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2500 - accuracy: 0.5370 - val_loss: 0.2499 - val_accuracy: 0.5399\n",
      "Epoch 36/1000\n",
      "563/563 [==============================] - 1s 947us/step - loss: 0.2500 - accuracy: 0.5390 - val_loss: 0.2499 - val_accuracy: 0.5400\n",
      "Epoch 37/1000\n",
      "563/563 [==============================] - 1s 970us/step - loss: 0.2498 - accuracy: 0.5353 - val_loss: 0.2499 - val_accuracy: 0.5400\n",
      "Epoch 38/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2498 - accuracy: 0.5426 - val_loss: 0.2498 - val_accuracy: 0.5407\n",
      "Epoch 39/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2499 - accuracy: 0.5372 - val_loss: 0.2498 - val_accuracy: 0.5544\n",
      "Epoch 40/1000\n",
      "563/563 [==============================] - 1s 975us/step - loss: 0.2499 - accuracy: 0.5557 - val_loss: 0.2498 - val_accuracy: 0.5715\n",
      "Epoch 41/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2499 - accuracy: 0.5581 - val_loss: 0.2498 - val_accuracy: 0.5715\n",
      "Epoch 42/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2501 - accuracy: 0.5595 - val_loss: 0.2497 - val_accuracy: 0.5751\n",
      "Epoch 43/1000\n",
      "563/563 [==============================] - 1s 972us/step - loss: 0.2497 - accuracy: 0.5636 - val_loss: 0.2497 - val_accuracy: 0.5759\n",
      "Epoch 44/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2499 - accuracy: 0.5673 - val_loss: 0.2497 - val_accuracy: 0.5761\n",
      "Epoch 45/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2497 - accuracy: 0.5694 - val_loss: 0.2497 - val_accuracy: 0.5763\n",
      "Epoch 46/1000\n",
      "563/563 [==============================] - 1s 990us/step - loss: 0.2497 - accuracy: 0.5661 - val_loss: 0.2496 - val_accuracy: 0.5763\n",
      "Epoch 47/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2496 - accuracy: 0.5702 - val_loss: 0.2496 - val_accuracy: 0.5766\n",
      "Epoch 48/1000\n",
      "563/563 [==============================] - 1s 991us/step - loss: 0.2496 - accuracy: 0.5673 - val_loss: 0.2496 - val_accuracy: 0.5764\n",
      "Epoch 49/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2496 - accuracy: 0.5651 - val_loss: 0.2496 - val_accuracy: 0.5764\n",
      "Epoch 50/1000\n",
      "563/563 [==============================] - 1s 979us/step - loss: 0.2497 - accuracy: 0.5638 - val_loss: 0.2495 - val_accuracy: 0.5771\n",
      "Epoch 51/1000\n",
      "563/563 [==============================] - 1s 986us/step - loss: 0.2495 - accuracy: 0.5699 - val_loss: 0.2495 - val_accuracy: 0.5877\n",
      "Epoch 52/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2497 - accuracy: 0.5692 - val_loss: 0.2495 - val_accuracy: 0.5877\n",
      "Epoch 53/1000\n",
      "563/563 [==============================] - 1s 997us/step - loss: 0.2497 - accuracy: 0.5710 - val_loss: 0.2495 - val_accuracy: 0.5877\n",
      "Epoch 54/1000\n",
      "563/563 [==============================] - 1s 984us/step - loss: 0.2496 - accuracy: 0.5742 - val_loss: 0.2494 - val_accuracy: 0.5877\n",
      "Epoch 55/1000\n",
      "563/563 [==============================] - 1s 986us/step - loss: 0.2497 - accuracy: 0.5746 - val_loss: 0.2494 - val_accuracy: 0.5893\n",
      "Epoch 56/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2495 - accuracy: 0.5737 - val_loss: 0.2494 - val_accuracy: 0.5890\n",
      "Epoch 57/1000\n",
      "563/563 [==============================] - 1s 956us/step - loss: 0.2497 - accuracy: 0.5738 - val_loss: 0.2494 - val_accuracy: 0.5888\n",
      "Epoch 58/1000\n",
      "563/563 [==============================] - 1s 979us/step - loss: 0.2495 - accuracy: 0.5772 - val_loss: 0.2494 - val_accuracy: 0.5947\n",
      "Epoch 59/1000\n",
      "563/563 [==============================] - 1s 990us/step - loss: 0.2495 - accuracy: 0.5782 - val_loss: 0.2493 - val_accuracy: 0.5948\n",
      "Epoch 60/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2493 - accuracy: 0.5825 - val_loss: 0.2493 - val_accuracy: 0.6158\n",
      "Epoch 61/1000\n",
      "563/563 [==============================] - 1s 977us/step - loss: 0.2493 - accuracy: 0.5949 - val_loss: 0.2493 - val_accuracy: 0.6158\n",
      "Epoch 62/1000\n",
      "563/563 [==============================] - 1s 982us/step - loss: 0.2495 - accuracy: 0.5936 - val_loss: 0.2493 - val_accuracy: 0.6158\n",
      "Epoch 63/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2493 - accuracy: 0.6000 - val_loss: 0.2492 - val_accuracy: 0.6160\n",
      "Epoch 64/1000\n",
      "563/563 [==============================] - 1s 981us/step - loss: 0.2495 - accuracy: 0.5948 - val_loss: 0.2492 - val_accuracy: 0.6161\n",
      "Epoch 65/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2495 - accuracy: 0.5892 - val_loss: 0.2492 - val_accuracy: 0.6161\n",
      "Epoch 66/1000\n",
      "563/563 [==============================] - 1s 947us/step - loss: 0.2492 - accuracy: 0.5938 - val_loss: 0.2492 - val_accuracy: 0.6161\n",
      "Epoch 67/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2493 - accuracy: 0.5995 - val_loss: 0.2492 - val_accuracy: 0.6160\n",
      "Epoch 68/1000\n",
      "563/563 [==============================] - 1s 982us/step - loss: 0.2491 - accuracy: 0.5935 - val_loss: 0.2491 - val_accuracy: 0.6158\n",
      "Epoch 69/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2493 - accuracy: 0.5965 - val_loss: 0.2491 - val_accuracy: 0.6164\n",
      "Epoch 70/1000\n",
      "563/563 [==============================] - 1s 998us/step - loss: 0.2492 - accuracy: 0.5961 - val_loss: 0.2491 - val_accuracy: 0.6162\n",
      "Epoch 71/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2491 - accuracy: 0.5973 - val_loss: 0.2491 - val_accuracy: 0.6192\n",
      "Epoch 72/1000\n",
      "563/563 [==============================] - 1s 961us/step - loss: 0.2490 - accuracy: 0.5977 - val_loss: 0.2491 - val_accuracy: 0.6192\n",
      "Epoch 73/1000\n",
      "563/563 [==============================] - 1s 990us/step - loss: 0.2491 - accuracy: 0.5973 - val_loss: 0.2490 - val_accuracy: 0.6192\n",
      "Epoch 74/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2493 - accuracy: 0.5979 - val_loss: 0.2490 - val_accuracy: 0.6227\n",
      "Epoch 75/1000\n",
      "563/563 [==============================] - 1s 984us/step - loss: 0.2489 - accuracy: 0.6009 - val_loss: 0.2490 - val_accuracy: 0.6230\n",
      "Epoch 76/1000\n",
      "563/563 [==============================] - 1s 977us/step - loss: 0.2490 - accuracy: 0.5996 - val_loss: 0.2490 - val_accuracy: 0.6227\n",
      "Epoch 77/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2489 - accuracy: 0.5997 - val_loss: 0.2490 - val_accuracy: 0.6227\n",
      "Epoch 78/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2489 - accuracy: 0.6034 - val_loss: 0.2489 - val_accuracy: 0.6228\n",
      "Epoch 79/1000\n",
      "563/563 [==============================] - 1s 949us/step - loss: 0.2490 - accuracy: 0.5983 - val_loss: 0.2489 - val_accuracy: 0.6228\n",
      "Epoch 80/1000\n",
      "563/563 [==============================] - 1s 970us/step - loss: 0.2491 - accuracy: 0.6012 - val_loss: 0.2489 - val_accuracy: 0.6231\n",
      "Epoch 81/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2491 - accuracy: 0.5982 - val_loss: 0.2489 - val_accuracy: 0.6231\n",
      "Epoch 82/1000\n",
      "563/563 [==============================] - 1s 966us/step - loss: 0.2490 - accuracy: 0.6011 - val_loss: 0.2489 - val_accuracy: 0.6231\n",
      "Epoch 83/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2489 - accuracy: 0.6027 - val_loss: 0.2488 - val_accuracy: 0.6231\n",
      "Epoch 84/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2488 - accuracy: 0.5983 - val_loss: 0.2488 - val_accuracy: 0.6231\n",
      "Epoch 85/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2489 - accuracy: 0.5941 - val_loss: 0.2488 - val_accuracy: 0.6230\n",
      "Epoch 86/1000\n",
      "563/563 [==============================] - 1s 982us/step - loss: 0.2488 - accuracy: 0.5986 - val_loss: 0.2488 - val_accuracy: 0.6236\n",
      "Epoch 87/1000\n",
      "563/563 [==============================] - 1s 986us/step - loss: 0.2491 - accuracy: 0.6009 - val_loss: 0.2488 - val_accuracy: 0.6232\n",
      "Epoch 88/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2487 - accuracy: 0.6038 - val_loss: 0.2487 - val_accuracy: 0.6236\n",
      "Epoch 89/1000\n",
      "563/563 [==============================] - 1s 966us/step - loss: 0.2488 - accuracy: 0.6049 - val_loss: 0.2487 - val_accuracy: 0.6235\n",
      "Epoch 90/1000\n",
      "563/563 [==============================] - 1s 981us/step - loss: 0.2487 - accuracy: 0.6088 - val_loss: 0.2487 - val_accuracy: 0.6235\n",
      "Epoch 91/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2489 - accuracy: 0.6007 - val_loss: 0.2487 - val_accuracy: 0.6232\n",
      "Epoch 92/1000\n",
      "563/563 [==============================] - 1s 982us/step - loss: 0.2487 - accuracy: 0.6030 - val_loss: 0.2486 - val_accuracy: 0.6232\n",
      "Epoch 93/1000\n",
      "563/563 [==============================] - 1s 981us/step - loss: 0.2488 - accuracy: 0.6038 - val_loss: 0.2486 - val_accuracy: 0.6232\n",
      "Epoch 94/1000\n",
      "563/563 [==============================] - 1s 976us/step - loss: 0.2489 - accuracy: 0.6056 - val_loss: 0.2486 - val_accuracy: 0.6233\n",
      "Epoch 95/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2488 - accuracy: 0.6070 - val_loss: 0.2486 - val_accuracy: 0.6233\n",
      "Epoch 96/1000\n",
      "563/563 [==============================] - 1s 984us/step - loss: 0.2486 - accuracy: 0.6061 - val_loss: 0.2486 - val_accuracy: 0.6256\n",
      "Epoch 97/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2487 - accuracy: 0.6082 - val_loss: 0.2485 - val_accuracy: 0.6258\n",
      "Epoch 98/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2487 - accuracy: 0.6019 - val_loss: 0.2485 - val_accuracy: 0.6259\n",
      "Epoch 99/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2487 - accuracy: 0.6029 - val_loss: 0.2485 - val_accuracy: 0.6261\n",
      "Epoch 100/1000\n",
      "563/563 [==============================] - 1s 997us/step - loss: 0.2487 - accuracy: 0.6080 - val_loss: 0.2485 - val_accuracy: 0.6261\n",
      "Epoch 101/1000\n",
      "563/563 [==============================] - 1s 965us/step - loss: 0.2487 - accuracy: 0.6033 - val_loss: 0.2485 - val_accuracy: 0.6268\n",
      "Epoch 102/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2488 - accuracy: 0.6049 - val_loss: 0.2484 - val_accuracy: 0.6267\n",
      "Epoch 103/1000\n",
      "563/563 [==============================] - 1s 974us/step - loss: 0.2487 - accuracy: 0.6066 - val_loss: 0.2484 - val_accuracy: 0.6266\n",
      "Epoch 104/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2485 - accuracy: 0.6031 - val_loss: 0.2484 - val_accuracy: 0.6271\n",
      "Epoch 105/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2485 - accuracy: 0.6065 - val_loss: 0.2484 - val_accuracy: 0.6276\n",
      "Epoch 106/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2485 - accuracy: 0.6111 - val_loss: 0.2483 - val_accuracy: 0.6278\n",
      "Epoch 107/1000\n",
      "563/563 [==============================] - 1s 984us/step - loss: 0.2487 - accuracy: 0.6125 - val_loss: 0.2483 - val_accuracy: 0.6278\n",
      "Epoch 108/1000\n",
      "563/563 [==============================] - 1s 974us/step - loss: 0.2485 - accuracy: 0.6058 - val_loss: 0.2483 - val_accuracy: 0.6279\n",
      "Epoch 109/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2484 - accuracy: 0.6132 - val_loss: 0.2482 - val_accuracy: 0.6279\n",
      "Epoch 110/1000\n",
      "563/563 [==============================] - 1s 988us/step - loss: 0.2484 - accuracy: 0.6052 - val_loss: 0.2482 - val_accuracy: 0.6294\n",
      "Epoch 111/1000\n",
      "563/563 [==============================] - 1s 979us/step - loss: 0.2485 - accuracy: 0.6131 - val_loss: 0.2482 - val_accuracy: 0.6298\n",
      "Epoch 112/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2483 - accuracy: 0.6137 - val_loss: 0.2482 - val_accuracy: 0.6298\n",
      "Epoch 113/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2484 - accuracy: 0.6121 - val_loss: 0.2481 - val_accuracy: 0.6294\n",
      "Epoch 114/1000\n",
      "563/563 [==============================] - 1s 966us/step - loss: 0.2484 - accuracy: 0.6110 - val_loss: 0.2481 - val_accuracy: 0.6331\n",
      "Epoch 115/1000\n",
      "563/563 [==============================] - 1s 981us/step - loss: 0.2482 - accuracy: 0.6114 - val_loss: 0.2481 - val_accuracy: 0.6331\n",
      "Epoch 116/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2482 - accuracy: 0.6195 - val_loss: 0.2480 - val_accuracy: 0.6328\n",
      "Epoch 117/1000\n",
      "563/563 [==============================] - 1s 987us/step - loss: 0.2483 - accuracy: 0.6149 - val_loss: 0.2480 - val_accuracy: 0.6329\n",
      "Epoch 118/1000\n",
      "563/563 [==============================] - 1s 987us/step - loss: 0.2483 - accuracy: 0.6119 - val_loss: 0.2480 - val_accuracy: 0.6329\n",
      "Epoch 119/1000\n",
      "563/563 [==============================] - 1s 952us/step - loss: 0.2480 - accuracy: 0.6191 - val_loss: 0.2479 - val_accuracy: 0.6329\n",
      "Epoch 120/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2481 - accuracy: 0.6104 - val_loss: 0.2479 - val_accuracy: 0.6331\n",
      "Epoch 121/1000\n",
      "563/563 [==============================] - 1s 984us/step - loss: 0.2482 - accuracy: 0.6097 - val_loss: 0.2479 - val_accuracy: 0.6331\n",
      "Epoch 122/1000\n",
      "563/563 [==============================] - 1s 943us/step - loss: 0.2481 - accuracy: 0.6101 - val_loss: 0.2479 - val_accuracy: 0.6325\n",
      "Epoch 123/1000\n",
      "563/563 [==============================] - 1s 991us/step - loss: 0.2479 - accuracy: 0.6141 - val_loss: 0.2478 - val_accuracy: 0.6325\n",
      "Epoch 124/1000\n",
      "563/563 [==============================] - 1s 997us/step - loss: 0.2478 - accuracy: 0.6161 - val_loss: 0.2478 - val_accuracy: 0.6325\n",
      "Epoch 125/1000\n",
      "563/563 [==============================] - 1s 954us/step - loss: 0.2479 - accuracy: 0.6167 - val_loss: 0.2478 - val_accuracy: 0.6325\n",
      "Epoch 126/1000\n",
      "563/563 [==============================] - 1s 963us/step - loss: 0.2478 - accuracy: 0.6195 - val_loss: 0.2477 - val_accuracy: 0.6325\n",
      "Epoch 127/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2478 - accuracy: 0.6182 - val_loss: 0.2477 - val_accuracy: 0.6324\n",
      "Epoch 128/1000\n",
      "563/563 [==============================] - 1s 986us/step - loss: 0.2477 - accuracy: 0.6138 - val_loss: 0.2477 - val_accuracy: 0.6324\n",
      "Epoch 129/1000\n",
      "563/563 [==============================] - 1s 966us/step - loss: 0.2479 - accuracy: 0.6175 - val_loss: 0.2476 - val_accuracy: 0.6324\n",
      "Epoch 130/1000\n",
      "563/563 [==============================] - 1s 992us/step - loss: 0.2481 - accuracy: 0.6127 - val_loss: 0.2476 - val_accuracy: 0.6324\n",
      "Epoch 131/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2478 - accuracy: 0.6142 - val_loss: 0.2476 - val_accuracy: 0.6324\n",
      "Epoch 132/1000\n",
      "563/563 [==============================] - 1s 961us/step - loss: 0.2480 - accuracy: 0.6109 - val_loss: 0.2475 - val_accuracy: 0.6325\n",
      "Epoch 133/1000\n",
      "563/563 [==============================] - 1s 975us/step - loss: 0.2478 - accuracy: 0.6160 - val_loss: 0.2475 - val_accuracy: 0.6325\n",
      "Epoch 134/1000\n",
      "563/563 [==============================] - 1s 991us/step - loss: 0.2475 - accuracy: 0.6188 - val_loss: 0.2474 - val_accuracy: 0.6328\n",
      "Epoch 135/1000\n",
      "563/563 [==============================] - 1s 981us/step - loss: 0.2476 - accuracy: 0.6177 - val_loss: 0.2474 - val_accuracy: 0.6331\n",
      "Epoch 136/1000\n",
      "563/563 [==============================] - 1s 982us/step - loss: 0.2477 - accuracy: 0.6117 - val_loss: 0.2474 - val_accuracy: 0.6329\n",
      "Epoch 137/1000\n",
      "563/563 [==============================] - 1s 988us/step - loss: 0.2474 - accuracy: 0.6142 - val_loss: 0.2473 - val_accuracy: 0.6329\n",
      "Epoch 138/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2476 - accuracy: 0.6177 - val_loss: 0.2473 - val_accuracy: 0.6349\n",
      "Epoch 139/1000\n",
      "563/563 [==============================] - 1s 979us/step - loss: 0.2474 - accuracy: 0.6185 - val_loss: 0.2473 - val_accuracy: 0.6349\n",
      "Epoch 140/1000\n",
      "563/563 [==============================] - 1s 993us/step - loss: 0.2475 - accuracy: 0.6176 - val_loss: 0.2472 - val_accuracy: 0.6349\n",
      "Epoch 141/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2474 - accuracy: 0.6165 - val_loss: 0.2472 - val_accuracy: 0.6533\n",
      "Epoch 142/1000\n",
      "563/563 [==============================] - 1s 995us/step - loss: 0.2473 - accuracy: 0.6359 - val_loss: 0.2472 - val_accuracy: 0.6533\n",
      "Epoch 143/1000\n",
      "563/563 [==============================] - 1s 984us/step - loss: 0.2473 - accuracy: 0.6467 - val_loss: 0.2471 - val_accuracy: 0.6531\n",
      "Epoch 144/1000\n",
      "563/563 [==============================] - 1s 950us/step - loss: 0.2473 - accuracy: 0.6397 - val_loss: 0.2471 - val_accuracy: 0.6531\n",
      "Epoch 145/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2471 - accuracy: 0.6437 - val_loss: 0.2470 - val_accuracy: 0.6548\n",
      "Epoch 146/1000\n",
      "563/563 [==============================] - 1s 959us/step - loss: 0.2472 - accuracy: 0.6467 - val_loss: 0.2470 - val_accuracy: 0.6547\n",
      "Epoch 147/1000\n",
      "563/563 [==============================] - 1s 952us/step - loss: 0.2470 - accuracy: 0.6397 - val_loss: 0.2470 - val_accuracy: 0.6547\n",
      "Epoch 148/1000\n",
      "563/563 [==============================] - 1s 984us/step - loss: 0.2473 - accuracy: 0.6455 - val_loss: 0.2469 - val_accuracy: 0.6814\n",
      "Epoch 149/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2469 - accuracy: 0.6772 - val_loss: 0.2469 - val_accuracy: 0.6814\n",
      "Epoch 150/1000\n",
      "563/563 [==============================] - 1s 972us/step - loss: 0.2470 - accuracy: 0.6768 - val_loss: 0.2468 - val_accuracy: 0.6814\n",
      "Epoch 151/1000\n",
      "563/563 [==============================] - 1s 982us/step - loss: 0.2470 - accuracy: 0.6695 - val_loss: 0.2468 - val_accuracy: 0.6832\n",
      "Epoch 152/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2470 - accuracy: 0.6782 - val_loss: 0.2468 - val_accuracy: 0.6902\n",
      "Epoch 153/1000\n",
      "563/563 [==============================] - 1s 988us/step - loss: 0.2467 - accuracy: 0.6853 - val_loss: 0.2467 - val_accuracy: 0.6902\n",
      "Epoch 154/1000\n",
      "563/563 [==============================] - 1s 995us/step - loss: 0.2467 - accuracy: 0.6840 - val_loss: 0.2467 - val_accuracy: 0.6902\n",
      "Epoch 155/1000\n",
      "563/563 [==============================] - 1s 959us/step - loss: 0.2468 - accuracy: 0.6788 - val_loss: 0.2466 - val_accuracy: 0.6903\n",
      "Epoch 156/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2469 - accuracy: 0.6839 - val_loss: 0.2466 - val_accuracy: 0.6906\n",
      "Epoch 157/1000\n",
      "563/563 [==============================] - 1s 977us/step - loss: 0.2469 - accuracy: 0.6743 - val_loss: 0.2466 - val_accuracy: 0.6906\n",
      "Epoch 158/1000\n",
      "563/563 [==============================] - 1s 939us/step - loss: 0.2467 - accuracy: 0.6855 - val_loss: 0.2465 - val_accuracy: 0.6909\n",
      "Epoch 159/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2466 - accuracy: 0.6882 - val_loss: 0.2465 - val_accuracy: 0.6909\n",
      "Epoch 160/1000\n",
      "563/563 [==============================] - 1s 995us/step - loss: 0.2468 - accuracy: 0.6740 - val_loss: 0.2464 - val_accuracy: 0.6909\n",
      "Epoch 161/1000\n",
      "563/563 [==============================] - 1s 984us/step - loss: 0.2467 - accuracy: 0.6806 - val_loss: 0.2464 - val_accuracy: 0.6924\n",
      "Epoch 162/1000\n",
      "563/563 [==============================] - 1s 966us/step - loss: 0.2466 - accuracy: 0.6810 - val_loss: 0.2463 - val_accuracy: 0.7148\n",
      "Epoch 163/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2466 - accuracy: 0.7056 - val_loss: 0.2463 - val_accuracy: 0.7148\n",
      "Epoch 164/1000\n",
      "563/563 [==============================] - 1s 949us/step - loss: 0.2464 - accuracy: 0.7056 - val_loss: 0.2463 - val_accuracy: 0.7151\n",
      "Epoch 165/1000\n",
      "563/563 [==============================] - 1s 950us/step - loss: 0.2465 - accuracy: 0.7054 - val_loss: 0.2462 - val_accuracy: 0.7151\n",
      "Epoch 166/1000\n",
      "563/563 [==============================] - 1s 974us/step - loss: 0.2464 - accuracy: 0.7037 - val_loss: 0.2462 - val_accuracy: 0.7151\n",
      "Epoch 167/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2462 - accuracy: 0.7061 - val_loss: 0.2461 - val_accuracy: 0.7152\n",
      "Epoch 168/1000\n",
      "563/563 [==============================] - 1s 980us/step - loss: 0.2465 - accuracy: 0.7012 - val_loss: 0.2461 - val_accuracy: 0.7146\n",
      "Epoch 169/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2464 - accuracy: 0.7016 - val_loss: 0.2460 - val_accuracy: 0.7143\n",
      "Epoch 170/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2463 - accuracy: 0.6970 - val_loss: 0.2460 - val_accuracy: 0.7143\n",
      "Epoch 171/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2461 - accuracy: 0.7039 - val_loss: 0.2459 - val_accuracy: 0.7143\n",
      "Epoch 172/1000\n",
      "563/563 [==============================] - 1s 956us/step - loss: 0.2463 - accuracy: 0.7027 - val_loss: 0.2459 - val_accuracy: 0.7139\n",
      "Epoch 173/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2458 - accuracy: 0.7073 - val_loss: 0.2458 - val_accuracy: 0.7139\n",
      "Epoch 174/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2459 - accuracy: 0.7079 - val_loss: 0.2458 - val_accuracy: 0.7140\n",
      "Epoch 175/1000\n",
      "563/563 [==============================] - 1s 966us/step - loss: 0.2458 - accuracy: 0.7037 - val_loss: 0.2457 - val_accuracy: 0.7140\n",
      "Epoch 176/1000\n",
      "563/563 [==============================] - 1s 977us/step - loss: 0.2459 - accuracy: 0.7031 - val_loss: 0.2457 - val_accuracy: 0.7142\n",
      "Epoch 177/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2459 - accuracy: 0.7021 - val_loss: 0.2456 - val_accuracy: 0.7142\n",
      "Epoch 178/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2458 - accuracy: 0.7056 - val_loss: 0.2456 - val_accuracy: 0.7140\n",
      "Epoch 179/1000\n",
      "563/563 [==============================] - 1s 989us/step - loss: 0.2457 - accuracy: 0.7085 - val_loss: 0.2455 - val_accuracy: 0.7140\n",
      "Epoch 180/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2457 - accuracy: 0.7040 - val_loss: 0.2455 - val_accuracy: 0.7147\n",
      "Epoch 181/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2456 - accuracy: 0.7060 - val_loss: 0.2454 - val_accuracy: 0.7147\n",
      "Epoch 182/1000\n",
      "563/563 [==============================] - 1s 977us/step - loss: 0.2457 - accuracy: 0.7054 - val_loss: 0.2454 - val_accuracy: 0.7147\n",
      "Epoch 183/1000\n",
      "563/563 [==============================] - 1s 972us/step - loss: 0.2455 - accuracy: 0.7109 - val_loss: 0.2453 - val_accuracy: 0.7148\n",
      "Epoch 184/1000\n",
      "563/563 [==============================] - 1s 993us/step - loss: 0.2454 - accuracy: 0.7019 - val_loss: 0.2453 - val_accuracy: 0.7148\n",
      "Epoch 185/1000\n",
      "563/563 [==============================] - 1s 990us/step - loss: 0.2456 - accuracy: 0.6997 - val_loss: 0.2452 - val_accuracy: 0.7143\n",
      "Epoch 186/1000\n",
      "563/563 [==============================] - 1s 956us/step - loss: 0.2453 - accuracy: 0.6985 - val_loss: 0.2452 - val_accuracy: 0.7143\n",
      "Epoch 187/1000\n",
      "563/563 [==============================] - 1s 966us/step - loss: 0.2453 - accuracy: 0.7029 - val_loss: 0.2451 - val_accuracy: 0.7143\n",
      "Epoch 188/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2454 - accuracy: 0.7028 - val_loss: 0.2451 - val_accuracy: 0.7143\n",
      "Epoch 189/1000\n",
      "563/563 [==============================] - 1s 981us/step - loss: 0.2453 - accuracy: 0.7049 - val_loss: 0.2450 - val_accuracy: 0.7144\n",
      "Epoch 190/1000\n",
      "563/563 [==============================] - 1s 990us/step - loss: 0.2452 - accuracy: 0.7063 - val_loss: 0.2450 - val_accuracy: 0.7144\n",
      "Epoch 191/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2452 - accuracy: 0.7027 - val_loss: 0.2449 - val_accuracy: 0.7144\n",
      "Epoch 192/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2451 - accuracy: 0.7061 - val_loss: 0.2449 - val_accuracy: 0.7144\n",
      "Epoch 193/1000\n",
      "563/563 [==============================] - 1s 958us/step - loss: 0.2452 - accuracy: 0.7018 - val_loss: 0.2448 - val_accuracy: 0.7144\n",
      "Epoch 194/1000\n",
      "563/563 [==============================] - 1s 931us/step - loss: 0.2451 - accuracy: 0.7074 - val_loss: 0.2447 - val_accuracy: 0.7144\n",
      "Epoch 195/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2449 - accuracy: 0.7045 - val_loss: 0.2447 - val_accuracy: 0.7144\n",
      "Epoch 196/1000\n",
      "563/563 [==============================] - 1s 972us/step - loss: 0.2447 - accuracy: 0.7075 - val_loss: 0.2446 - val_accuracy: 0.7144\n",
      "Epoch 197/1000\n",
      "563/563 [==============================] - 1s 970us/step - loss: 0.2446 - accuracy: 0.7074 - val_loss: 0.2446 - val_accuracy: 0.7150\n",
      "Epoch 198/1000\n",
      "563/563 [==============================] - 1s 988us/step - loss: 0.2448 - accuracy: 0.7025 - val_loss: 0.2445 - val_accuracy: 0.7150\n",
      "Epoch 199/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2446 - accuracy: 0.7056 - val_loss: 0.2444 - val_accuracy: 0.7150\n",
      "Epoch 200/1000\n",
      "563/563 [==============================] - 1s 984us/step - loss: 0.2451 - accuracy: 0.6974 - val_loss: 0.2444 - val_accuracy: 0.7150\n",
      "Epoch 201/1000\n",
      "563/563 [==============================] - 1s 979us/step - loss: 0.2446 - accuracy: 0.7016 - val_loss: 0.2443 - val_accuracy: 0.7150\n",
      "Epoch 202/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2447 - accuracy: 0.7024 - val_loss: 0.2442 - val_accuracy: 0.7150\n",
      "Epoch 203/1000\n",
      "563/563 [==============================] - 1s 986us/step - loss: 0.2446 - accuracy: 0.7060 - val_loss: 0.2441 - val_accuracy: 0.7150\n",
      "Epoch 204/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2444 - accuracy: 0.7041 - val_loss: 0.2441 - val_accuracy: 0.7150\n",
      "Epoch 205/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2440 - accuracy: 0.7017 - val_loss: 0.2440 - val_accuracy: 0.7135\n",
      "Epoch 206/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2444 - accuracy: 0.7011 - val_loss: 0.2439 - val_accuracy: 0.7135\n",
      "Epoch 207/1000\n",
      "563/563 [==============================] - 1s 993us/step - loss: 0.2443 - accuracy: 0.7051 - val_loss: 0.2438 - val_accuracy: 0.7135\n",
      "Epoch 208/1000\n",
      "563/563 [==============================] - 1s 995us/step - loss: 0.2440 - accuracy: 0.7054 - val_loss: 0.2438 - val_accuracy: 0.7135\n",
      "Epoch 209/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2439 - accuracy: 0.7030 - val_loss: 0.2437 - val_accuracy: 0.7135\n",
      "Epoch 210/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2438 - accuracy: 0.7053 - val_loss: 0.2436 - val_accuracy: 0.7135\n",
      "Epoch 211/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2439 - accuracy: 0.6971 - val_loss: 0.2435 - val_accuracy: 0.7135\n",
      "Epoch 212/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2437 - accuracy: 0.7031 - val_loss: 0.2434 - val_accuracy: 0.7137\n",
      "Epoch 213/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2433 - accuracy: 0.7087 - val_loss: 0.2433 - val_accuracy: 0.7142\n",
      "Epoch 214/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2434 - accuracy: 0.7033 - val_loss: 0.2433 - val_accuracy: 0.7144\n",
      "Epoch 215/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2431 - accuracy: 0.7103 - val_loss: 0.2432 - val_accuracy: 0.7143\n",
      "Epoch 216/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2435 - accuracy: 0.7065 - val_loss: 0.2431 - val_accuracy: 0.7134\n",
      "Epoch 217/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2431 - accuracy: 0.7039 - val_loss: 0.2430 - val_accuracy: 0.7130\n",
      "Epoch 218/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2434 - accuracy: 0.6995 - val_loss: 0.2429 - val_accuracy: 0.7131\n",
      "Epoch 219/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2433 - accuracy: 0.7004 - val_loss: 0.2428 - val_accuracy: 0.7130\n",
      "Epoch 220/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2428 - accuracy: 0.7073 - val_loss: 0.2427 - val_accuracy: 0.7129\n",
      "Epoch 221/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2428 - accuracy: 0.7039 - val_loss: 0.2426 - val_accuracy: 0.7129\n",
      "Epoch 222/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2429 - accuracy: 0.7072 - val_loss: 0.2425 - val_accuracy: 0.7127\n",
      "Epoch 223/1000\n",
      "563/563 [==============================] - 1s 975us/step - loss: 0.2427 - accuracy: 0.7033 - val_loss: 0.2424 - val_accuracy: 0.7126\n",
      "Epoch 224/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2427 - accuracy: 0.7007 - val_loss: 0.2423 - val_accuracy: 0.7125\n",
      "Epoch 225/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2426 - accuracy: 0.7032 - val_loss: 0.2421 - val_accuracy: 0.7125\n",
      "Epoch 226/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2420 - accuracy: 0.7048 - val_loss: 0.2420 - val_accuracy: 0.7125\n",
      "Epoch 227/1000\n",
      "563/563 [==============================] - 1s 977us/step - loss: 0.2421 - accuracy: 0.7026 - val_loss: 0.2418 - val_accuracy: 0.7125\n",
      "Epoch 228/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2420 - accuracy: 0.7008 - val_loss: 0.2417 - val_accuracy: 0.7125\n",
      "Epoch 229/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2420 - accuracy: 0.7031 - val_loss: 0.2415 - val_accuracy: 0.7126\n",
      "Epoch 230/1000\n",
      "563/563 [==============================] - 1s 972us/step - loss: 0.2416 - accuracy: 0.7011 - val_loss: 0.2414 - val_accuracy: 0.7126\n",
      "Epoch 231/1000\n",
      "563/563 [==============================] - 1s 979us/step - loss: 0.2414 - accuracy: 0.7033 - val_loss: 0.2412 - val_accuracy: 0.7126\n",
      "Epoch 232/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2412 - accuracy: 0.7063 - val_loss: 0.2411 - val_accuracy: 0.7126\n",
      "Epoch 233/1000\n",
      "563/563 [==============================] - 1s 986us/step - loss: 0.2415 - accuracy: 0.6999 - val_loss: 0.2409 - val_accuracy: 0.7126\n",
      "Epoch 234/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2406 - accuracy: 0.7057 - val_loss: 0.2407 - val_accuracy: 0.7127\n",
      "Epoch 235/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2407 - accuracy: 0.6984 - val_loss: 0.2405 - val_accuracy: 0.7127\n",
      "Epoch 236/1000\n",
      "563/563 [==============================] - 1s 975us/step - loss: 0.2403 - accuracy: 0.7007 - val_loss: 0.2403 - val_accuracy: 0.7104\n",
      "Epoch 237/1000\n",
      "563/563 [==============================] - 1s 972us/step - loss: 0.2403 - accuracy: 0.6997 - val_loss: 0.2401 - val_accuracy: 0.7108\n",
      "Epoch 238/1000\n",
      "563/563 [==============================] - 1s 970us/step - loss: 0.2396 - accuracy: 0.7023 - val_loss: 0.2399 - val_accuracy: 0.7099\n",
      "Epoch 239/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2397 - accuracy: 0.7024 - val_loss: 0.2397 - val_accuracy: 0.7098\n",
      "Epoch 240/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2397 - accuracy: 0.7018 - val_loss: 0.2395 - val_accuracy: 0.7104\n",
      "Epoch 241/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2399 - accuracy: 0.6988 - val_loss: 0.2393 - val_accuracy: 0.7104\n",
      "Epoch 242/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2390 - accuracy: 0.6967 - val_loss: 0.2390 - val_accuracy: 0.7104\n",
      "Epoch 243/1000\n",
      "563/563 [==============================] - 1s 997us/step - loss: 0.2390 - accuracy: 0.6986 - val_loss: 0.2388 - val_accuracy: 0.7107\n",
      "Epoch 244/1000\n",
      "563/563 [==============================] - 1s 974us/step - loss: 0.2386 - accuracy: 0.7002 - val_loss: 0.2385 - val_accuracy: 0.7108\n",
      "Epoch 245/1000\n",
      "563/563 [==============================] - 1s 963us/step - loss: 0.2385 - accuracy: 0.6977 - val_loss: 0.2383 - val_accuracy: 0.7108\n",
      "Epoch 246/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2385 - accuracy: 0.6942 - val_loss: 0.2380 - val_accuracy: 0.7108\n",
      "Epoch 247/1000\n",
      "563/563 [==============================] - 1s 950us/step - loss: 0.2382 - accuracy: 0.6963 - val_loss: 0.2378 - val_accuracy: 0.7108\n",
      "Epoch 248/1000\n",
      "563/563 [==============================] - 1s 979us/step - loss: 0.2377 - accuracy: 0.6982 - val_loss: 0.2375 - val_accuracy: 0.7107\n",
      "Epoch 249/1000\n",
      "563/563 [==============================] - 1s 998us/step - loss: 0.2377 - accuracy: 0.6993 - val_loss: 0.2373 - val_accuracy: 0.7109\n",
      "Epoch 250/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2367 - accuracy: 0.6982 - val_loss: 0.2370 - val_accuracy: 0.7109\n",
      "Epoch 251/1000\n",
      "563/563 [==============================] - 1s 970us/step - loss: 0.2367 - accuracy: 0.6984 - val_loss: 0.2368 - val_accuracy: 0.7111\n",
      "Epoch 252/1000\n",
      "563/563 [==============================] - 1s 940us/step - loss: 0.2367 - accuracy: 0.6979 - val_loss: 0.2365 - val_accuracy: 0.7111\n",
      "Epoch 253/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2365 - accuracy: 0.6975 - val_loss: 0.2363 - val_accuracy: 0.7108\n",
      "Epoch 254/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2357 - accuracy: 0.7010 - val_loss: 0.2360 - val_accuracy: 0.7105\n",
      "Epoch 255/1000\n",
      "563/563 [==============================] - 1s 968us/step - loss: 0.2365 - accuracy: 0.6978 - val_loss: 0.2358 - val_accuracy: 0.7105\n",
      "Epoch 256/1000\n",
      "563/563 [==============================] - 1s 977us/step - loss: 0.2354 - accuracy: 0.7005 - val_loss: 0.2355 - val_accuracy: 0.7105\n",
      "Epoch 257/1000\n",
      "563/563 [==============================] - 1s 991us/step - loss: 0.2355 - accuracy: 0.6962 - val_loss: 0.2353 - val_accuracy: 0.7107\n",
      "Epoch 258/1000\n",
      "563/563 [==============================] - 1s 982us/step - loss: 0.2344 - accuracy: 0.7068 - val_loss: 0.2350 - val_accuracy: 0.7107\n",
      "Epoch 259/1000\n",
      "563/563 [==============================] - 1s 947us/step - loss: 0.2342 - accuracy: 0.7024 - val_loss: 0.2348 - val_accuracy: 0.7107\n",
      "Epoch 260/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2348 - accuracy: 0.6930 - val_loss: 0.2345 - val_accuracy: 0.7107\n",
      "Epoch 261/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2345 - accuracy: 0.6981 - val_loss: 0.2343 - val_accuracy: 0.7115\n",
      "Epoch 262/1000\n",
      "563/563 [==============================] - 1s 959us/step - loss: 0.2342 - accuracy: 0.6985 - val_loss: 0.2341 - val_accuracy: 0.7115\n",
      "Epoch 263/1000\n",
      "563/563 [==============================] - 1s 974us/step - loss: 0.2340 - accuracy: 0.6998 - val_loss: 0.2338 - val_accuracy: 0.7122\n",
      "Epoch 264/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2334 - accuracy: 0.7031 - val_loss: 0.2336 - val_accuracy: 0.7122\n",
      "Epoch 265/1000\n",
      "563/563 [==============================] - 1s 974us/step - loss: 0.2336 - accuracy: 0.7016 - val_loss: 0.2334 - val_accuracy: 0.7122\n",
      "Epoch 266/1000\n",
      "563/563 [==============================] - 1s 954us/step - loss: 0.2327 - accuracy: 0.7052 - val_loss: 0.2332 - val_accuracy: 0.7122\n",
      "Epoch 267/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2328 - accuracy: 0.7009 - val_loss: 0.2329 - val_accuracy: 0.7121\n",
      "Epoch 268/1000\n",
      "563/563 [==============================] - 1s 958us/step - loss: 0.2327 - accuracy: 0.6994 - val_loss: 0.2327 - val_accuracy: 0.7120\n",
      "Epoch 269/1000\n",
      "563/563 [==============================] - 1s 947us/step - loss: 0.2325 - accuracy: 0.7035 - val_loss: 0.2325 - val_accuracy: 0.7120\n",
      "Epoch 270/1000\n",
      "563/563 [==============================] - 1s 970us/step - loss: 0.2328 - accuracy: 0.6927 - val_loss: 0.2323 - val_accuracy: 0.7116\n",
      "Epoch 271/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2324 - accuracy: 0.6975 - val_loss: 0.2321 - val_accuracy: 0.7116\n",
      "Epoch 272/1000\n",
      "563/563 [==============================] - 1s 958us/step - loss: 0.2325 - accuracy: 0.6956 - val_loss: 0.2319 - val_accuracy: 0.7107\n",
      "Epoch 273/1000\n",
      "563/563 [==============================] - 1s 963us/step - loss: 0.2317 - accuracy: 0.7012 - val_loss: 0.2317 - val_accuracy: 0.7107\n",
      "Epoch 274/1000\n",
      "563/563 [==============================] - 1s 986us/step - loss: 0.2318 - accuracy: 0.6962 - val_loss: 0.2315 - val_accuracy: 0.7100\n",
      "Epoch 275/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2313 - accuracy: 0.6983 - val_loss: 0.2313 - val_accuracy: 0.7100\n",
      "Epoch 276/1000\n",
      "563/563 [==============================] - 1s 990us/step - loss: 0.2314 - accuracy: 0.7032 - val_loss: 0.2311 - val_accuracy: 0.7099\n",
      "Epoch 277/1000\n",
      "563/563 [==============================] - 1s 961us/step - loss: 0.2312 - accuracy: 0.7021 - val_loss: 0.2309 - val_accuracy: 0.7100\n",
      "Epoch 278/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2305 - accuracy: 0.6993 - val_loss: 0.2307 - val_accuracy: 0.7100\n",
      "Epoch 279/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2311 - accuracy: 0.7013 - val_loss: 0.2305 - val_accuracy: 0.7102\n",
      "Epoch 280/1000\n",
      "563/563 [==============================] - 1s 984us/step - loss: 0.2312 - accuracy: 0.6931 - val_loss: 0.2303 - val_accuracy: 0.7102\n",
      "Epoch 281/1000\n",
      "563/563 [==============================] - 1s 949us/step - loss: 0.2296 - accuracy: 0.7054 - val_loss: 0.2301 - val_accuracy: 0.7113\n",
      "Epoch 282/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2304 - accuracy: 0.6996 - val_loss: 0.2299 - val_accuracy: 0.7115\n",
      "Epoch 283/1000\n",
      "563/563 [==============================] - 1s 968us/step - loss: 0.2301 - accuracy: 0.6989 - val_loss: 0.2298 - val_accuracy: 0.7117\n",
      "Epoch 284/1000\n",
      "563/563 [==============================] - 1s 970us/step - loss: 0.2300 - accuracy: 0.7033 - val_loss: 0.2296 - val_accuracy: 0.7117\n",
      "Epoch 285/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2306 - accuracy: 0.6952 - val_loss: 0.2294 - val_accuracy: 0.7117\n",
      "Epoch 286/1000\n",
      "563/563 [==============================] - 1s 990us/step - loss: 0.2282 - accuracy: 0.7043 - val_loss: 0.2292 - val_accuracy: 0.7117\n",
      "Epoch 287/1000\n",
      "563/563 [==============================] - 1s 964us/step - loss: 0.2296 - accuracy: 0.7000 - val_loss: 0.2290 - val_accuracy: 0.7122\n",
      "Epoch 288/1000\n",
      "563/563 [==============================] - 1s 981us/step - loss: 0.2280 - accuracy: 0.7059 - val_loss: 0.2289 - val_accuracy: 0.7122\n",
      "Epoch 289/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2287 - accuracy: 0.7064 - val_loss: 0.2287 - val_accuracy: 0.7125\n",
      "Epoch 290/1000\n",
      "563/563 [==============================] - 1s 961us/step - loss: 0.2281 - accuracy: 0.7075 - val_loss: 0.2285 - val_accuracy: 0.7122\n",
      "Epoch 291/1000\n",
      "563/563 [==============================] - 1s 950us/step - loss: 0.2279 - accuracy: 0.7045 - val_loss: 0.2284 - val_accuracy: 0.7122\n",
      "Epoch 292/1000\n",
      "563/563 [==============================] - 1s 990us/step - loss: 0.2282 - accuracy: 0.7068 - val_loss: 0.2282 - val_accuracy: 0.7121\n",
      "Epoch 293/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2272 - accuracy: 0.7067 - val_loss: 0.2281 - val_accuracy: 0.7121\n",
      "Epoch 294/1000\n",
      "563/563 [==============================] - 1s 963us/step - loss: 0.2273 - accuracy: 0.7073 - val_loss: 0.2279 - val_accuracy: 0.7118\n",
      "Epoch 295/1000\n",
      "563/563 [==============================] - 1s 997us/step - loss: 0.2282 - accuracy: 0.7033 - val_loss: 0.2278 - val_accuracy: 0.7118\n",
      "Epoch 296/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2260 - accuracy: 0.7052 - val_loss: 0.2276 - val_accuracy: 0.7120\n",
      "Epoch 297/1000\n",
      "563/563 [==============================] - 1s 979us/step - loss: 0.2277 - accuracy: 0.7059 - val_loss: 0.2275 - val_accuracy: 0.7120\n",
      "Epoch 298/1000\n",
      "563/563 [==============================] - 1s 974us/step - loss: 0.2269 - accuracy: 0.7099 - val_loss: 0.2273 - val_accuracy: 0.7120\n",
      "Epoch 299/1000\n",
      "563/563 [==============================] - 1s 961us/step - loss: 0.2270 - accuracy: 0.7031 - val_loss: 0.2272 - val_accuracy: 0.7120\n",
      "Epoch 300/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2262 - accuracy: 0.7018 - val_loss: 0.2270 - val_accuracy: 0.7120\n",
      "Epoch 301/1000\n",
      "563/563 [==============================] - 1s 998us/step - loss: 0.2271 - accuracy: 0.7001 - val_loss: 0.2269 - val_accuracy: 0.7120\n",
      "Epoch 302/1000\n",
      "563/563 [==============================] - 1s 940us/step - loss: 0.2269 - accuracy: 0.7016 - val_loss: 0.2268 - val_accuracy: 0.7120\n",
      "Epoch 303/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2255 - accuracy: 0.7072 - val_loss: 0.2266 - val_accuracy: 0.7122\n",
      "Epoch 304/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2264 - accuracy: 0.7025 - val_loss: 0.2265 - val_accuracy: 0.7118\n",
      "Epoch 305/1000\n",
      "563/563 [==============================] - 1s 979us/step - loss: 0.2268 - accuracy: 0.7009 - val_loss: 0.2264 - val_accuracy: 0.7121\n",
      "Epoch 306/1000\n",
      "563/563 [==============================] - 1s 958us/step - loss: 0.2266 - accuracy: 0.7046 - val_loss: 0.2262 - val_accuracy: 0.7121\n",
      "Epoch 307/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2268 - accuracy: 0.7027 - val_loss: 0.2261 - val_accuracy: 0.7121\n",
      "Epoch 308/1000\n",
      "563/563 [==============================] - 1s 961us/step - loss: 0.2264 - accuracy: 0.6952 - val_loss: 0.2260 - val_accuracy: 0.7120\n",
      "Epoch 309/1000\n",
      "563/563 [==============================] - 1s 972us/step - loss: 0.2260 - accuracy: 0.7035 - val_loss: 0.2258 - val_accuracy: 0.7120\n",
      "Epoch 310/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2252 - accuracy: 0.7096 - val_loss: 0.2257 - val_accuracy: 0.7120\n",
      "Epoch 311/1000\n",
      "563/563 [==============================] - 1s 986us/step - loss: 0.2264 - accuracy: 0.6978 - val_loss: 0.2256 - val_accuracy: 0.7120\n",
      "Epoch 312/1000\n",
      "563/563 [==============================] - 1s 959us/step - loss: 0.2264 - accuracy: 0.7039 - val_loss: 0.2255 - val_accuracy: 0.7120\n",
      "Epoch 313/1000\n",
      "563/563 [==============================] - 1s 990us/step - loss: 0.2243 - accuracy: 0.7076 - val_loss: 0.2254 - val_accuracy: 0.7120\n",
      "Epoch 314/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2235 - accuracy: 0.7054 - val_loss: 0.2252 - val_accuracy: 0.7117\n",
      "Epoch 315/1000\n",
      "563/563 [==============================] - 1s 943us/step - loss: 0.2251 - accuracy: 0.7026 - val_loss: 0.2251 - val_accuracy: 0.7115\n",
      "Epoch 316/1000\n",
      "563/563 [==============================] - 1s 972us/step - loss: 0.2259 - accuracy: 0.7036 - val_loss: 0.2250 - val_accuracy: 0.7115\n",
      "Epoch 317/1000\n",
      "563/563 [==============================] - 1s 998us/step - loss: 0.2233 - accuracy: 0.7109 - val_loss: 0.2249 - val_accuracy: 0.7115\n",
      "Epoch 318/1000\n",
      "563/563 [==============================] - 1s 984us/step - loss: 0.2247 - accuracy: 0.7058 - val_loss: 0.2248 - val_accuracy: 0.7118\n",
      "Epoch 319/1000\n",
      "563/563 [==============================] - 1s 950us/step - loss: 0.2245 - accuracy: 0.7037 - val_loss: 0.2247 - val_accuracy: 0.7117\n",
      "Epoch 320/1000\n",
      "563/563 [==============================] - 1s 993us/step - loss: 0.2239 - accuracy: 0.7058 - val_loss: 0.2245 - val_accuracy: 0.7118\n",
      "Epoch 321/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2249 - accuracy: 0.7009 - val_loss: 0.2244 - val_accuracy: 0.7124\n",
      "Epoch 322/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2243 - accuracy: 0.7044 - val_loss: 0.2243 - val_accuracy: 0.7125\n",
      "Epoch 323/1000\n",
      "563/563 [==============================] - 1s 959us/step - loss: 0.2239 - accuracy: 0.7075 - val_loss: 0.2242 - val_accuracy: 0.7125\n",
      "Epoch 324/1000\n",
      "563/563 [==============================] - 1s 961us/step - loss: 0.2240 - accuracy: 0.6997 - val_loss: 0.2241 - val_accuracy: 0.7124\n",
      "Epoch 325/1000\n",
      "563/563 [==============================] - 1s 995us/step - loss: 0.2229 - accuracy: 0.7063 - val_loss: 0.2240 - val_accuracy: 0.7124\n",
      "Epoch 326/1000\n",
      "563/563 [==============================] - 1s 988us/step - loss: 0.2234 - accuracy: 0.7058 - val_loss: 0.2239 - val_accuracy: 0.7124\n",
      "Epoch 327/1000\n",
      "563/563 [==============================] - 1s 935us/step - loss: 0.2225 - accuracy: 0.7073 - val_loss: 0.2238 - val_accuracy: 0.7124\n",
      "Epoch 328/1000\n",
      "563/563 [==============================] - 1s 981us/step - loss: 0.2230 - accuracy: 0.7103 - val_loss: 0.2237 - val_accuracy: 0.7122\n",
      "Epoch 329/1000\n",
      "563/563 [==============================] - 1s 991us/step - loss: 0.2243 - accuracy: 0.7020 - val_loss: 0.2236 - val_accuracy: 0.7124\n",
      "Epoch 330/1000\n",
      "563/563 [==============================] - 1s 947us/step - loss: 0.2234 - accuracy: 0.7037 - val_loss: 0.2235 - val_accuracy: 0.7125\n",
      "Epoch 331/1000\n",
      "563/563 [==============================] - 1s 947us/step - loss: 0.2237 - accuracy: 0.7000 - val_loss: 0.2234 - val_accuracy: 0.7117\n",
      "Epoch 332/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2225 - accuracy: 0.7068 - val_loss: 0.2233 - val_accuracy: 0.7117\n",
      "Epoch 333/1000\n",
      "563/563 [==============================] - 1s 958us/step - loss: 0.2230 - accuracy: 0.7037 - val_loss: 0.2232 - val_accuracy: 0.7117\n",
      "Epoch 334/1000\n",
      "563/563 [==============================] - 1s 952us/step - loss: 0.2229 - accuracy: 0.7055 - val_loss: 0.2231 - val_accuracy: 0.7116\n",
      "Epoch 335/1000\n",
      "563/563 [==============================] - 1s 997us/step - loss: 0.2232 - accuracy: 0.7029 - val_loss: 0.2230 - val_accuracy: 0.7116\n",
      "Epoch 336/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2224 - accuracy: 0.7049 - val_loss: 0.2229 - val_accuracy: 0.7116\n",
      "Epoch 337/1000\n",
      "563/563 [==============================] - 1s 945us/step - loss: 0.2228 - accuracy: 0.7077 - val_loss: 0.2228 - val_accuracy: 0.7116\n",
      "Epoch 338/1000\n",
      "563/563 [==============================] - 1s 953us/step - loss: 0.2234 - accuracy: 0.7040 - val_loss: 0.2227 - val_accuracy: 0.7115\n",
      "Epoch 339/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2216 - accuracy: 0.7067 - val_loss: 0.2226 - val_accuracy: 0.7115\n",
      "Epoch 340/1000\n",
      "563/563 [==============================] - 1s 972us/step - loss: 0.2223 - accuracy: 0.7040 - val_loss: 0.2225 - val_accuracy: 0.7116\n",
      "Epoch 341/1000\n",
      "563/563 [==============================] - 1s 959us/step - loss: 0.2227 - accuracy: 0.6999 - val_loss: 0.2224 - val_accuracy: 0.7124\n",
      "Epoch 342/1000\n",
      "563/563 [==============================] - 1s 945us/step - loss: 0.2222 - accuracy: 0.7090 - val_loss: 0.2223 - val_accuracy: 0.7124\n",
      "Epoch 343/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2235 - accuracy: 0.7067 - val_loss: 0.2222 - val_accuracy: 0.7124\n",
      "Epoch 344/1000\n",
      "563/563 [==============================] - 1s 945us/step - loss: 0.2225 - accuracy: 0.7023 - val_loss: 0.2222 - val_accuracy: 0.7124\n",
      "Epoch 345/1000\n",
      "563/563 [==============================] - 1s 949us/step - loss: 0.2218 - accuracy: 0.7093 - val_loss: 0.2221 - val_accuracy: 0.7130\n",
      "Epoch 346/1000\n",
      "563/563 [==============================] - 1s 966us/step - loss: 0.2216 - accuracy: 0.7050 - val_loss: 0.2220 - val_accuracy: 0.7130\n",
      "Epoch 347/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2222 - accuracy: 0.7031 - val_loss: 0.2219 - val_accuracy: 0.7134\n",
      "Epoch 348/1000\n",
      "563/563 [==============================] - 1s 965us/step - loss: 0.2223 - accuracy: 0.7028 - val_loss: 0.2218 - val_accuracy: 0.7133\n",
      "Epoch 349/1000\n",
      "563/563 [==============================] - 1s 956us/step - loss: 0.2206 - accuracy: 0.7072 - val_loss: 0.2217 - val_accuracy: 0.7133\n",
      "Epoch 350/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2230 - accuracy: 0.7018 - val_loss: 0.2216 - val_accuracy: 0.7133\n",
      "Epoch 351/1000\n",
      "563/563 [==============================] - 1s 990us/step - loss: 0.2204 - accuracy: 0.7101 - val_loss: 0.2216 - val_accuracy: 0.7135\n",
      "Epoch 352/1000\n",
      "563/563 [==============================] - 1s 947us/step - loss: 0.2214 - accuracy: 0.7078 - val_loss: 0.2215 - val_accuracy: 0.7135\n",
      "Epoch 353/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2215 - accuracy: 0.7019 - val_loss: 0.2214 - val_accuracy: 0.7139\n",
      "Epoch 354/1000\n",
      "563/563 [==============================] - 1s 997us/step - loss: 0.2201 - accuracy: 0.7066 - val_loss: 0.2213 - val_accuracy: 0.7138\n",
      "Epoch 355/1000\n",
      "563/563 [==============================] - 1s 982us/step - loss: 0.2212 - accuracy: 0.7058 - val_loss: 0.2212 - val_accuracy: 0.7148\n",
      "Epoch 356/1000\n",
      "563/563 [==============================] - 1s 963us/step - loss: 0.2208 - accuracy: 0.7114 - val_loss: 0.2211 - val_accuracy: 0.7148\n",
      "Epoch 357/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2210 - accuracy: 0.7064 - val_loss: 0.2211 - val_accuracy: 0.7148\n",
      "Epoch 358/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2204 - accuracy: 0.7046 - val_loss: 0.2210 - val_accuracy: 0.7148\n",
      "Epoch 359/1000\n",
      "563/563 [==============================] - 1s 995us/step - loss: 0.2209 - accuracy: 0.7054 - val_loss: 0.2209 - val_accuracy: 0.7148\n",
      "Epoch 360/1000\n",
      "563/563 [==============================] - 1s 972us/step - loss: 0.2200 - accuracy: 0.7044 - val_loss: 0.2208 - val_accuracy: 0.7150\n",
      "Epoch 361/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2203 - accuracy: 0.7100 - val_loss: 0.2207 - val_accuracy: 0.7151\n",
      "Epoch 362/1000\n",
      "563/563 [==============================] - 1s 952us/step - loss: 0.2216 - accuracy: 0.7023 - val_loss: 0.2207 - val_accuracy: 0.7153\n",
      "Epoch 363/1000\n",
      "563/563 [==============================] - 1s 979us/step - loss: 0.2214 - accuracy: 0.7020 - val_loss: 0.2206 - val_accuracy: 0.7156\n",
      "Epoch 364/1000\n",
      "563/563 [==============================] - 1s 966us/step - loss: 0.2197 - accuracy: 0.7128 - val_loss: 0.2205 - val_accuracy: 0.7156\n",
      "Epoch 365/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2200 - accuracy: 0.7043 - val_loss: 0.2204 - val_accuracy: 0.7156\n",
      "Epoch 366/1000\n",
      "563/563 [==============================] - 1s 965us/step - loss: 0.2198 - accuracy: 0.7095 - val_loss: 0.2203 - val_accuracy: 0.7155\n",
      "Epoch 367/1000\n",
      "563/563 [==============================] - 1s 970us/step - loss: 0.2203 - accuracy: 0.7093 - val_loss: 0.2203 - val_accuracy: 0.7155\n",
      "Epoch 368/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2205 - accuracy: 0.7050 - val_loss: 0.2202 - val_accuracy: 0.7155\n",
      "Epoch 369/1000\n",
      "563/563 [==============================] - 1s 974us/step - loss: 0.2206 - accuracy: 0.7092 - val_loss: 0.2201 - val_accuracy: 0.7155\n",
      "Epoch 370/1000\n",
      "563/563 [==============================] - 1s 940us/step - loss: 0.2209 - accuracy: 0.7071 - val_loss: 0.2200 - val_accuracy: 0.7155\n",
      "Epoch 371/1000\n",
      "563/563 [==============================] - 1s 936us/step - loss: 0.2205 - accuracy: 0.7038 - val_loss: 0.2200 - val_accuracy: 0.7160\n",
      "Epoch 372/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2180 - accuracy: 0.7157 - val_loss: 0.2199 - val_accuracy: 0.7160\n",
      "Epoch 373/1000\n",
      "563/563 [==============================] - 1s 954us/step - loss: 0.2192 - accuracy: 0.7126 - val_loss: 0.2198 - val_accuracy: 0.7166\n",
      "Epoch 374/1000\n",
      "563/563 [==============================] - 1s 950us/step - loss: 0.2196 - accuracy: 0.7093 - val_loss: 0.2197 - val_accuracy: 0.7165\n",
      "Epoch 375/1000\n",
      "563/563 [==============================] - 1s 975us/step - loss: 0.2207 - accuracy: 0.7042 - val_loss: 0.2197 - val_accuracy: 0.7165\n",
      "Epoch 376/1000\n",
      "563/563 [==============================] - 1s 993us/step - loss: 0.2205 - accuracy: 0.7062 - val_loss: 0.2196 - val_accuracy: 0.7182\n",
      "Epoch 377/1000\n",
      "563/563 [==============================] - 1s 943us/step - loss: 0.2194 - accuracy: 0.7107 - val_loss: 0.2195 - val_accuracy: 0.7185\n",
      "Epoch 378/1000\n",
      "563/563 [==============================] - 1s 970us/step - loss: 0.2180 - accuracy: 0.7069 - val_loss: 0.2194 - val_accuracy: 0.7187\n",
      "Epoch 379/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2207 - accuracy: 0.7048 - val_loss: 0.2194 - val_accuracy: 0.7203\n",
      "Epoch 380/1000\n",
      "563/563 [==============================] - 1s 992us/step - loss: 0.2197 - accuracy: 0.7096 - val_loss: 0.2193 - val_accuracy: 0.7203\n",
      "Epoch 381/1000\n",
      "563/563 [==============================] - 1s 938us/step - loss: 0.2187 - accuracy: 0.7141 - val_loss: 0.2192 - val_accuracy: 0.7205\n",
      "Epoch 382/1000\n",
      "563/563 [==============================] - 1s 970us/step - loss: 0.2196 - accuracy: 0.7075 - val_loss: 0.2191 - val_accuracy: 0.7207\n",
      "Epoch 383/1000\n",
      "563/563 [==============================] - 1s 995us/step - loss: 0.2193 - accuracy: 0.7096 - val_loss: 0.2191 - val_accuracy: 0.7208\n",
      "Epoch 384/1000\n",
      "563/563 [==============================] - 1s 961us/step - loss: 0.2192 - accuracy: 0.7109 - val_loss: 0.2190 - val_accuracy: 0.7209\n",
      "Epoch 385/1000\n",
      "563/563 [==============================] - 1s 948us/step - loss: 0.2186 - accuracy: 0.7118 - val_loss: 0.2189 - val_accuracy: 0.7209\n",
      "Epoch 386/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2194 - accuracy: 0.7102 - val_loss: 0.2188 - val_accuracy: 0.7209\n",
      "Epoch 387/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2182 - accuracy: 0.7142 - val_loss: 0.2188 - val_accuracy: 0.7209\n",
      "Epoch 388/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2200 - accuracy: 0.7102 - val_loss: 0.2187 - val_accuracy: 0.7208\n",
      "Epoch 389/1000\n",
      "563/563 [==============================] - 1s 955us/step - loss: 0.2192 - accuracy: 0.7056 - val_loss: 0.2186 - val_accuracy: 0.7209\n",
      "Epoch 390/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2199 - accuracy: 0.7084 - val_loss: 0.2185 - val_accuracy: 0.7209\n",
      "Epoch 391/1000\n",
      "563/563 [==============================] - 1s 945us/step - loss: 0.2193 - accuracy: 0.7114 - val_loss: 0.2185 - val_accuracy: 0.7212\n",
      "Epoch 392/1000\n",
      "563/563 [==============================] - 1s 968us/step - loss: 0.2176 - accuracy: 0.7113 - val_loss: 0.2184 - val_accuracy: 0.7212\n",
      "Epoch 393/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2188 - accuracy: 0.7126 - val_loss: 0.2183 - val_accuracy: 0.7219\n",
      "Epoch 394/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2180 - accuracy: 0.7117 - val_loss: 0.2183 - val_accuracy: 0.7218\n",
      "Epoch 395/1000\n",
      "563/563 [==============================] - 1s 966us/step - loss: 0.2193 - accuracy: 0.7070 - val_loss: 0.2182 - val_accuracy: 0.7218\n",
      "Epoch 396/1000\n",
      "563/563 [==============================] - 1s 945us/step - loss: 0.2179 - accuracy: 0.7136 - val_loss: 0.2181 - val_accuracy: 0.7218\n",
      "Epoch 397/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2193 - accuracy: 0.7062 - val_loss: 0.2181 - val_accuracy: 0.7218\n",
      "Epoch 398/1000\n",
      "563/563 [==============================] - 1s 961us/step - loss: 0.2191 - accuracy: 0.7032 - val_loss: 0.2180 - val_accuracy: 0.7217\n",
      "Epoch 399/1000\n",
      "563/563 [==============================] - 1s 961us/step - loss: 0.2188 - accuracy: 0.7134 - val_loss: 0.2179 - val_accuracy: 0.7219\n",
      "Epoch 400/1000\n",
      "563/563 [==============================] - 1s 942us/step - loss: 0.2177 - accuracy: 0.7077 - val_loss: 0.2178 - val_accuracy: 0.7221\n",
      "Epoch 401/1000\n",
      "563/563 [==============================] - 1s 993us/step - loss: 0.2168 - accuracy: 0.7179 - val_loss: 0.2178 - val_accuracy: 0.7221\n",
      "Epoch 402/1000\n",
      "563/563 [==============================] - 1s 982us/step - loss: 0.2177 - accuracy: 0.7109 - val_loss: 0.2177 - val_accuracy: 0.7222\n",
      "Epoch 403/1000\n",
      "563/563 [==============================] - 1s 988us/step - loss: 0.2173 - accuracy: 0.7111 - val_loss: 0.2176 - val_accuracy: 0.7226\n",
      "Epoch 404/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2163 - accuracy: 0.7202 - val_loss: 0.2176 - val_accuracy: 0.7226\n",
      "Epoch 405/1000\n",
      "563/563 [==============================] - 1s 998us/step - loss: 0.2188 - accuracy: 0.7106 - val_loss: 0.2175 - val_accuracy: 0.7225\n",
      "Epoch 406/1000\n",
      "563/563 [==============================] - 1s 961us/step - loss: 0.2185 - accuracy: 0.7107 - val_loss: 0.2174 - val_accuracy: 0.7225\n",
      "Epoch 407/1000\n",
      "563/563 [==============================] - 1s 949us/step - loss: 0.2171 - accuracy: 0.7156 - val_loss: 0.2174 - val_accuracy: 0.7225\n",
      "Epoch 408/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2168 - accuracy: 0.7097 - val_loss: 0.2173 - val_accuracy: 0.7225\n",
      "Epoch 409/1000\n",
      "563/563 [==============================] - 1s 934us/step - loss: 0.2182 - accuracy: 0.7095 - val_loss: 0.2172 - val_accuracy: 0.7225\n",
      "Epoch 410/1000\n",
      "563/563 [==============================] - 1s 955us/step - loss: 0.2166 - accuracy: 0.7122 - val_loss: 0.2172 - val_accuracy: 0.7229\n",
      "Epoch 411/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2183 - accuracy: 0.7086 - val_loss: 0.2171 - val_accuracy: 0.7229\n",
      "Epoch 412/1000\n",
      "563/563 [==============================] - 1s 998us/step - loss: 0.2171 - accuracy: 0.7126 - val_loss: 0.2171 - val_accuracy: 0.7229\n",
      "Epoch 413/1000\n",
      "563/563 [==============================] - 1s 961us/step - loss: 0.2174 - accuracy: 0.7101 - val_loss: 0.2170 - val_accuracy: 0.7229\n",
      "Epoch 414/1000\n",
      "563/563 [==============================] - 1s 936us/step - loss: 0.2156 - accuracy: 0.7186 - val_loss: 0.2169 - val_accuracy: 0.7229\n",
      "Epoch 415/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2168 - accuracy: 0.7106 - val_loss: 0.2169 - val_accuracy: 0.7229\n",
      "Epoch 416/1000\n",
      "563/563 [==============================] - 1s 981us/step - loss: 0.2168 - accuracy: 0.7154 - val_loss: 0.2168 - val_accuracy: 0.7229\n",
      "Epoch 417/1000\n",
      "563/563 [==============================] - 1s 988us/step - loss: 0.2187 - accuracy: 0.7058 - val_loss: 0.2167 - val_accuracy: 0.7230\n",
      "Epoch 418/1000\n",
      "563/563 [==============================] - 1s 950us/step - loss: 0.2163 - accuracy: 0.7115 - val_loss: 0.2167 - val_accuracy: 0.7229\n",
      "Epoch 419/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2159 - accuracy: 0.7142 - val_loss: 0.2166 - val_accuracy: 0.7230\n",
      "Epoch 420/1000\n",
      "563/563 [==============================] - 1s 975us/step - loss: 0.2193 - accuracy: 0.7048 - val_loss: 0.2165 - val_accuracy: 0.7230\n",
      "Epoch 421/1000\n",
      "563/563 [==============================] - 1s 943us/step - loss: 0.2170 - accuracy: 0.7113 - val_loss: 0.2165 - val_accuracy: 0.7230\n",
      "Epoch 422/1000\n",
      "563/563 [==============================] - 1s 995us/step - loss: 0.2163 - accuracy: 0.7062 - val_loss: 0.2164 - val_accuracy: 0.7229\n",
      "Epoch 423/1000\n",
      "563/563 [==============================] - 1s 979us/step - loss: 0.2170 - accuracy: 0.7057 - val_loss: 0.2164 - val_accuracy: 0.7227\n",
      "Epoch 424/1000\n",
      "563/563 [==============================] - 1s 988us/step - loss: 0.2175 - accuracy: 0.7077 - val_loss: 0.2163 - val_accuracy: 0.7226\n",
      "Epoch 425/1000\n",
      "563/563 [==============================] - 1s 973us/step - loss: 0.2196 - accuracy: 0.7021 - val_loss: 0.2162 - val_accuracy: 0.7226\n",
      "Epoch 426/1000\n",
      "563/563 [==============================] - 1s 990us/step - loss: 0.2167 - accuracy: 0.7135 - val_loss: 0.2162 - val_accuracy: 0.7231\n",
      "Epoch 427/1000\n",
      "563/563 [==============================] - 1s 982us/step - loss: 0.2156 - accuracy: 0.7146 - val_loss: 0.2161 - val_accuracy: 0.7231\n",
      "Epoch 428/1000\n",
      "563/563 [==============================] - 1s 966us/step - loss: 0.2161 - accuracy: 0.7170 - val_loss: 0.2161 - val_accuracy: 0.7232\n",
      "Epoch 429/1000\n",
      "563/563 [==============================] - 1s 998us/step - loss: 0.2164 - accuracy: 0.7111 - val_loss: 0.2160 - val_accuracy: 0.7232\n",
      "Epoch 430/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2159 - accuracy: 0.7138 - val_loss: 0.2160 - val_accuracy: 0.7234\n",
      "Epoch 431/1000\n",
      "563/563 [==============================] - 1s 977us/step - loss: 0.2161 - accuracy: 0.7131 - val_loss: 0.2159 - val_accuracy: 0.7234\n",
      "Epoch 432/1000\n",
      "563/563 [==============================] - 1s 965us/step - loss: 0.2170 - accuracy: 0.7129 - val_loss: 0.2158 - val_accuracy: 0.7234\n",
      "Epoch 433/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2165 - accuracy: 0.7117 - val_loss: 0.2158 - val_accuracy: 0.7234\n",
      "Epoch 434/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2161 - accuracy: 0.7126 - val_loss: 0.2157 - val_accuracy: 0.7234\n",
      "Epoch 435/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2151 - accuracy: 0.7167 - val_loss: 0.2157 - val_accuracy: 0.7235\n",
      "Epoch 436/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2158 - accuracy: 0.7118 - val_loss: 0.2156 - val_accuracy: 0.7235\n",
      "Epoch 437/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2166 - accuracy: 0.7057 - val_loss: 0.2155 - val_accuracy: 0.7235\n",
      "Epoch 438/1000\n",
      "563/563 [==============================] - 1s 961us/step - loss: 0.2161 - accuracy: 0.7106 - val_loss: 0.2155 - val_accuracy: 0.7235\n",
      "Epoch 439/1000\n",
      "563/563 [==============================] - 1s 950us/step - loss: 0.2152 - accuracy: 0.7117 - val_loss: 0.2154 - val_accuracy: 0.7235\n",
      "Epoch 440/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2165 - accuracy: 0.7074 - val_loss: 0.2154 - val_accuracy: 0.7235\n",
      "Epoch 441/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2144 - accuracy: 0.7194 - val_loss: 0.2153 - val_accuracy: 0.7236\n",
      "Epoch 442/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2157 - accuracy: 0.7129 - val_loss: 0.2153 - val_accuracy: 0.7239\n",
      "Epoch 443/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2141 - accuracy: 0.7179 - val_loss: 0.2152 - val_accuracy: 0.7242\n",
      "Epoch 444/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2146 - accuracy: 0.7178 - val_loss: 0.2151 - val_accuracy: 0.7242\n",
      "Epoch 445/1000\n",
      "563/563 [==============================] - 1s 963us/step - loss: 0.2160 - accuracy: 0.7115 - val_loss: 0.2151 - val_accuracy: 0.7249\n",
      "Epoch 446/1000\n",
      "563/563 [==============================] - 1s 954us/step - loss: 0.2146 - accuracy: 0.7168 - val_loss: 0.2150 - val_accuracy: 0.7249\n",
      "Epoch 447/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2154 - accuracy: 0.7130 - val_loss: 0.2150 - val_accuracy: 0.7249\n",
      "Epoch 448/1000\n",
      "563/563 [==============================] - 1s 943us/step - loss: 0.2146 - accuracy: 0.7133 - val_loss: 0.2149 - val_accuracy: 0.7249\n",
      "Epoch 449/1000\n",
      "563/563 [==============================] - 1s 958us/step - loss: 0.2139 - accuracy: 0.7189 - val_loss: 0.2149 - val_accuracy: 0.7249\n",
      "Epoch 450/1000\n",
      "563/563 [==============================] - 1s 974us/step - loss: 0.2139 - accuracy: 0.7168 - val_loss: 0.2148 - val_accuracy: 0.7249\n",
      "Epoch 451/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2153 - accuracy: 0.7118 - val_loss: 0.2148 - val_accuracy: 0.7249\n",
      "Epoch 452/1000\n",
      "563/563 [==============================] - 1s 974us/step - loss: 0.2166 - accuracy: 0.7115 - val_loss: 0.2147 - val_accuracy: 0.7256\n",
      "Epoch 453/1000\n",
      "563/563 [==============================] - 1s 965us/step - loss: 0.2140 - accuracy: 0.7173 - val_loss: 0.2146 - val_accuracy: 0.7260\n",
      "Epoch 454/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2139 - accuracy: 0.7152 - val_loss: 0.2146 - val_accuracy: 0.7260\n",
      "Epoch 455/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2145 - accuracy: 0.7141 - val_loss: 0.2145 - val_accuracy: 0.7260\n",
      "Epoch 456/1000\n",
      "563/563 [==============================] - 1s 970us/step - loss: 0.2142 - accuracy: 0.7150 - val_loss: 0.2145 - val_accuracy: 0.7260\n",
      "Epoch 457/1000\n",
      "563/563 [==============================] - 1s 965us/step - loss: 0.2145 - accuracy: 0.7165 - val_loss: 0.2144 - val_accuracy: 0.7260\n",
      "Epoch 458/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2161 - accuracy: 0.7049 - val_loss: 0.2144 - val_accuracy: 0.7260\n",
      "Epoch 459/1000\n",
      "563/563 [==============================] - 1s 977us/step - loss: 0.2136 - accuracy: 0.7190 - val_loss: 0.2143 - val_accuracy: 0.7260\n",
      "Epoch 460/1000\n",
      "563/563 [==============================] - 1s 952us/step - loss: 0.2133 - accuracy: 0.7176 - val_loss: 0.2143 - val_accuracy: 0.7260\n",
      "Epoch 461/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2145 - accuracy: 0.7179 - val_loss: 0.2142 - val_accuracy: 0.7260\n",
      "Epoch 462/1000\n",
      "563/563 [==============================] - 1s 982us/step - loss: 0.2163 - accuracy: 0.7104 - val_loss: 0.2142 - val_accuracy: 0.7260\n",
      "Epoch 463/1000\n",
      "563/563 [==============================] - 1s 979us/step - loss: 0.2150 - accuracy: 0.7141 - val_loss: 0.2141 - val_accuracy: 0.7260\n",
      "Epoch 464/1000\n",
      "563/563 [==============================] - 1s 979us/step - loss: 0.2139 - accuracy: 0.7170 - val_loss: 0.2141 - val_accuracy: 0.7260\n",
      "Epoch 465/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2133 - accuracy: 0.7133 - val_loss: 0.2140 - val_accuracy: 0.7260\n",
      "Epoch 466/1000\n",
      "563/563 [==============================] - 1s 967us/step - loss: 0.2135 - accuracy: 0.7160 - val_loss: 0.2140 - val_accuracy: 0.7260\n",
      "Epoch 467/1000\n",
      "563/563 [==============================] - 1s 950us/step - loss: 0.2143 - accuracy: 0.7099 - val_loss: 0.2139 - val_accuracy: 0.7260\n",
      "Epoch 468/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2143 - accuracy: 0.7196 - val_loss: 0.2138 - val_accuracy: 0.7260\n",
      "Epoch 469/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2143 - accuracy: 0.7150 - val_loss: 0.2138 - val_accuracy: 0.7260\n",
      "Epoch 470/1000\n",
      "563/563 [==============================] - 1s 994us/step - loss: 0.2140 - accuracy: 0.7163 - val_loss: 0.2137 - val_accuracy: 0.7260\n",
      "Epoch 471/1000\n",
      "563/563 [==============================] - 1s 934us/step - loss: 0.2144 - accuracy: 0.7132 - val_loss: 0.2137 - val_accuracy: 0.7261\n",
      "Epoch 472/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2142 - accuracy: 0.7136 - val_loss: 0.2136 - val_accuracy: 0.7261\n",
      "Epoch 473/1000\n",
      "563/563 [==============================] - 1s 998us/step - loss: 0.2129 - accuracy: 0.7209 - val_loss: 0.2136 - val_accuracy: 0.7261\n",
      "Epoch 474/1000\n",
      "563/563 [==============================] - 1s 963us/step - loss: 0.2138 - accuracy: 0.7125 - val_loss: 0.2135 - val_accuracy: 0.7261\n",
      "Epoch 475/1000\n",
      "563/563 [==============================] - 1s 970us/step - loss: 0.2150 - accuracy: 0.7121 - val_loss: 0.2135 - val_accuracy: 0.7261\n",
      "Epoch 476/1000\n",
      "563/563 [==============================] - 1s 997us/step - loss: 0.2155 - accuracy: 0.7128 - val_loss: 0.2134 - val_accuracy: 0.7261\n",
      "Epoch 477/1000\n",
      "563/563 [==============================] - 1s 966us/step - loss: 0.2151 - accuracy: 0.7096 - val_loss: 0.2134 - val_accuracy: 0.7261\n",
      "Epoch 478/1000\n",
      "563/563 [==============================] - 1s 961us/step - loss: 0.2133 - accuracy: 0.7152 - val_loss: 0.2133 - val_accuracy: 0.7260\n",
      "Epoch 479/1000\n",
      "563/563 [==============================] - 1s 986us/step - loss: 0.2138 - accuracy: 0.7157 - val_loss: 0.2133 - val_accuracy: 0.7260\n",
      "Epoch 480/1000\n",
      "563/563 [==============================] - 1s 968us/step - loss: 0.2140 - accuracy: 0.7184 - val_loss: 0.2132 - val_accuracy: 0.7261\n",
      "Epoch 481/1000\n",
      "563/563 [==============================] - 1s 950us/step - loss: 0.2130 - accuracy: 0.7173 - val_loss: 0.2132 - val_accuracy: 0.7278\n",
      "Epoch 482/1000\n",
      "563/563 [==============================] - 1s 981us/step - loss: 0.2131 - accuracy: 0.7178 - val_loss: 0.2131 - val_accuracy: 0.7278\n",
      "Epoch 483/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2132 - accuracy: 0.7175 - val_loss: 0.2131 - val_accuracy: 0.7279\n",
      "Epoch 484/1000\n",
      "563/563 [==============================] - 1s 945us/step - loss: 0.2116 - accuracy: 0.7243 - val_loss: 0.2130 - val_accuracy: 0.7279\n",
      "Epoch 485/1000\n",
      "563/563 [==============================] - 1s 972us/step - loss: 0.2125 - accuracy: 0.7222 - val_loss: 0.2130 - val_accuracy: 0.7279\n",
      "Epoch 486/1000\n",
      "563/563 [==============================] - 1s 963us/step - loss: 0.2140 - accuracy: 0.7138 - val_loss: 0.2129 - val_accuracy: 0.7279\n",
      "Epoch 487/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2121 - accuracy: 0.7180 - val_loss: 0.2129 - val_accuracy: 0.7279\n",
      "Epoch 488/1000\n",
      "563/563 [==============================] - 1s 984us/step - loss: 0.2134 - accuracy: 0.7170 - val_loss: 0.2128 - val_accuracy: 0.7278\n",
      "Epoch 489/1000\n",
      "563/563 [==============================] - 1s 958us/step - loss: 0.2135 - accuracy: 0.7144 - val_loss: 0.2128 - val_accuracy: 0.7278\n",
      "Epoch 490/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2129 - accuracy: 0.7157 - val_loss: 0.2127 - val_accuracy: 0.7276\n",
      "Epoch 491/1000\n",
      "563/563 [==============================] - 1s 990us/step - loss: 0.2124 - accuracy: 0.7192 - val_loss: 0.2127 - val_accuracy: 0.7276\n",
      "Epoch 492/1000\n",
      "563/563 [==============================] - 1s 956us/step - loss: 0.2134 - accuracy: 0.7185 - val_loss: 0.2126 - val_accuracy: 0.7276\n",
      "Epoch 493/1000\n",
      "563/563 [==============================] - 1s 965us/step - loss: 0.2125 - accuracy: 0.7205 - val_loss: 0.2126 - val_accuracy: 0.7276\n",
      "Epoch 494/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2121 - accuracy: 0.7209 - val_loss: 0.2125 - val_accuracy: 0.7276\n",
      "Epoch 495/1000\n",
      "563/563 [==============================] - 1s 958us/step - loss: 0.2121 - accuracy: 0.7165 - val_loss: 0.2125 - val_accuracy: 0.7278\n",
      "Epoch 496/1000\n",
      "563/563 [==============================] - 1s 945us/step - loss: 0.2123 - accuracy: 0.7193 - val_loss: 0.2124 - val_accuracy: 0.7278\n",
      "Epoch 497/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2107 - accuracy: 0.7219 - val_loss: 0.2124 - val_accuracy: 0.7278\n",
      "Epoch 498/1000\n",
      "563/563 [==============================] - 1s 990us/step - loss: 0.2134 - accuracy: 0.7137 - val_loss: 0.2123 - val_accuracy: 0.7279\n",
      "Epoch 499/1000\n",
      "563/563 [==============================] - 1s 963us/step - loss: 0.2115 - accuracy: 0.7208 - val_loss: 0.2123 - val_accuracy: 0.7279\n",
      "Epoch 500/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2133 - accuracy: 0.7118 - val_loss: 0.2122 - val_accuracy: 0.7279\n",
      "Epoch 501/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2129 - accuracy: 0.7162 - val_loss: 0.2122 - val_accuracy: 0.7279\n",
      "Epoch 502/1000\n",
      "563/563 [==============================] - 1s 995us/step - loss: 0.2129 - accuracy: 0.7167 - val_loss: 0.2121 - val_accuracy: 0.7279\n",
      "Epoch 503/1000\n",
      "563/563 [==============================] - 1s 967us/step - loss: 0.2130 - accuracy: 0.7168 - val_loss: 0.2121 - val_accuracy: 0.7279\n",
      "Epoch 504/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2115 - accuracy: 0.7200 - val_loss: 0.2120 - val_accuracy: 0.7279\n",
      "Epoch 505/1000\n",
      "563/563 [==============================] - 1s 990us/step - loss: 0.2126 - accuracy: 0.7172 - val_loss: 0.2120 - val_accuracy: 0.7279\n",
      "Epoch 506/1000\n",
      "563/563 [==============================] - 1s 948us/step - loss: 0.2134 - accuracy: 0.7187 - val_loss: 0.2120 - val_accuracy: 0.7279\n",
      "Epoch 507/1000\n",
      "563/563 [==============================] - 1s 942us/step - loss: 0.2114 - accuracy: 0.7176 - val_loss: 0.2119 - val_accuracy: 0.7300\n",
      "Epoch 508/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2123 - accuracy: 0.7209 - val_loss: 0.2119 - val_accuracy: 0.7300\n",
      "Epoch 509/1000\n",
      "563/563 [==============================] - 1s 998us/step - loss: 0.2103 - accuracy: 0.7244 - val_loss: 0.2118 - val_accuracy: 0.7300\n",
      "Epoch 510/1000\n",
      "563/563 [==============================] - 1s 950us/step - loss: 0.2116 - accuracy: 0.7175 - val_loss: 0.2118 - val_accuracy: 0.7300\n",
      "Epoch 511/1000\n",
      "563/563 [==============================] - 1s 990us/step - loss: 0.2122 - accuracy: 0.7193 - val_loss: 0.2117 - val_accuracy: 0.7304\n",
      "Epoch 512/1000\n",
      "563/563 [==============================] - 1s 998us/step - loss: 0.2118 - accuracy: 0.7204 - val_loss: 0.2117 - val_accuracy: 0.7304\n",
      "Epoch 513/1000\n",
      "563/563 [==============================] - 1s 981us/step - loss: 0.2120 - accuracy: 0.7198 - val_loss: 0.2116 - val_accuracy: 0.7304\n",
      "Epoch 514/1000\n",
      "563/563 [==============================] - 1s 940us/step - loss: 0.2107 - accuracy: 0.7216 - val_loss: 0.2116 - val_accuracy: 0.7304\n",
      "Epoch 515/1000\n",
      "563/563 [==============================] - 1s 986us/step - loss: 0.2112 - accuracy: 0.7195 - val_loss: 0.2115 - val_accuracy: 0.7305\n",
      "Epoch 516/1000\n",
      "563/563 [==============================] - 1s 966us/step - loss: 0.2121 - accuracy: 0.7177 - val_loss: 0.2115 - val_accuracy: 0.7305\n",
      "Epoch 517/1000\n",
      "563/563 [==============================] - 1s 963us/step - loss: 0.2137 - accuracy: 0.7144 - val_loss: 0.2114 - val_accuracy: 0.7305\n",
      "Epoch 518/1000\n",
      "563/563 [==============================] - 1s 947us/step - loss: 0.2119 - accuracy: 0.7225 - val_loss: 0.2114 - val_accuracy: 0.7305\n",
      "Epoch 519/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2136 - accuracy: 0.7147 - val_loss: 0.2113 - val_accuracy: 0.7305\n",
      "Epoch 520/1000\n",
      "563/563 [==============================] - 1s 972us/step - loss: 0.2121 - accuracy: 0.7233 - val_loss: 0.2113 - val_accuracy: 0.7305\n",
      "Epoch 521/1000\n",
      "563/563 [==============================] - 1s 952us/step - loss: 0.2120 - accuracy: 0.7216 - val_loss: 0.2113 - val_accuracy: 0.7300\n",
      "Epoch 522/1000\n",
      "563/563 [==============================] - 1s 988us/step - loss: 0.2105 - accuracy: 0.7222 - val_loss: 0.2112 - val_accuracy: 0.7300\n",
      "Epoch 523/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2114 - accuracy: 0.7190 - val_loss: 0.2112 - val_accuracy: 0.7299\n",
      "Epoch 524/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2140 - accuracy: 0.7145 - val_loss: 0.2111 - val_accuracy: 0.7299\n",
      "Epoch 525/1000\n",
      "563/563 [==============================] - 1s 956us/step - loss: 0.2105 - accuracy: 0.7229 - val_loss: 0.2111 - val_accuracy: 0.7299\n",
      "Epoch 526/1000\n",
      "563/563 [==============================] - 1s 982us/step - loss: 0.2109 - accuracy: 0.7223 - val_loss: 0.2110 - val_accuracy: 0.7299\n",
      "Epoch 527/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2117 - accuracy: 0.7161 - val_loss: 0.2110 - val_accuracy: 0.7297\n",
      "Epoch 528/1000\n",
      "563/563 [==============================] - 1s 961us/step - loss: 0.2116 - accuracy: 0.7190 - val_loss: 0.2109 - val_accuracy: 0.7299\n",
      "Epoch 529/1000\n",
      "563/563 [==============================] - 1s 950us/step - loss: 0.2115 - accuracy: 0.7176 - val_loss: 0.2109 - val_accuracy: 0.7299\n",
      "Epoch 530/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2126 - accuracy: 0.7156 - val_loss: 0.2109 - val_accuracy: 0.7299\n",
      "Epoch 531/1000\n",
      "563/563 [==============================] - 1s 949us/step - loss: 0.2111 - accuracy: 0.7227 - val_loss: 0.2108 - val_accuracy: 0.7300\n",
      "Epoch 532/1000\n",
      "563/563 [==============================] - 1s 974us/step - loss: 0.2106 - accuracy: 0.7239 - val_loss: 0.2108 - val_accuracy: 0.7300\n",
      "Epoch 533/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2099 - accuracy: 0.7235 - val_loss: 0.2107 - val_accuracy: 0.7301\n",
      "Epoch 534/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2125 - accuracy: 0.7160 - val_loss: 0.2107 - val_accuracy: 0.7302\n",
      "Epoch 535/1000\n",
      "563/563 [==============================] - 1s 991us/step - loss: 0.2103 - accuracy: 0.7186 - val_loss: 0.2106 - val_accuracy: 0.7302\n",
      "Epoch 536/1000\n",
      "563/563 [==============================] - 1s 981us/step - loss: 0.2121 - accuracy: 0.7200 - val_loss: 0.2106 - val_accuracy: 0.7304\n",
      "Epoch 537/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2090 - accuracy: 0.7251 - val_loss: 0.2105 - val_accuracy: 0.7304\n",
      "Epoch 538/1000\n",
      "563/563 [==============================] - 1s 958us/step - loss: 0.2114 - accuracy: 0.7230 - val_loss: 0.2105 - val_accuracy: 0.7305\n",
      "Epoch 539/1000\n",
      "563/563 [==============================] - 1s 974us/step - loss: 0.2108 - accuracy: 0.7226 - val_loss: 0.2105 - val_accuracy: 0.7305\n",
      "Epoch 540/1000\n",
      "563/563 [==============================] - 1s 972us/step - loss: 0.2106 - accuracy: 0.7199 - val_loss: 0.2104 - val_accuracy: 0.7305\n",
      "Epoch 541/1000\n",
      "563/563 [==============================] - 1s 986us/step - loss: 0.2103 - accuracy: 0.7247 - val_loss: 0.2104 - val_accuracy: 0.7305\n",
      "Epoch 542/1000\n",
      "563/563 [==============================] - 1s 954us/step - loss: 0.2125 - accuracy: 0.7147 - val_loss: 0.2103 - val_accuracy: 0.7305\n",
      "Epoch 543/1000\n",
      "563/563 [==============================] - 1s 950us/step - loss: 0.2114 - accuracy: 0.7154 - val_loss: 0.2103 - val_accuracy: 0.7305\n",
      "Epoch 544/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2107 - accuracy: 0.7231 - val_loss: 0.2102 - val_accuracy: 0.7305\n",
      "Epoch 545/1000\n",
      "563/563 [==============================] - 1s 974us/step - loss: 0.2102 - accuracy: 0.7199 - val_loss: 0.2102 - val_accuracy: 0.7304\n",
      "Epoch 546/1000\n",
      "563/563 [==============================] - 1s 952us/step - loss: 0.2097 - accuracy: 0.7217 - val_loss: 0.2102 - val_accuracy: 0.7306\n",
      "Epoch 547/1000\n",
      "563/563 [==============================] - 1s 938us/step - loss: 0.2099 - accuracy: 0.7229 - val_loss: 0.2101 - val_accuracy: 0.7306\n",
      "Epoch 548/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2097 - accuracy: 0.7198 - val_loss: 0.2101 - val_accuracy: 0.7306\n",
      "Epoch 549/1000\n",
      "563/563 [==============================] - 1s 965us/step - loss: 0.2091 - accuracy: 0.7294 - val_loss: 0.2100 - val_accuracy: 0.7306\n",
      "Epoch 550/1000\n",
      "563/563 [==============================] - 1s 940us/step - loss: 0.2099 - accuracy: 0.7230 - val_loss: 0.2100 - val_accuracy: 0.7306\n",
      "Epoch 551/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2098 - accuracy: 0.7204 - val_loss: 0.2100 - val_accuracy: 0.7306\n",
      "Epoch 552/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2116 - accuracy: 0.7198 - val_loss: 0.2099 - val_accuracy: 0.7306\n",
      "Epoch 553/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2110 - accuracy: 0.7169 - val_loss: 0.2099 - val_accuracy: 0.7306\n",
      "Epoch 554/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2106 - accuracy: 0.7179 - val_loss: 0.2098 - val_accuracy: 0.7305\n",
      "Epoch 555/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2102 - accuracy: 0.7212 - val_loss: 0.2098 - val_accuracy: 0.7305\n",
      "Epoch 556/1000\n",
      "563/563 [==============================] - 1s 958us/step - loss: 0.2102 - accuracy: 0.7203 - val_loss: 0.2098 - val_accuracy: 0.7305\n",
      "Epoch 557/1000\n",
      "563/563 [==============================] - 1s 943us/step - loss: 0.2108 - accuracy: 0.7179 - val_loss: 0.2097 - val_accuracy: 0.7305\n",
      "Epoch 558/1000\n",
      "563/563 [==============================] - 1s 998us/step - loss: 0.2093 - accuracy: 0.7221 - val_loss: 0.2097 - val_accuracy: 0.7305\n",
      "Epoch 559/1000\n",
      "563/563 [==============================] - 1s 977us/step - loss: 0.2109 - accuracy: 0.7207 - val_loss: 0.2096 - val_accuracy: 0.7305\n",
      "Epoch 560/1000\n",
      "563/563 [==============================] - 1s 954us/step - loss: 0.2098 - accuracy: 0.7219 - val_loss: 0.2096 - val_accuracy: 0.7305\n",
      "Epoch 561/1000\n",
      "563/563 [==============================] - 1s 954us/step - loss: 0.2091 - accuracy: 0.7243 - val_loss: 0.2095 - val_accuracy: 0.7305\n",
      "Epoch 562/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2107 - accuracy: 0.7175 - val_loss: 0.2095 - val_accuracy: 0.7305\n",
      "Epoch 563/1000\n",
      "563/563 [==============================] - 1s 995us/step - loss: 0.2101 - accuracy: 0.7221 - val_loss: 0.2095 - val_accuracy: 0.7305\n",
      "Epoch 564/1000\n",
      "563/563 [==============================] - 1s 945us/step - loss: 0.2104 - accuracy: 0.7209 - val_loss: 0.2094 - val_accuracy: 0.7305\n",
      "Epoch 565/1000\n",
      "563/563 [==============================] - 1s 990us/step - loss: 0.2110 - accuracy: 0.7193 - val_loss: 0.2094 - val_accuracy: 0.7305\n",
      "Epoch 566/1000\n",
      "563/563 [==============================] - 1s 973us/step - loss: 0.2097 - accuracy: 0.7152 - val_loss: 0.2093 - val_accuracy: 0.7305\n",
      "Epoch 567/1000\n",
      "563/563 [==============================] - 1s 965us/step - loss: 0.2101 - accuracy: 0.7233 - val_loss: 0.2093 - val_accuracy: 0.7305\n",
      "Epoch 568/1000\n",
      "563/563 [==============================] - 1s 968us/step - loss: 0.2118 - accuracy: 0.7170 - val_loss: 0.2093 - val_accuracy: 0.7306\n",
      "Epoch 569/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2096 - accuracy: 0.7217 - val_loss: 0.2092 - val_accuracy: 0.7306\n",
      "Epoch 570/1000\n",
      "563/563 [==============================] - 1s 952us/step - loss: 0.2107 - accuracy: 0.7217 - val_loss: 0.2092 - val_accuracy: 0.7306\n",
      "Epoch 571/1000\n",
      "563/563 [==============================] - 1s 970us/step - loss: 0.2089 - accuracy: 0.7250 - val_loss: 0.2091 - val_accuracy: 0.7306\n",
      "Epoch 572/1000\n",
      "563/563 [==============================] - 1s 984us/step - loss: 0.2092 - accuracy: 0.7209 - val_loss: 0.2091 - val_accuracy: 0.7308\n",
      "Epoch 573/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2080 - accuracy: 0.7266 - val_loss: 0.2091 - val_accuracy: 0.7308\n",
      "Epoch 574/1000\n",
      "563/563 [==============================] - 1s 972us/step - loss: 0.2079 - accuracy: 0.7255 - val_loss: 0.2090 - val_accuracy: 0.7306\n",
      "Epoch 575/1000\n",
      "563/563 [==============================] - 1s 947us/step - loss: 0.2102 - accuracy: 0.7165 - val_loss: 0.2090 - val_accuracy: 0.7308\n",
      "Epoch 576/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2100 - accuracy: 0.7210 - val_loss: 0.2089 - val_accuracy: 0.7306\n",
      "Epoch 577/1000\n",
      "563/563 [==============================] - 1s 963us/step - loss: 0.2091 - accuracy: 0.7241 - val_loss: 0.2089 - val_accuracy: 0.7308\n",
      "Epoch 578/1000\n",
      "563/563 [==============================] - 1s 984us/step - loss: 0.2084 - accuracy: 0.7230 - val_loss: 0.2089 - val_accuracy: 0.7310\n",
      "Epoch 579/1000\n",
      "563/563 [==============================] - 1s 947us/step - loss: 0.2107 - accuracy: 0.7156 - val_loss: 0.2088 - val_accuracy: 0.7308\n",
      "Epoch 580/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2092 - accuracy: 0.7192 - val_loss: 0.2088 - val_accuracy: 0.7308\n",
      "Epoch 581/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2094 - accuracy: 0.7183 - val_loss: 0.2087 - val_accuracy: 0.7309\n",
      "Epoch 582/1000\n",
      "563/563 [==============================] - 1s 977us/step - loss: 0.2096 - accuracy: 0.7197 - val_loss: 0.2087 - val_accuracy: 0.7309\n",
      "Epoch 583/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2088 - accuracy: 0.7249 - val_loss: 0.2087 - val_accuracy: 0.7309\n",
      "Epoch 584/1000\n",
      "563/563 [==============================] - 1s 966us/step - loss: 0.2078 - accuracy: 0.7258 - val_loss: 0.2086 - val_accuracy: 0.7309\n",
      "Epoch 585/1000\n",
      "563/563 [==============================] - 1s 941us/step - loss: 0.2088 - accuracy: 0.7174 - val_loss: 0.2086 - val_accuracy: 0.7310\n",
      "Epoch 586/1000\n",
      "563/563 [==============================] - 1s 960us/step - loss: 0.2065 - accuracy: 0.7287 - val_loss: 0.2086 - val_accuracy: 0.7310\n",
      "Epoch 587/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2100 - accuracy: 0.7199 - val_loss: 0.2085 - val_accuracy: 0.7310\n",
      "Epoch 588/1000\n",
      "563/563 [==============================] - 1s 956us/step - loss: 0.2084 - accuracy: 0.7205 - val_loss: 0.2085 - val_accuracy: 0.7310\n",
      "Epoch 589/1000\n",
      "563/563 [==============================] - 1s 950us/step - loss: 0.2094 - accuracy: 0.7182 - val_loss: 0.2084 - val_accuracy: 0.7310\n",
      "Epoch 590/1000\n",
      "563/563 [==============================] - 1s 997us/step - loss: 0.2083 - accuracy: 0.7207 - val_loss: 0.2084 - val_accuracy: 0.7310\n",
      "Epoch 591/1000\n",
      "563/563 [==============================] - 1s 988us/step - loss: 0.2085 - accuracy: 0.7205 - val_loss: 0.2084 - val_accuracy: 0.7310\n",
      "Epoch 592/1000\n",
      "563/563 [==============================] - 1s 952us/step - loss: 0.2094 - accuracy: 0.7197 - val_loss: 0.2083 - val_accuracy: 0.7311\n",
      "Epoch 593/1000\n",
      "563/563 [==============================] - 1s 949us/step - loss: 0.2085 - accuracy: 0.7230 - val_loss: 0.2083 - val_accuracy: 0.7311\n",
      "Epoch 594/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2077 - accuracy: 0.7245 - val_loss: 0.2083 - val_accuracy: 0.7311\n",
      "Epoch 595/1000\n",
      "563/563 [==============================] - 1s 986us/step - loss: 0.2064 - accuracy: 0.7263 - val_loss: 0.2082 - val_accuracy: 0.7311\n",
      "Epoch 596/1000\n",
      "563/563 [==============================] - 1s 956us/step - loss: 0.2077 - accuracy: 0.7227 - val_loss: 0.2082 - val_accuracy: 0.7311\n",
      "Epoch 597/1000\n",
      "563/563 [==============================] - 1s 952us/step - loss: 0.2090 - accuracy: 0.7197 - val_loss: 0.2081 - val_accuracy: 0.7311\n",
      "Epoch 598/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2076 - accuracy: 0.7216 - val_loss: 0.2081 - val_accuracy: 0.7311\n",
      "Epoch 599/1000\n",
      "563/563 [==============================] - 1s 982us/step - loss: 0.2084 - accuracy: 0.7208 - val_loss: 0.2081 - val_accuracy: 0.7311\n",
      "Epoch 600/1000\n",
      "563/563 [==============================] - 1s 959us/step - loss: 0.2096 - accuracy: 0.7200 - val_loss: 0.2080 - val_accuracy: 0.7313\n",
      "Epoch 601/1000\n",
      "563/563 [==============================] - 1s 966us/step - loss: 0.2095 - accuracy: 0.7218 - val_loss: 0.2080 - val_accuracy: 0.7313\n",
      "Epoch 602/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2081 - accuracy: 0.7222 - val_loss: 0.2079 - val_accuracy: 0.7313\n",
      "Epoch 603/1000\n",
      "563/563 [==============================] - 1s 982us/step - loss: 0.2103 - accuracy: 0.7179 - val_loss: 0.2079 - val_accuracy: 0.7311\n",
      "Epoch 604/1000\n",
      "563/563 [==============================] - 1s 950us/step - loss: 0.2084 - accuracy: 0.7205 - val_loss: 0.2079 - val_accuracy: 0.7311\n",
      "Epoch 605/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2095 - accuracy: 0.7179 - val_loss: 0.2078 - val_accuracy: 0.7313\n",
      "Epoch 606/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2102 - accuracy: 0.7154 - val_loss: 0.2078 - val_accuracy: 0.7313\n",
      "Epoch 607/1000\n",
      "563/563 [==============================] - 1s 954us/step - loss: 0.2075 - accuracy: 0.7268 - val_loss: 0.2078 - val_accuracy: 0.7314\n",
      "Epoch 608/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2080 - accuracy: 0.7225 - val_loss: 0.2077 - val_accuracy: 0.7314\n",
      "Epoch 609/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2079 - accuracy: 0.7210 - val_loss: 0.2077 - val_accuracy: 0.7315\n",
      "Epoch 610/1000\n",
      "563/563 [==============================] - 1s 961us/step - loss: 0.2065 - accuracy: 0.7279 - val_loss: 0.2077 - val_accuracy: 0.7317\n",
      "Epoch 611/1000\n",
      "563/563 [==============================] - 1s 958us/step - loss: 0.2095 - accuracy: 0.7176 - val_loss: 0.2076 - val_accuracy: 0.7317\n",
      "Epoch 612/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2073 - accuracy: 0.7250 - val_loss: 0.2076 - val_accuracy: 0.7317\n",
      "Epoch 613/1000\n",
      "563/563 [==============================] - 1s 984us/step - loss: 0.2083 - accuracy: 0.7254 - val_loss: 0.2075 - val_accuracy: 0.7317\n",
      "Epoch 614/1000\n",
      "563/563 [==============================] - 1s 975us/step - loss: 0.2080 - accuracy: 0.7293 - val_loss: 0.2075 - val_accuracy: 0.7317\n",
      "Epoch 615/1000\n",
      "563/563 [==============================] - 1s 993us/step - loss: 0.2073 - accuracy: 0.7255 - val_loss: 0.2075 - val_accuracy: 0.7317\n",
      "Epoch 616/1000\n",
      "563/563 [==============================] - 1s 998us/step - loss: 0.2092 - accuracy: 0.7161 - val_loss: 0.2074 - val_accuracy: 0.7318\n",
      "Epoch 617/1000\n",
      "563/563 [==============================] - 1s 963us/step - loss: 0.2069 - accuracy: 0.7284 - val_loss: 0.2074 - val_accuracy: 0.7318\n",
      "Epoch 618/1000\n",
      "563/563 [==============================] - 1s 984us/step - loss: 0.2084 - accuracy: 0.7230 - val_loss: 0.2074 - val_accuracy: 0.7318\n",
      "Epoch 619/1000\n",
      "563/563 [==============================] - 1s 998us/step - loss: 0.2070 - accuracy: 0.7275 - val_loss: 0.2073 - val_accuracy: 0.7318\n",
      "Epoch 620/1000\n",
      "563/563 [==============================] - 1s 954us/step - loss: 0.2060 - accuracy: 0.7266 - val_loss: 0.2073 - val_accuracy: 0.7321\n",
      "Epoch 621/1000\n",
      "563/563 [==============================] - 1s 936us/step - loss: 0.2086 - accuracy: 0.7210 - val_loss: 0.2073 - val_accuracy: 0.7326\n",
      "Epoch 622/1000\n",
      "563/563 [==============================] - 1s 975us/step - loss: 0.2076 - accuracy: 0.7261 - val_loss: 0.2072 - val_accuracy: 0.7331\n",
      "Epoch 623/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2080 - accuracy: 0.7215 - val_loss: 0.2072 - val_accuracy: 0.7330\n",
      "Epoch 624/1000\n",
      "563/563 [==============================] - 1s 991us/step - loss: 0.2080 - accuracy: 0.7253 - val_loss: 0.2071 - val_accuracy: 0.7335\n",
      "Epoch 625/1000\n",
      "563/563 [==============================] - 1s 963us/step - loss: 0.2071 - accuracy: 0.7257 - val_loss: 0.2071 - val_accuracy: 0.7335\n",
      "Epoch 626/1000\n",
      "563/563 [==============================] - 1s 997us/step - loss: 0.2097 - accuracy: 0.7143 - val_loss: 0.2071 - val_accuracy: 0.7335\n",
      "Epoch 627/1000\n",
      "563/563 [==============================] - 1s 988us/step - loss: 0.2072 - accuracy: 0.7234 - val_loss: 0.2070 - val_accuracy: 0.7335\n",
      "Epoch 628/1000\n",
      "563/563 [==============================] - 1s 962us/step - loss: 0.2080 - accuracy: 0.7221 - val_loss: 0.2070 - val_accuracy: 0.7335\n",
      "Epoch 629/1000\n",
      "563/563 [==============================] - 1s 961us/step - loss: 0.2057 - accuracy: 0.7286 - val_loss: 0.2070 - val_accuracy: 0.7335\n",
      "Epoch 630/1000\n",
      "563/563 [==============================] - 1s 990us/step - loss: 0.2064 - accuracy: 0.7255 - val_loss: 0.2069 - val_accuracy: 0.7335\n",
      "Epoch 631/1000\n",
      "563/563 [==============================] - 1s 972us/step - loss: 0.2074 - accuracy: 0.7232 - val_loss: 0.2069 - val_accuracy: 0.7335\n",
      "Epoch 632/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2078 - accuracy: 0.7246 - val_loss: 0.2069 - val_accuracy: 0.7335\n",
      "Epoch 633/1000\n",
      "563/563 [==============================] - 1s 967us/step - loss: 0.2069 - accuracy: 0.7245 - val_loss: 0.2068 - val_accuracy: 0.7336\n",
      "Epoch 634/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2090 - accuracy: 0.7224 - val_loss: 0.2068 - val_accuracy: 0.7336\n",
      "Epoch 635/1000\n",
      "563/563 [==============================] - 1s 970us/step - loss: 0.2069 - accuracy: 0.7230 - val_loss: 0.2068 - val_accuracy: 0.7336\n",
      "Epoch 636/1000\n",
      "563/563 [==============================] - 1s 991us/step - loss: 0.2063 - accuracy: 0.7250 - val_loss: 0.2067 - val_accuracy: 0.7336\n",
      "Epoch 637/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2089 - accuracy: 0.7162 - val_loss: 0.2067 - val_accuracy: 0.7336\n",
      "Epoch 638/1000\n",
      "563/563 [==============================] - 1s 958us/step - loss: 0.2067 - accuracy: 0.7239 - val_loss: 0.2066 - val_accuracy: 0.7336\n",
      "Epoch 639/1000\n",
      "563/563 [==============================] - 1s 933us/step - loss: 0.2073 - accuracy: 0.7236 - val_loss: 0.2066 - val_accuracy: 0.7336\n",
      "Epoch 640/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2065 - accuracy: 0.7268 - val_loss: 0.2066 - val_accuracy: 0.7336\n",
      "Epoch 641/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2072 - accuracy: 0.7238 - val_loss: 0.2065 - val_accuracy: 0.7336\n",
      "Epoch 642/1000\n",
      "563/563 [==============================] - 1s 936us/step - loss: 0.2061 - accuracy: 0.7271 - val_loss: 0.2065 - val_accuracy: 0.7336\n",
      "Epoch 643/1000\n",
      "563/563 [==============================] - 1s 965us/step - loss: 0.2072 - accuracy: 0.7228 - val_loss: 0.2065 - val_accuracy: 0.7336\n",
      "Epoch 644/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2081 - accuracy: 0.7210 - val_loss: 0.2064 - val_accuracy: 0.7336\n",
      "Epoch 645/1000\n",
      "563/563 [==============================] - 1s 975us/step - loss: 0.2065 - accuracy: 0.7260 - val_loss: 0.2064 - val_accuracy: 0.7336\n",
      "Epoch 646/1000\n",
      "563/563 [==============================] - 1s 947us/step - loss: 0.2064 - accuracy: 0.7238 - val_loss: 0.2064 - val_accuracy: 0.7336\n",
      "Epoch 647/1000\n",
      "563/563 [==============================] - 1s 958us/step - loss: 0.2062 - accuracy: 0.7246 - val_loss: 0.2063 - val_accuracy: 0.7336\n",
      "Epoch 648/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2063 - accuracy: 0.7223 - val_loss: 0.2063 - val_accuracy: 0.7336\n",
      "Epoch 649/1000\n",
      "563/563 [==============================] - 1s 981us/step - loss: 0.2079 - accuracy: 0.7214 - val_loss: 0.2063 - val_accuracy: 0.7336\n",
      "Epoch 650/1000\n",
      "563/563 [==============================] - 1s 998us/step - loss: 0.2055 - accuracy: 0.7257 - val_loss: 0.2062 - val_accuracy: 0.7336\n",
      "Epoch 651/1000\n",
      "563/563 [==============================] - 1s 986us/step - loss: 0.2075 - accuracy: 0.7232 - val_loss: 0.2062 - val_accuracy: 0.7336\n",
      "Epoch 652/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2069 - accuracy: 0.7230 - val_loss: 0.2062 - val_accuracy: 0.7336\n",
      "Epoch 653/1000\n",
      "563/563 [==============================] - 1s 974us/step - loss: 0.2064 - accuracy: 0.7211 - val_loss: 0.2061 - val_accuracy: 0.7336\n",
      "Epoch 654/1000\n",
      "563/563 [==============================] - 1s 970us/step - loss: 0.2062 - accuracy: 0.7263 - val_loss: 0.2061 - val_accuracy: 0.7336\n",
      "Epoch 655/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2050 - accuracy: 0.7264 - val_loss: 0.2061 - val_accuracy: 0.7336\n",
      "Epoch 656/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2066 - accuracy: 0.7227 - val_loss: 0.2060 - val_accuracy: 0.7336\n",
      "Epoch 657/1000\n",
      "563/563 [==============================] - 1s 969us/step - loss: 0.2070 - accuracy: 0.7246 - val_loss: 0.2060 - val_accuracy: 0.7336\n",
      "Epoch 658/1000\n",
      "563/563 [==============================] - 1s 975us/step - loss: 0.2061 - accuracy: 0.7220 - val_loss: 0.2060 - val_accuracy: 0.7336\n",
      "Epoch 659/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2087 - accuracy: 0.7203 - val_loss: 0.2059 - val_accuracy: 0.7336\n",
      "Epoch 660/1000\n",
      "563/563 [==============================] - 1s 936us/step - loss: 0.2061 - accuracy: 0.7238 - val_loss: 0.2059 - val_accuracy: 0.7336\n",
      "Epoch 661/1000\n",
      "563/563 [==============================] - 1s 972us/step - loss: 0.2071 - accuracy: 0.7209 - val_loss: 0.2059 - val_accuracy: 0.7336\n",
      "Epoch 662/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2061 - accuracy: 0.7235 - val_loss: 0.2058 - val_accuracy: 0.7336\n",
      "Epoch 663/1000\n",
      "563/563 [==============================] - 1s 981us/step - loss: 0.2061 - accuracy: 0.7263 - val_loss: 0.2058 - val_accuracy: 0.7336\n",
      "Epoch 664/1000\n",
      "563/563 [==============================] - 1s 941us/step - loss: 0.2046 - accuracy: 0.7300 - val_loss: 0.2058 - val_accuracy: 0.7336\n",
      "Epoch 665/1000\n",
      "563/563 [==============================] - 1s 974us/step - loss: 0.2073 - accuracy: 0.7215 - val_loss: 0.2057 - val_accuracy: 0.7336\n",
      "Epoch 666/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2054 - accuracy: 0.7264 - val_loss: 0.2057 - val_accuracy: 0.7336\n",
      "Epoch 667/1000\n",
      "563/563 [==============================] - 1s 966us/step - loss: 0.2073 - accuracy: 0.7227 - val_loss: 0.2057 - val_accuracy: 0.7336\n",
      "Epoch 668/1000\n",
      "563/563 [==============================] - 1s 967us/step - loss: 0.2057 - accuracy: 0.7247 - val_loss: 0.2056 - val_accuracy: 0.7336\n",
      "Epoch 669/1000\n",
      "563/563 [==============================] - 1s 976us/step - loss: 0.2071 - accuracy: 0.7206 - val_loss: 0.2056 - val_accuracy: 0.7336\n",
      "Epoch 670/1000\n",
      "563/563 [==============================] - 1s 991us/step - loss: 0.2077 - accuracy: 0.7186 - val_loss: 0.2056 - val_accuracy: 0.7336\n",
      "Epoch 671/1000\n",
      "563/563 [==============================] - 1s 956us/step - loss: 0.2077 - accuracy: 0.7202 - val_loss: 0.2055 - val_accuracy: 0.7336\n",
      "Epoch 672/1000\n",
      "563/563 [==============================] - 1s 943us/step - loss: 0.2064 - accuracy: 0.7212 - val_loss: 0.2055 - val_accuracy: 0.7335\n",
      "Epoch 673/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2068 - accuracy: 0.7204 - val_loss: 0.2055 - val_accuracy: 0.7335\n",
      "Epoch 674/1000\n",
      "563/563 [==============================] - 1s 975us/step - loss: 0.2080 - accuracy: 0.7177 - val_loss: 0.2054 - val_accuracy: 0.7335\n",
      "Epoch 675/1000\n",
      "563/563 [==============================] - 1s 934us/step - loss: 0.2076 - accuracy: 0.7210 - val_loss: 0.2054 - val_accuracy: 0.7335\n",
      "Epoch 676/1000\n",
      "563/563 [==============================] - 1s 956us/step - loss: 0.2047 - accuracy: 0.7246 - val_loss: 0.2054 - val_accuracy: 0.7335\n",
      "Epoch 677/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2057 - accuracy: 0.7269 - val_loss: 0.2053 - val_accuracy: 0.7335\n",
      "Epoch 678/1000\n",
      "563/563 [==============================] - 1s 956us/step - loss: 0.2058 - accuracy: 0.7248 - val_loss: 0.2053 - val_accuracy: 0.7335\n",
      "Epoch 679/1000\n",
      "563/563 [==============================] - 1s 966us/step - loss: 0.2057 - accuracy: 0.7260 - val_loss: 0.2053 - val_accuracy: 0.7335\n",
      "Epoch 680/1000\n",
      "563/563 [==============================] - 1s 977us/step - loss: 0.2059 - accuracy: 0.7222 - val_loss: 0.2052 - val_accuracy: 0.7336\n",
      "Epoch 681/1000\n",
      "563/563 [==============================] - 1s 979us/step - loss: 0.2055 - accuracy: 0.7253 - val_loss: 0.2052 - val_accuracy: 0.7336\n",
      "Epoch 682/1000\n",
      "563/563 [==============================] - 1s 961us/step - loss: 0.2082 - accuracy: 0.7193 - val_loss: 0.2052 - val_accuracy: 0.7336\n",
      "Epoch 683/1000\n",
      "563/563 [==============================] - 1s 936us/step - loss: 0.2050 - accuracy: 0.7248 - val_loss: 0.2052 - val_accuracy: 0.7336\n",
      "Epoch 684/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2040 - accuracy: 0.7330 - val_loss: 0.2051 - val_accuracy: 0.7336\n",
      "Epoch 685/1000\n",
      "563/563 [==============================] - 1s 970us/step - loss: 0.2073 - accuracy: 0.7208 - val_loss: 0.2051 - val_accuracy: 0.7336\n",
      "Epoch 686/1000\n",
      "563/563 [==============================] - 1s 977us/step - loss: 0.2070 - accuracy: 0.7220 - val_loss: 0.2051 - val_accuracy: 0.7336\n",
      "Epoch 687/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2085 - accuracy: 0.7182 - val_loss: 0.2050 - val_accuracy: 0.7336\n",
      "Epoch 688/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2061 - accuracy: 0.7208 - val_loss: 0.2050 - val_accuracy: 0.7335\n",
      "Epoch 689/1000\n",
      "563/563 [==============================] - 1s 954us/step - loss: 0.2046 - accuracy: 0.7260 - val_loss: 0.2050 - val_accuracy: 0.7335\n",
      "Epoch 690/1000\n",
      "563/563 [==============================] - 1s 938us/step - loss: 0.2057 - accuracy: 0.7223 - val_loss: 0.2049 - val_accuracy: 0.7335\n",
      "Epoch 691/1000\n",
      "563/563 [==============================] - 1s 984us/step - loss: 0.2050 - accuracy: 0.7251 - val_loss: 0.2049 - val_accuracy: 0.7335\n",
      "Epoch 692/1000\n",
      "563/563 [==============================] - 1s 972us/step - loss: 0.2038 - accuracy: 0.7308 - val_loss: 0.2049 - val_accuracy: 0.7335\n",
      "Epoch 693/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2063 - accuracy: 0.7210 - val_loss: 0.2048 - val_accuracy: 0.7335\n",
      "Epoch 694/1000\n",
      "563/563 [==============================] - 1s 995us/step - loss: 0.2067 - accuracy: 0.7201 - val_loss: 0.2048 - val_accuracy: 0.7335\n",
      "Epoch 695/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2053 - accuracy: 0.7270 - val_loss: 0.2048 - val_accuracy: 0.7335\n",
      "Epoch 696/1000\n",
      "563/563 [==============================] - 1s 940us/step - loss: 0.2063 - accuracy: 0.7215 - val_loss: 0.2047 - val_accuracy: 0.7335\n",
      "Epoch 697/1000\n",
      "563/563 [==============================] - 1s 956us/step - loss: 0.2055 - accuracy: 0.7222 - val_loss: 0.2047 - val_accuracy: 0.7335\n",
      "Epoch 698/1000\n",
      "563/563 [==============================] - 1s 968us/step - loss: 0.2068 - accuracy: 0.7184 - val_loss: 0.2047 - val_accuracy: 0.7335\n",
      "Epoch 699/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2043 - accuracy: 0.7253 - val_loss: 0.2046 - val_accuracy: 0.7335\n",
      "Epoch 700/1000\n",
      "563/563 [==============================] - 1s 961us/step - loss: 0.2037 - accuracy: 0.7293 - val_loss: 0.2046 - val_accuracy: 0.7335\n",
      "Epoch 701/1000\n",
      "563/563 [==============================] - 1s 974us/step - loss: 0.2024 - accuracy: 0.7302 - val_loss: 0.2046 - val_accuracy: 0.7335\n",
      "Epoch 702/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2052 - accuracy: 0.7224 - val_loss: 0.2045 - val_accuracy: 0.7335\n",
      "Epoch 703/1000\n",
      "563/563 [==============================] - 1s 970us/step - loss: 0.2070 - accuracy: 0.7173 - val_loss: 0.2045 - val_accuracy: 0.7335\n",
      "Epoch 704/1000\n",
      "563/563 [==============================] - 1s 975us/step - loss: 0.2061 - accuracy: 0.7215 - val_loss: 0.2045 - val_accuracy: 0.7335\n",
      "Epoch 705/1000\n",
      "563/563 [==============================] - 1s 943us/step - loss: 0.2050 - accuracy: 0.7258 - val_loss: 0.2044 - val_accuracy: 0.7336\n",
      "Epoch 706/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2072 - accuracy: 0.7190 - val_loss: 0.2044 - val_accuracy: 0.7336\n",
      "Epoch 707/1000\n",
      "563/563 [==============================] - 1s 940us/step - loss: 0.2063 - accuracy: 0.7175 - val_loss: 0.2044 - val_accuracy: 0.7336\n",
      "Epoch 708/1000\n",
      "563/563 [==============================] - 1s 941us/step - loss: 0.2056 - accuracy: 0.7199 - val_loss: 0.2044 - val_accuracy: 0.7336\n",
      "Epoch 709/1000\n",
      "563/563 [==============================] - 1s 984us/step - loss: 0.2051 - accuracy: 0.7233 - val_loss: 0.2043 - val_accuracy: 0.7336\n",
      "Epoch 710/1000\n",
      "563/563 [==============================] - 1s 990us/step - loss: 0.2040 - accuracy: 0.7283 - val_loss: 0.2043 - val_accuracy: 0.7336\n",
      "Epoch 711/1000\n",
      "563/563 [==============================] - 1s 943us/step - loss: 0.2066 - accuracy: 0.7196 - val_loss: 0.2043 - val_accuracy: 0.7336\n",
      "Epoch 712/1000\n",
      "563/563 [==============================] - 1s 950us/step - loss: 0.2053 - accuracy: 0.7239 - val_loss: 0.2042 - val_accuracy: 0.7336\n",
      "Epoch 713/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2059 - accuracy: 0.7217 - val_loss: 0.2042 - val_accuracy: 0.7336\n",
      "Epoch 714/1000\n",
      "563/563 [==============================] - 1s 977us/step - loss: 0.2049 - accuracy: 0.7211 - val_loss: 0.2042 - val_accuracy: 0.7336\n",
      "Epoch 715/1000\n",
      "563/563 [==============================] - 1s 945us/step - loss: 0.2051 - accuracy: 0.7219 - val_loss: 0.2041 - val_accuracy: 0.7336\n",
      "Epoch 716/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2052 - accuracy: 0.7249 - val_loss: 0.2041 - val_accuracy: 0.7336\n",
      "Epoch 717/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2046 - accuracy: 0.7253 - val_loss: 0.2041 - val_accuracy: 0.7336\n",
      "Epoch 718/1000\n",
      "563/563 [==============================] - 1s 945us/step - loss: 0.2063 - accuracy: 0.7211 - val_loss: 0.2041 - val_accuracy: 0.7336\n",
      "Epoch 719/1000\n",
      "563/563 [==============================] - 1s 947us/step - loss: 0.2062 - accuracy: 0.7233 - val_loss: 0.2040 - val_accuracy: 0.7336\n",
      "Epoch 720/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2043 - accuracy: 0.7259 - val_loss: 0.2040 - val_accuracy: 0.7336\n",
      "Epoch 721/1000\n",
      "563/563 [==============================] - 1s 984us/step - loss: 0.2048 - accuracy: 0.7238 - val_loss: 0.2040 - val_accuracy: 0.7336\n",
      "Epoch 722/1000\n",
      "563/563 [==============================] - 1s 965us/step - loss: 0.2055 - accuracy: 0.7197 - val_loss: 0.2039 - val_accuracy: 0.7336\n",
      "Epoch 723/1000\n",
      "563/563 [==============================] - 1s 947us/step - loss: 0.2059 - accuracy: 0.7204 - val_loss: 0.2039 - val_accuracy: 0.7336\n",
      "Epoch 724/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2018 - accuracy: 0.7317 - val_loss: 0.2039 - val_accuracy: 0.7336\n",
      "Epoch 725/1000\n",
      "563/563 [==============================] - 1s 950us/step - loss: 0.2036 - accuracy: 0.7270 - val_loss: 0.2038 - val_accuracy: 0.7336\n",
      "Epoch 726/1000\n",
      "563/563 [==============================] - 1s 961us/step - loss: 0.2060 - accuracy: 0.7202 - val_loss: 0.2038 - val_accuracy: 0.7336\n",
      "Epoch 727/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2060 - accuracy: 0.7209 - val_loss: 0.2038 - val_accuracy: 0.7336\n",
      "Epoch 728/1000\n",
      "563/563 [==============================] - 1s 979us/step - loss: 0.2035 - accuracy: 0.7267 - val_loss: 0.2038 - val_accuracy: 0.7336\n",
      "Epoch 729/1000\n",
      "563/563 [==============================] - 1s 959us/step - loss: 0.2054 - accuracy: 0.7204 - val_loss: 0.2037 - val_accuracy: 0.7336\n",
      "Epoch 730/1000\n",
      "563/563 [==============================] - 1s 949us/step - loss: 0.2054 - accuracy: 0.7228 - val_loss: 0.2037 - val_accuracy: 0.7336\n",
      "Epoch 731/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2046 - accuracy: 0.7246 - val_loss: 0.2037 - val_accuracy: 0.7336\n",
      "Epoch 732/1000\n",
      "563/563 [==============================] - 1s 947us/step - loss: 0.2039 - accuracy: 0.7237 - val_loss: 0.2036 - val_accuracy: 0.7336\n",
      "Epoch 733/1000\n",
      "563/563 [==============================] - 1s 963us/step - loss: 0.2052 - accuracy: 0.7230 - val_loss: 0.2036 - val_accuracy: 0.7336\n",
      "Epoch 734/1000\n",
      "563/563 [==============================] - 1s 979us/step - loss: 0.2027 - accuracy: 0.7258 - val_loss: 0.2036 - val_accuracy: 0.7336\n",
      "Epoch 735/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2061 - accuracy: 0.7218 - val_loss: 0.2036 - val_accuracy: 0.7336\n",
      "Epoch 736/1000\n",
      "563/563 [==============================] - 1s 959us/step - loss: 0.2047 - accuracy: 0.7229 - val_loss: 0.2035 - val_accuracy: 0.7336\n",
      "Epoch 737/1000\n",
      "563/563 [==============================] - 1s 949us/step - loss: 0.2053 - accuracy: 0.7200 - val_loss: 0.2035 - val_accuracy: 0.7335\n",
      "Epoch 738/1000\n",
      "563/563 [==============================] - 1s 991us/step - loss: 0.2030 - accuracy: 0.7279 - val_loss: 0.2035 - val_accuracy: 0.7335\n",
      "Epoch 739/1000\n",
      "563/563 [==============================] - 1s 979us/step - loss: 0.2043 - accuracy: 0.7255 - val_loss: 0.2034 - val_accuracy: 0.7335\n",
      "Epoch 740/1000\n",
      "563/563 [==============================] - 1s 997us/step - loss: 0.2041 - accuracy: 0.7234 - val_loss: 0.2034 - val_accuracy: 0.7335\n",
      "Epoch 741/1000\n",
      "563/563 [==============================] - 1s 984us/step - loss: 0.2043 - accuracy: 0.7267 - val_loss: 0.2034 - val_accuracy: 0.7335\n",
      "Epoch 742/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2038 - accuracy: 0.7225 - val_loss: 0.2034 - val_accuracy: 0.7335\n",
      "Epoch 743/1000\n",
      "563/563 [==============================] - 1s 954us/step - loss: 0.2043 - accuracy: 0.7240 - val_loss: 0.2033 - val_accuracy: 0.7335\n",
      "Epoch 744/1000\n",
      "563/563 [==============================] - 1s 972us/step - loss: 0.2053 - accuracy: 0.7225 - val_loss: 0.2033 - val_accuracy: 0.7335\n",
      "Epoch 745/1000\n",
      "563/563 [==============================] - 1s 988us/step - loss: 0.2019 - accuracy: 0.7319 - val_loss: 0.2033 - val_accuracy: 0.7335\n",
      "Epoch 746/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2064 - accuracy: 0.7169 - val_loss: 0.2032 - val_accuracy: 0.7335\n",
      "Epoch 747/1000\n",
      "563/563 [==============================] - 1s 963us/step - loss: 0.2045 - accuracy: 0.7205 - val_loss: 0.2032 - val_accuracy: 0.7335\n",
      "Epoch 748/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2035 - accuracy: 0.7243 - val_loss: 0.2032 - val_accuracy: 0.7335\n",
      "Epoch 749/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2032 - accuracy: 0.7290 - val_loss: 0.2032 - val_accuracy: 0.7335\n",
      "Epoch 750/1000\n",
      "563/563 [==============================] - 1s 951us/step - loss: 0.2040 - accuracy: 0.7239 - val_loss: 0.2031 - val_accuracy: 0.7335\n",
      "Epoch 751/1000\n",
      "563/563 [==============================] - 1s 956us/step - loss: 0.2036 - accuracy: 0.7250 - val_loss: 0.2031 - val_accuracy: 0.7336\n",
      "Epoch 752/1000\n",
      "563/563 [==============================] - 1s 981us/step - loss: 0.2036 - accuracy: 0.7278 - val_loss: 0.2031 - val_accuracy: 0.7336\n",
      "Epoch 753/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2030 - accuracy: 0.7250 - val_loss: 0.2030 - val_accuracy: 0.7336\n",
      "Epoch 754/1000\n",
      "563/563 [==============================] - 1s 947us/step - loss: 0.2041 - accuracy: 0.7229 - val_loss: 0.2030 - val_accuracy: 0.7336\n",
      "Epoch 755/1000\n",
      "563/563 [==============================] - 1s 954us/step - loss: 0.2044 - accuracy: 0.7217 - val_loss: 0.2030 - val_accuracy: 0.7336\n",
      "Epoch 756/1000\n",
      "563/563 [==============================] - 1s 993us/step - loss: 0.2044 - accuracy: 0.7224 - val_loss: 0.2030 - val_accuracy: 0.7336\n",
      "Epoch 757/1000\n",
      "563/563 [==============================] - 1s 966us/step - loss: 0.2031 - accuracy: 0.7275 - val_loss: 0.2029 - val_accuracy: 0.7336\n",
      "Epoch 758/1000\n",
      "563/563 [==============================] - 1s 977us/step - loss: 0.2038 - accuracy: 0.7212 - val_loss: 0.2029 - val_accuracy: 0.7336\n",
      "Epoch 759/1000\n",
      "563/563 [==============================] - 1s 956us/step - loss: 0.2034 - accuracy: 0.7241 - val_loss: 0.2029 - val_accuracy: 0.7336\n",
      "Epoch 760/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2066 - accuracy: 0.7171 - val_loss: 0.2028 - val_accuracy: 0.7336\n",
      "Epoch 761/1000\n",
      "563/563 [==============================] - 1s 961us/step - loss: 0.2057 - accuracy: 0.7187 - val_loss: 0.2028 - val_accuracy: 0.7336\n",
      "Epoch 762/1000\n",
      "563/563 [==============================] - 1s 963us/step - loss: 0.2042 - accuracy: 0.7225 - val_loss: 0.2028 - val_accuracy: 0.7337\n",
      "Epoch 763/1000\n",
      "563/563 [==============================] - 1s 997us/step - loss: 0.2049 - accuracy: 0.7219 - val_loss: 0.2028 - val_accuracy: 0.7337\n",
      "Epoch 764/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2042 - accuracy: 0.7230 - val_loss: 0.2027 - val_accuracy: 0.7337\n",
      "Epoch 765/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2034 - accuracy: 0.7256 - val_loss: 0.2027 - val_accuracy: 0.7337\n",
      "Epoch 766/1000\n",
      "563/563 [==============================] - 1s 947us/step - loss: 0.2041 - accuracy: 0.7244 - val_loss: 0.2027 - val_accuracy: 0.7337\n",
      "Epoch 767/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2041 - accuracy: 0.7210 - val_loss: 0.2027 - val_accuracy: 0.7337\n",
      "Epoch 768/1000\n",
      "563/563 [==============================] - 1s 968us/step - loss: 0.2011 - accuracy: 0.7289 - val_loss: 0.2026 - val_accuracy: 0.7337\n",
      "Epoch 769/1000\n",
      "563/563 [==============================] - 1s 925us/step - loss: 0.2042 - accuracy: 0.7207 - val_loss: 0.2026 - val_accuracy: 0.7337\n",
      "Epoch 770/1000\n",
      "563/563 [==============================] - 1s 975us/step - loss: 0.2041 - accuracy: 0.7180 - val_loss: 0.2026 - val_accuracy: 0.7337\n",
      "Epoch 771/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2025 - accuracy: 0.7269 - val_loss: 0.2026 - val_accuracy: 0.7337\n",
      "Epoch 772/1000\n",
      "563/563 [==============================] - 1s 938us/step - loss: 0.2016 - accuracy: 0.7314 - val_loss: 0.2025 - val_accuracy: 0.7337\n",
      "Epoch 773/1000\n",
      "563/563 [==============================] - 1s 952us/step - loss: 0.2030 - accuracy: 0.7262 - val_loss: 0.2025 - val_accuracy: 0.7337\n",
      "Epoch 774/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2036 - accuracy: 0.7224 - val_loss: 0.2025 - val_accuracy: 0.7337\n",
      "Epoch 775/1000\n",
      "563/563 [==============================] - 1s 990us/step - loss: 0.2019 - accuracy: 0.7298 - val_loss: 0.2024 - val_accuracy: 0.7337\n",
      "Epoch 776/1000\n",
      "563/563 [==============================] - 1s 965us/step - loss: 0.2021 - accuracy: 0.7269 - val_loss: 0.2024 - val_accuracy: 0.7337\n",
      "Epoch 777/1000\n",
      "563/563 [==============================] - 1s 943us/step - loss: 0.2050 - accuracy: 0.7184 - val_loss: 0.2024 - val_accuracy: 0.7339\n",
      "Epoch 778/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2043 - accuracy: 0.7238 - val_loss: 0.2024 - val_accuracy: 0.7339\n",
      "Epoch 779/1000\n",
      "563/563 [==============================] - 1s 956us/step - loss: 0.2031 - accuracy: 0.7249 - val_loss: 0.2023 - val_accuracy: 0.7339\n",
      "Epoch 780/1000\n",
      "563/563 [==============================] - 1s 959us/step - loss: 0.2026 - accuracy: 0.7227 - val_loss: 0.2023 - val_accuracy: 0.7339\n",
      "Epoch 781/1000\n",
      "563/563 [==============================] - 1s 984us/step - loss: 0.2033 - accuracy: 0.7249 - val_loss: 0.2023 - val_accuracy: 0.7339\n",
      "Epoch 782/1000\n",
      "563/563 [==============================] - 1s 991us/step - loss: 0.2045 - accuracy: 0.7252 - val_loss: 0.2023 - val_accuracy: 0.7339\n",
      "Epoch 783/1000\n",
      "563/563 [==============================] - 1s 936us/step - loss: 0.2032 - accuracy: 0.7203 - val_loss: 0.2022 - val_accuracy: 0.7339\n",
      "Epoch 784/1000\n",
      "563/563 [==============================] - 1s 968us/step - loss: 0.2027 - accuracy: 0.7264 - val_loss: 0.2022 - val_accuracy: 0.7339\n",
      "Epoch 785/1000\n",
      "563/563 [==============================] - 1s 975us/step - loss: 0.2034 - accuracy: 0.7237 - val_loss: 0.2022 - val_accuracy: 0.7339\n",
      "Epoch 786/1000\n",
      "563/563 [==============================] - 1s 975us/step - loss: 0.2044 - accuracy: 0.7209 - val_loss: 0.2021 - val_accuracy: 0.7339\n",
      "Epoch 787/1000\n",
      "563/563 [==============================] - 1s 958us/step - loss: 0.2029 - accuracy: 0.7277 - val_loss: 0.2021 - val_accuracy: 0.7339\n",
      "Epoch 788/1000\n",
      "563/563 [==============================] - 1s 945us/step - loss: 0.2023 - accuracy: 0.7227 - val_loss: 0.2021 - val_accuracy: 0.7339\n",
      "Epoch 789/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2034 - accuracy: 0.7235 - val_loss: 0.2021 - val_accuracy: 0.7339\n",
      "Epoch 790/1000\n",
      "563/563 [==============================] - 1s 963us/step - loss: 0.2024 - accuracy: 0.7237 - val_loss: 0.2020 - val_accuracy: 0.7339\n",
      "Epoch 791/1000\n",
      "563/563 [==============================] - 1s 952us/step - loss: 0.2025 - accuracy: 0.7277 - val_loss: 0.2020 - val_accuracy: 0.7339\n",
      "Epoch 792/1000\n",
      "563/563 [==============================] - 1s 984us/step - loss: 0.2004 - accuracy: 0.7332 - val_loss: 0.2020 - val_accuracy: 0.7339\n",
      "Epoch 793/1000\n",
      "563/563 [==============================] - 1s 975us/step - loss: 0.2035 - accuracy: 0.7176 - val_loss: 0.2020 - val_accuracy: 0.7339\n",
      "Epoch 794/1000\n",
      "563/563 [==============================] - 1s 974us/step - loss: 0.2014 - accuracy: 0.7284 - val_loss: 0.2019 - val_accuracy: 0.7339\n",
      "Epoch 795/1000\n",
      "563/563 [==============================] - 1s 933us/step - loss: 0.2031 - accuracy: 0.7253 - val_loss: 0.2019 - val_accuracy: 0.7339\n",
      "Epoch 796/1000\n",
      "563/563 [==============================] - 1s 990us/step - loss: 0.2025 - accuracy: 0.7242 - val_loss: 0.2019 - val_accuracy: 0.7339\n",
      "Epoch 797/1000\n",
      "563/563 [==============================] - 1s 981us/step - loss: 0.2035 - accuracy: 0.7210 - val_loss: 0.2019 - val_accuracy: 0.7339\n",
      "Epoch 798/1000\n",
      "563/563 [==============================] - 1s 977us/step - loss: 0.2026 - accuracy: 0.7220 - val_loss: 0.2018 - val_accuracy: 0.7339\n",
      "Epoch 799/1000\n",
      "563/563 [==============================] - 1s 950us/step - loss: 0.2016 - accuracy: 0.7263 - val_loss: 0.2018 - val_accuracy: 0.7339\n",
      "Epoch 800/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2018 - accuracy: 0.7272 - val_loss: 0.2018 - val_accuracy: 0.7339\n",
      "Epoch 801/1000\n",
      "563/563 [==============================] - 1s 956us/step - loss: 0.2026 - accuracy: 0.7232 - val_loss: 0.2018 - val_accuracy: 0.7339\n",
      "Epoch 802/1000\n",
      "563/563 [==============================] - 1s 952us/step - loss: 0.2042 - accuracy: 0.7236 - val_loss: 0.2017 - val_accuracy: 0.7339\n",
      "Epoch 803/1000\n",
      "563/563 [==============================] - 1s 988us/step - loss: 0.2035 - accuracy: 0.7228 - val_loss: 0.2017 - val_accuracy: 0.7339\n",
      "Epoch 804/1000\n",
      "563/563 [==============================] - 1s 995us/step - loss: 0.2021 - accuracy: 0.7260 - val_loss: 0.2017 - val_accuracy: 0.7339\n",
      "Epoch 805/1000\n",
      "563/563 [==============================] - 1s 941us/step - loss: 0.2031 - accuracy: 0.7223 - val_loss: 0.2017 - val_accuracy: 0.7339\n",
      "Epoch 806/1000\n",
      "563/563 [==============================] - 1s 941us/step - loss: 0.2037 - accuracy: 0.7192 - val_loss: 0.2016 - val_accuracy: 0.7339\n",
      "Epoch 807/1000\n",
      "563/563 [==============================] - 1s 998us/step - loss: 0.2026 - accuracy: 0.7246 - val_loss: 0.2016 - val_accuracy: 0.7339\n",
      "Epoch 808/1000\n",
      "563/563 [==============================] - 1s 956us/step - loss: 0.2006 - accuracy: 0.7294 - val_loss: 0.2016 - val_accuracy: 0.7339\n",
      "Epoch 809/1000\n",
      "563/563 [==============================] - 1s 966us/step - loss: 0.2039 - accuracy: 0.7207 - val_loss: 0.2016 - val_accuracy: 0.7339\n",
      "Epoch 810/1000\n",
      "563/563 [==============================] - 1s 931us/step - loss: 0.2034 - accuracy: 0.7193 - val_loss: 0.2015 - val_accuracy: 0.7339\n",
      "Epoch 811/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2018 - accuracy: 0.7242 - val_loss: 0.2015 - val_accuracy: 0.7339\n",
      "Epoch 812/1000\n",
      "563/563 [==============================] - 1s 968us/step - loss: 0.2034 - accuracy: 0.7238 - val_loss: 0.2015 - val_accuracy: 0.7339\n",
      "Epoch 813/1000\n",
      "563/563 [==============================] - 1s 965us/step - loss: 0.2030 - accuracy: 0.7227 - val_loss: 0.2015 - val_accuracy: 0.7339\n",
      "Epoch 814/1000\n",
      "563/563 [==============================] - 1s 974us/step - loss: 0.2008 - accuracy: 0.7238 - val_loss: 0.2014 - val_accuracy: 0.7339\n",
      "Epoch 815/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2023 - accuracy: 0.7247 - val_loss: 0.2014 - val_accuracy: 0.7339\n",
      "Epoch 816/1000\n",
      "563/563 [==============================] - 1s 933us/step - loss: 0.2014 - accuracy: 0.7290 - val_loss: 0.2014 - val_accuracy: 0.7339\n",
      "Epoch 817/1000\n",
      "563/563 [==============================] - 1s 943us/step - loss: 0.2018 - accuracy: 0.7259 - val_loss: 0.2014 - val_accuracy: 0.7339\n",
      "Epoch 818/1000\n",
      "563/563 [==============================] - 1s 993us/step - loss: 0.2030 - accuracy: 0.7249 - val_loss: 0.2013 - val_accuracy: 0.7339\n",
      "Epoch 819/1000\n",
      "563/563 [==============================] - 1s 974us/step - loss: 0.2011 - accuracy: 0.7277 - val_loss: 0.2013 - val_accuracy: 0.7339\n",
      "Epoch 820/1000\n",
      "563/563 [==============================] - 1s 954us/step - loss: 0.2020 - accuracy: 0.7258 - val_loss: 0.2013 - val_accuracy: 0.7339\n",
      "Epoch 821/1000\n",
      "563/563 [==============================] - 1s 961us/step - loss: 0.2020 - accuracy: 0.7222 - val_loss: 0.2013 - val_accuracy: 0.7339\n",
      "Epoch 822/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2028 - accuracy: 0.7226 - val_loss: 0.2012 - val_accuracy: 0.7339\n",
      "Epoch 823/1000\n",
      "563/563 [==============================] - 1s 956us/step - loss: 0.2026 - accuracy: 0.7239 - val_loss: 0.2012 - val_accuracy: 0.7339\n",
      "Epoch 824/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2001 - accuracy: 0.7290 - val_loss: 0.2012 - val_accuracy: 0.7337\n",
      "Epoch 825/1000\n",
      "563/563 [==============================] - 1s 982us/step - loss: 0.2010 - accuracy: 0.7270 - val_loss: 0.2012 - val_accuracy: 0.7337\n",
      "Epoch 826/1000\n",
      "563/563 [==============================] - 1s 982us/step - loss: 0.2030 - accuracy: 0.7224 - val_loss: 0.2011 - val_accuracy: 0.7337\n",
      "Epoch 827/1000\n",
      "563/563 [==============================] - 1s 940us/step - loss: 0.2033 - accuracy: 0.7218 - val_loss: 0.2011 - val_accuracy: 0.7337\n",
      "Epoch 828/1000\n",
      "563/563 [==============================] - 1s 974us/step - loss: 0.2033 - accuracy: 0.7208 - val_loss: 0.2011 - val_accuracy: 0.7337\n",
      "Epoch 829/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2030 - accuracy: 0.7227 - val_loss: 0.2011 - val_accuracy: 0.7337\n",
      "Epoch 830/1000\n",
      "563/563 [==============================] - 1s 961us/step - loss: 0.2018 - accuracy: 0.7261 - val_loss: 0.2010 - val_accuracy: 0.7337\n",
      "Epoch 831/1000\n",
      "563/563 [==============================] - 1s 959us/step - loss: 0.2008 - accuracy: 0.7291 - val_loss: 0.2010 - val_accuracy: 0.7337\n",
      "Epoch 832/1000\n",
      "563/563 [==============================] - 1s 985us/step - loss: 0.2024 - accuracy: 0.7237 - val_loss: 0.2010 - val_accuracy: 0.7337\n",
      "Epoch 833/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2023 - accuracy: 0.7285 - val_loss: 0.2010 - val_accuracy: 0.7337\n",
      "Epoch 834/1000\n",
      "563/563 [==============================] - 1s 945us/step - loss: 0.2044 - accuracy: 0.7155 - val_loss: 0.2009 - val_accuracy: 0.7337\n",
      "Epoch 835/1000\n",
      "563/563 [==============================] - 1s 965us/step - loss: 0.2030 - accuracy: 0.7225 - val_loss: 0.2009 - val_accuracy: 0.7337\n",
      "Epoch 836/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2016 - accuracy: 0.7253 - val_loss: 0.2009 - val_accuracy: 0.7337\n",
      "Epoch 837/1000\n",
      "563/563 [==============================] - 1s 953us/step - loss: 0.2018 - accuracy: 0.7244 - val_loss: 0.2009 - val_accuracy: 0.7337\n",
      "Epoch 838/1000\n",
      "563/563 [==============================] - 1s 958us/step - loss: 0.2026 - accuracy: 0.7212 - val_loss: 0.2008 - val_accuracy: 0.7337\n",
      "Epoch 839/1000\n",
      "563/563 [==============================] - 1s 965us/step - loss: 0.2038 - accuracy: 0.7194 - val_loss: 0.2008 - val_accuracy: 0.7337\n",
      "Epoch 840/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2031 - accuracy: 0.7210 - val_loss: 0.2008 - val_accuracy: 0.7345\n",
      "Epoch 841/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2021 - accuracy: 0.7261 - val_loss: 0.2008 - val_accuracy: 0.7345\n",
      "Epoch 842/1000\n",
      "563/563 [==============================] - 1s 936us/step - loss: 0.2014 - accuracy: 0.7288 - val_loss: 0.2007 - val_accuracy: 0.7345\n",
      "Epoch 843/1000\n",
      "563/563 [==============================] - 1s 979us/step - loss: 0.2028 - accuracy: 0.7225 - val_loss: 0.2007 - val_accuracy: 0.7345\n",
      "Epoch 844/1000\n",
      "563/563 [==============================] - 1s 986us/step - loss: 0.2029 - accuracy: 0.7213 - val_loss: 0.2007 - val_accuracy: 0.7345\n",
      "Epoch 845/1000\n",
      "563/563 [==============================] - 1s 954us/step - loss: 0.2027 - accuracy: 0.7244 - val_loss: 0.2007 - val_accuracy: 0.7345\n",
      "Epoch 846/1000\n",
      "563/563 [==============================] - 1s 949us/step - loss: 0.2003 - accuracy: 0.7281 - val_loss: 0.2006 - val_accuracy: 0.7345\n",
      "Epoch 847/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2011 - accuracy: 0.7232 - val_loss: 0.2006 - val_accuracy: 0.7345\n",
      "Epoch 848/1000\n",
      "563/563 [==============================] - 1s 977us/step - loss: 0.2011 - accuracy: 0.7272 - val_loss: 0.2006 - val_accuracy: 0.7345\n",
      "Epoch 849/1000\n",
      "563/563 [==============================] - 1s 958us/step - loss: 0.2016 - accuracy: 0.7220 - val_loss: 0.2006 - val_accuracy: 0.7345\n",
      "Epoch 850/1000\n",
      "563/563 [==============================] - 1s 981us/step - loss: 0.2020 - accuracy: 0.7242 - val_loss: 0.2006 - val_accuracy: 0.7345\n",
      "Epoch 851/1000\n",
      "563/563 [==============================] - 1s 977us/step - loss: 0.2030 - accuracy: 0.7204 - val_loss: 0.2005 - val_accuracy: 0.7345\n",
      "Epoch 852/1000\n",
      "563/563 [==============================] - 1s 934us/step - loss: 0.2011 - accuracy: 0.7259 - val_loss: 0.2005 - val_accuracy: 0.7345\n",
      "Epoch 853/1000\n",
      "563/563 [==============================] - 1s 979us/step - loss: 0.2007 - accuracy: 0.7290 - val_loss: 0.2005 - val_accuracy: 0.7345\n",
      "Epoch 854/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2000 - accuracy: 0.7301 - val_loss: 0.2005 - val_accuracy: 0.7345\n",
      "Epoch 855/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2010 - accuracy: 0.7270 - val_loss: 0.2004 - val_accuracy: 0.7345\n",
      "Epoch 856/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2023 - accuracy: 0.7204 - val_loss: 0.2004 - val_accuracy: 0.7345\n",
      "Epoch 857/1000\n",
      "563/563 [==============================] - 1s 988us/step - loss: 0.2018 - accuracy: 0.7256 - val_loss: 0.2004 - val_accuracy: 0.7345\n",
      "Epoch 858/1000\n",
      "563/563 [==============================] - 1s 998us/step - loss: 0.2017 - accuracy: 0.7220 - val_loss: 0.2004 - val_accuracy: 0.7345\n",
      "Epoch 859/1000\n",
      "563/563 [==============================] - 1s 979us/step - loss: 0.1999 - accuracy: 0.7266 - val_loss: 0.2003 - val_accuracy: 0.7345\n",
      "Epoch 860/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2030 - accuracy: 0.7216 - val_loss: 0.2003 - val_accuracy: 0.7345\n",
      "Epoch 861/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2008 - accuracy: 0.7252 - val_loss: 0.2003 - val_accuracy: 0.7345\n",
      "Epoch 862/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2017 - accuracy: 0.7249 - val_loss: 0.2003 - val_accuracy: 0.7345\n",
      "Epoch 863/1000\n",
      "563/563 [==============================] - 1s 961us/step - loss: 0.2004 - accuracy: 0.7248 - val_loss: 0.2002 - val_accuracy: 0.7345\n",
      "Epoch 864/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2010 - accuracy: 0.7263 - val_loss: 0.2002 - val_accuracy: 0.7345\n",
      "Epoch 865/1000\n",
      "563/563 [==============================] - 1s 988us/step - loss: 0.2017 - accuracy: 0.7237 - val_loss: 0.2002 - val_accuracy: 0.7345\n",
      "Epoch 866/1000\n",
      "563/563 [==============================] - 1s 994us/step - loss: 0.1999 - accuracy: 0.7292 - val_loss: 0.2002 - val_accuracy: 0.7345\n",
      "Epoch 867/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2027 - accuracy: 0.7225 - val_loss: 0.2002 - val_accuracy: 0.7345\n",
      "Epoch 868/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2014 - accuracy: 0.7250 - val_loss: 0.2001 - val_accuracy: 0.7345\n",
      "Epoch 869/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2025 - accuracy: 0.7204 - val_loss: 0.2001 - val_accuracy: 0.7345\n",
      "Epoch 870/1000\n",
      "563/563 [==============================] - 1s 965us/step - loss: 0.2008 - accuracy: 0.7245 - val_loss: 0.2001 - val_accuracy: 0.7346\n",
      "Epoch 871/1000\n",
      "563/563 [==============================] - 1s 974us/step - loss: 0.1993 - accuracy: 0.7281 - val_loss: 0.2001 - val_accuracy: 0.7346\n",
      "Epoch 872/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2013 - accuracy: 0.7255 - val_loss: 0.2000 - val_accuracy: 0.7349\n",
      "Epoch 873/1000\n",
      "563/563 [==============================] - 1s 952us/step - loss: 0.2010 - accuracy: 0.7283 - val_loss: 0.2000 - val_accuracy: 0.7349\n",
      "Epoch 874/1000\n",
      "563/563 [==============================] - 1s 965us/step - loss: 0.2016 - accuracy: 0.7244 - val_loss: 0.2000 - val_accuracy: 0.7349\n",
      "Epoch 875/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2012 - accuracy: 0.7235 - val_loss: 0.2000 - val_accuracy: 0.7349\n",
      "Epoch 876/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2014 - accuracy: 0.7245 - val_loss: 0.1999 - val_accuracy: 0.7349\n",
      "Epoch 877/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2006 - accuracy: 0.7263 - val_loss: 0.1999 - val_accuracy: 0.7349\n",
      "Epoch 878/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2006 - accuracy: 0.7267 - val_loss: 0.1999 - val_accuracy: 0.7349\n",
      "Epoch 879/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2020 - accuracy: 0.7211 - val_loss: 0.1999 - val_accuracy: 0.7349\n",
      "Epoch 880/1000\n",
      "563/563 [==============================] - 1s 956us/step - loss: 0.2003 - accuracy: 0.7251 - val_loss: 0.1999 - val_accuracy: 0.7349\n",
      "Epoch 881/1000\n",
      "563/563 [==============================] - 1s 965us/step - loss: 0.2012 - accuracy: 0.7263 - val_loss: 0.1998 - val_accuracy: 0.7349\n",
      "Epoch 882/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2015 - accuracy: 0.7209 - val_loss: 0.1998 - val_accuracy: 0.7349\n",
      "Epoch 883/1000\n",
      "563/563 [==============================] - 1s 991us/step - loss: 0.1993 - accuracy: 0.7276 - val_loss: 0.1998 - val_accuracy: 0.7349\n",
      "Epoch 884/1000\n",
      "563/563 [==============================] - 1s 956us/step - loss: 0.2004 - accuracy: 0.7304 - val_loss: 0.1998 - val_accuracy: 0.7350\n",
      "Epoch 885/1000\n",
      "563/563 [==============================] - 1s 984us/step - loss: 0.2036 - accuracy: 0.7128 - val_loss: 0.1997 - val_accuracy: 0.7350\n",
      "Epoch 886/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1999 - accuracy: 0.7286 - val_loss: 0.1997 - val_accuracy: 0.7350\n",
      "Epoch 887/1000\n",
      "563/563 [==============================] - 1s 938us/step - loss: 0.1998 - accuracy: 0.7268 - val_loss: 0.1997 - val_accuracy: 0.7350\n",
      "Epoch 888/1000\n",
      "563/563 [==============================] - 1s 952us/step - loss: 0.2000 - accuracy: 0.7237 - val_loss: 0.1997 - val_accuracy: 0.7350\n",
      "Epoch 889/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1985 - accuracy: 0.7289 - val_loss: 0.1996 - val_accuracy: 0.7350\n",
      "Epoch 890/1000\n",
      "563/563 [==============================] - 1s 991us/step - loss: 0.2004 - accuracy: 0.7250 - val_loss: 0.1996 - val_accuracy: 0.7350\n",
      "Epoch 891/1000\n",
      "563/563 [==============================] - 1s 943us/step - loss: 0.1997 - accuracy: 0.7297 - val_loss: 0.1996 - val_accuracy: 0.7352\n",
      "Epoch 892/1000\n",
      "563/563 [==============================] - 1s 965us/step - loss: 0.1997 - accuracy: 0.7283 - val_loss: 0.1996 - val_accuracy: 0.7352\n",
      "Epoch 893/1000\n",
      "563/563 [==============================] - 1s 993us/step - loss: 0.2014 - accuracy: 0.7236 - val_loss: 0.1996 - val_accuracy: 0.7352\n",
      "Epoch 894/1000\n",
      "563/563 [==============================] - 1s 961us/step - loss: 0.1997 - accuracy: 0.7259 - val_loss: 0.1995 - val_accuracy: 0.7350\n",
      "Epoch 895/1000\n",
      "563/563 [==============================] - 1s 945us/step - loss: 0.2002 - accuracy: 0.7291 - val_loss: 0.1995 - val_accuracy: 0.7350\n",
      "Epoch 896/1000\n",
      "563/563 [==============================] - 1s 991us/step - loss: 0.1999 - accuracy: 0.7297 - val_loss: 0.1995 - val_accuracy: 0.7350\n",
      "Epoch 897/1000\n",
      "563/563 [==============================] - 1s 997us/step - loss: 0.2020 - accuracy: 0.7187 - val_loss: 0.1995 - val_accuracy: 0.7350\n",
      "Epoch 898/1000\n",
      "563/563 [==============================] - 1s 963us/step - loss: 0.1995 - accuracy: 0.7259 - val_loss: 0.1994 - val_accuracy: 0.7350\n",
      "Epoch 899/1000\n",
      "563/563 [==============================] - 1s 934us/step - loss: 0.2013 - accuracy: 0.7224 - val_loss: 0.1994 - val_accuracy: 0.7350\n",
      "Epoch 900/1000\n",
      "563/563 [==============================] - 1s 979us/step - loss: 0.1980 - accuracy: 0.7313 - val_loss: 0.1994 - val_accuracy: 0.7350\n",
      "Epoch 901/1000\n",
      "563/563 [==============================] - 1s 963us/step - loss: 0.2004 - accuracy: 0.7306 - val_loss: 0.1994 - val_accuracy: 0.7350\n",
      "Epoch 902/1000\n",
      "563/563 [==============================] - 1s 979us/step - loss: 0.2014 - accuracy: 0.7200 - val_loss: 0.1994 - val_accuracy: 0.7350\n",
      "Epoch 903/1000\n",
      "563/563 [==============================] - 1s 950us/step - loss: 0.1994 - accuracy: 0.7271 - val_loss: 0.1993 - val_accuracy: 0.7350\n",
      "Epoch 904/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2006 - accuracy: 0.7264 - val_loss: 0.1993 - val_accuracy: 0.7350\n",
      "Epoch 905/1000\n",
      "563/563 [==============================] - 1s 959us/step - loss: 0.2019 - accuracy: 0.7216 - val_loss: 0.1993 - val_accuracy: 0.7350\n",
      "Epoch 906/1000\n",
      "563/563 [==============================] - 1s 956us/step - loss: 0.2003 - accuracy: 0.7267 - val_loss: 0.1993 - val_accuracy: 0.7350\n",
      "Epoch 907/1000\n",
      "563/563 [==============================] - 1s 991us/step - loss: 0.1994 - accuracy: 0.7261 - val_loss: 0.1992 - val_accuracy: 0.7350\n",
      "Epoch 908/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2005 - accuracy: 0.7252 - val_loss: 0.1992 - val_accuracy: 0.7350\n",
      "Epoch 909/1000\n",
      "563/563 [==============================] - 1s 961us/step - loss: 0.1998 - accuracy: 0.7262 - val_loss: 0.1992 - val_accuracy: 0.7350\n",
      "Epoch 910/1000\n",
      "563/563 [==============================] - 1s 973us/step - loss: 0.2001 - accuracy: 0.7276 - val_loss: 0.1992 - val_accuracy: 0.7350\n",
      "Epoch 911/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1995 - accuracy: 0.7295 - val_loss: 0.1992 - val_accuracy: 0.7350\n",
      "Epoch 912/1000\n",
      "563/563 [==============================] - 1s 970us/step - loss: 0.2005 - accuracy: 0.7225 - val_loss: 0.1991 - val_accuracy: 0.7350\n",
      "Epoch 913/1000\n",
      "563/563 [==============================] - 1s 958us/step - loss: 0.2004 - accuracy: 0.7247 - val_loss: 0.1991 - val_accuracy: 0.7350\n",
      "Epoch 914/1000\n",
      "563/563 [==============================] - 1s 968us/step - loss: 0.1983 - accuracy: 0.7300 - val_loss: 0.1991 - val_accuracy: 0.7350\n",
      "Epoch 915/1000\n",
      "563/563 [==============================] - 1s 993us/step - loss: 0.2007 - accuracy: 0.7234 - val_loss: 0.1991 - val_accuracy: 0.7350\n",
      "Epoch 916/1000\n",
      "563/563 [==============================] - 1s 952us/step - loss: 0.1997 - accuracy: 0.7263 - val_loss: 0.1990 - val_accuracy: 0.7350\n",
      "Epoch 917/1000\n",
      "563/563 [==============================] - 1s 947us/step - loss: 0.2006 - accuracy: 0.7255 - val_loss: 0.1990 - val_accuracy: 0.7350\n",
      "Epoch 918/1000\n",
      "563/563 [==============================] - 1s 956us/step - loss: 0.2003 - accuracy: 0.7221 - val_loss: 0.1990 - val_accuracy: 0.7352\n",
      "Epoch 919/1000\n",
      "563/563 [==============================] - 1s 970us/step - loss: 0.2024 - accuracy: 0.7202 - val_loss: 0.1990 - val_accuracy: 0.7352\n",
      "Epoch 920/1000\n",
      "563/563 [==============================] - 1s 948us/step - loss: 0.1992 - accuracy: 0.7253 - val_loss: 0.1990 - val_accuracy: 0.7352\n",
      "Epoch 921/1000\n",
      "563/563 [==============================] - 1s 950us/step - loss: 0.2014 - accuracy: 0.7204 - val_loss: 0.1989 - val_accuracy: 0.7352\n",
      "Epoch 922/1000\n",
      "563/563 [==============================] - 1s 990us/step - loss: 0.2002 - accuracy: 0.7262 - val_loss: 0.1989 - val_accuracy: 0.7352\n",
      "Epoch 923/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2017 - accuracy: 0.7201 - val_loss: 0.1989 - val_accuracy: 0.7352\n",
      "Epoch 924/1000\n",
      "563/563 [==============================] - 1s 972us/step - loss: 0.2021 - accuracy: 0.7175 - val_loss: 0.1989 - val_accuracy: 0.7352\n",
      "Epoch 925/1000\n",
      "563/563 [==============================] - 1s 998us/step - loss: 0.1991 - accuracy: 0.7238 - val_loss: 0.1988 - val_accuracy: 0.7352\n",
      "Epoch 926/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2003 - accuracy: 0.7222 - val_loss: 0.1988 - val_accuracy: 0.7352\n",
      "Epoch 927/1000\n",
      "563/563 [==============================] - 1s 949us/step - loss: 0.2012 - accuracy: 0.7256 - val_loss: 0.1988 - val_accuracy: 0.7352\n",
      "Epoch 928/1000\n",
      "563/563 [==============================] - 1s 950us/step - loss: 0.2013 - accuracy: 0.7210 - val_loss: 0.1988 - val_accuracy: 0.7352\n",
      "Epoch 929/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1991 - accuracy: 0.7272 - val_loss: 0.1988 - val_accuracy: 0.7352\n",
      "Epoch 930/1000\n",
      "563/563 [==============================] - 1s 979us/step - loss: 0.1996 - accuracy: 0.7212 - val_loss: 0.1987 - val_accuracy: 0.7352\n",
      "Epoch 931/1000\n",
      "563/563 [==============================] - 1s 958us/step - loss: 0.2005 - accuracy: 0.7233 - val_loss: 0.1987 - val_accuracy: 0.7352\n",
      "Epoch 932/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2004 - accuracy: 0.7245 - val_loss: 0.1987 - val_accuracy: 0.7352\n",
      "Epoch 933/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2015 - accuracy: 0.7223 - val_loss: 0.1987 - val_accuracy: 0.7352\n",
      "Epoch 934/1000\n",
      "563/563 [==============================] - 1s 956us/step - loss: 0.2000 - accuracy: 0.7215 - val_loss: 0.1987 - val_accuracy: 0.7352\n",
      "Epoch 935/1000\n",
      "563/563 [==============================] - 1s 947us/step - loss: 0.2024 - accuracy: 0.7184 - val_loss: 0.1986 - val_accuracy: 0.7352\n",
      "Epoch 936/1000\n",
      "563/563 [==============================] - 1s 963us/step - loss: 0.1981 - accuracy: 0.7295 - val_loss: 0.1986 - val_accuracy: 0.7352\n",
      "Epoch 937/1000\n",
      "563/563 [==============================] - 1s 991us/step - loss: 0.1990 - accuracy: 0.7278 - val_loss: 0.1986 - val_accuracy: 0.7352\n",
      "Epoch 938/1000\n",
      "563/563 [==============================] - 1s 931us/step - loss: 0.1990 - accuracy: 0.7248 - val_loss: 0.1986 - val_accuracy: 0.7352\n",
      "Epoch 939/1000\n",
      "563/563 [==============================] - 1s 961us/step - loss: 0.2010 - accuracy: 0.7211 - val_loss: 0.1986 - val_accuracy: 0.7352\n",
      "Epoch 940/1000\n",
      "563/563 [==============================] - 1s 993us/step - loss: 0.2003 - accuracy: 0.7206 - val_loss: 0.1985 - val_accuracy: 0.7352\n",
      "Epoch 941/1000\n",
      "563/563 [==============================] - 1s 968us/step - loss: 0.1999 - accuracy: 0.7239 - val_loss: 0.1985 - val_accuracy: 0.7352\n",
      "Epoch 942/1000\n",
      "563/563 [==============================] - 1s 950us/step - loss: 0.1994 - accuracy: 0.7241 - val_loss: 0.1985 - val_accuracy: 0.7352\n",
      "Epoch 943/1000\n",
      "563/563 [==============================] - 1s 977us/step - loss: 0.1985 - accuracy: 0.7256 - val_loss: 0.1985 - val_accuracy: 0.7352\n",
      "Epoch 944/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2003 - accuracy: 0.7244 - val_loss: 0.1985 - val_accuracy: 0.7352\n",
      "Epoch 945/1000\n",
      "563/563 [==============================] - 1s 956us/step - loss: 0.2006 - accuracy: 0.7231 - val_loss: 0.1984 - val_accuracy: 0.7362\n",
      "Epoch 946/1000\n",
      "563/563 [==============================] - 1s 934us/step - loss: 0.1999 - accuracy: 0.7259 - val_loss: 0.1984 - val_accuracy: 0.7363\n",
      "Epoch 947/1000\n",
      "563/563 [==============================] - 1s 995us/step - loss: 0.1987 - accuracy: 0.7307 - val_loss: 0.1984 - val_accuracy: 0.7362\n",
      "Epoch 948/1000\n",
      "563/563 [==============================] - 1s 984us/step - loss: 0.2010 - accuracy: 0.7234 - val_loss: 0.1984 - val_accuracy: 0.7363\n",
      "Epoch 949/1000\n",
      "563/563 [==============================] - 1s 941us/step - loss: 0.1997 - accuracy: 0.7271 - val_loss: 0.1984 - val_accuracy: 0.7362\n",
      "Epoch 950/1000\n",
      "563/563 [==============================] - 1s 943us/step - loss: 0.1983 - accuracy: 0.7309 - val_loss: 0.1983 - val_accuracy: 0.7362\n",
      "Epoch 951/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2019 - accuracy: 0.7227 - val_loss: 0.1983 - val_accuracy: 0.7363\n",
      "Epoch 952/1000\n",
      "563/563 [==============================] - 1s 968us/step - loss: 0.2008 - accuracy: 0.7237 - val_loss: 0.1983 - val_accuracy: 0.7363\n",
      "Epoch 953/1000\n",
      "563/563 [==============================] - 1s 952us/step - loss: 0.2005 - accuracy: 0.7245 - val_loss: 0.1983 - val_accuracy: 0.7365\n",
      "Epoch 954/1000\n",
      "563/563 [==============================] - 1s 991us/step - loss: 0.1996 - accuracy: 0.7256 - val_loss: 0.1982 - val_accuracy: 0.7366\n",
      "Epoch 955/1000\n",
      "563/563 [==============================] - 1s 981us/step - loss: 0.1994 - accuracy: 0.7286 - val_loss: 0.1982 - val_accuracy: 0.7367\n",
      "Epoch 956/1000\n",
      "563/563 [==============================] - 1s 977us/step - loss: 0.1983 - accuracy: 0.7286 - val_loss: 0.1982 - val_accuracy: 0.7366\n",
      "Epoch 957/1000\n",
      "563/563 [==============================] - 1s 959us/step - loss: 0.2004 - accuracy: 0.7225 - val_loss: 0.1982 - val_accuracy: 0.7366\n",
      "Epoch 958/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2018 - accuracy: 0.7251 - val_loss: 0.1982 - val_accuracy: 0.7366\n",
      "Epoch 959/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2007 - accuracy: 0.7238 - val_loss: 0.1981 - val_accuracy: 0.7367\n",
      "Epoch 960/1000\n",
      "563/563 [==============================] - 1s 971us/step - loss: 0.1995 - accuracy: 0.7243 - val_loss: 0.1981 - val_accuracy: 0.7367\n",
      "Epoch 961/1000\n",
      "563/563 [==============================] - 1s 965us/step - loss: 0.2013 - accuracy: 0.7223 - val_loss: 0.1981 - val_accuracy: 0.7368\n",
      "Epoch 962/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1989 - accuracy: 0.7267 - val_loss: 0.1981 - val_accuracy: 0.7368\n",
      "Epoch 963/1000\n",
      "563/563 [==============================] - 1s 954us/step - loss: 0.2003 - accuracy: 0.7243 - val_loss: 0.1981 - val_accuracy: 0.7368\n",
      "Epoch 964/1000\n",
      "563/563 [==============================] - 1s 940us/step - loss: 0.2005 - accuracy: 0.7238 - val_loss: 0.1980 - val_accuracy: 0.7368\n",
      "Epoch 965/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1991 - accuracy: 0.7330 - val_loss: 0.1980 - val_accuracy: 0.7368\n",
      "Epoch 966/1000\n",
      "563/563 [==============================] - 1s 966us/step - loss: 0.2008 - accuracy: 0.7248 - val_loss: 0.1980 - val_accuracy: 0.7368\n",
      "Epoch 967/1000\n",
      "563/563 [==============================] - 1s 958us/step - loss: 0.2004 - accuracy: 0.7256 - val_loss: 0.1980 - val_accuracy: 0.7368\n",
      "Epoch 968/1000\n",
      "563/563 [==============================] - 1s 975us/step - loss: 0.2021 - accuracy: 0.7203 - val_loss: 0.1980 - val_accuracy: 0.7368\n",
      "Epoch 969/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2004 - accuracy: 0.7244 - val_loss: 0.1980 - val_accuracy: 0.7368\n",
      "Epoch 970/1000\n",
      "563/563 [==============================] - 1s 942us/step - loss: 0.1995 - accuracy: 0.7272 - val_loss: 0.1979 - val_accuracy: 0.7370\n",
      "Epoch 971/1000\n",
      "563/563 [==============================] - 1s 940us/step - loss: 0.1985 - accuracy: 0.7299 - val_loss: 0.1979 - val_accuracy: 0.7370\n",
      "Epoch 972/1000\n",
      "563/563 [==============================] - 1s 972us/step - loss: 0.1992 - accuracy: 0.7282 - val_loss: 0.1979 - val_accuracy: 0.7370\n",
      "Epoch 973/1000\n",
      "563/563 [==============================] - 1s 993us/step - loss: 0.1996 - accuracy: 0.7234 - val_loss: 0.1979 - val_accuracy: 0.7370\n",
      "Epoch 974/1000\n",
      "563/563 [==============================] - 1s 945us/step - loss: 0.2004 - accuracy: 0.7245 - val_loss: 0.1979 - val_accuracy: 0.7370\n",
      "Epoch 975/1000\n",
      "563/563 [==============================] - 1s 940us/step - loss: 0.1998 - accuracy: 0.7278 - val_loss: 0.1978 - val_accuracy: 0.7368\n",
      "Epoch 976/1000\n",
      "563/563 [==============================] - 1s 979us/step - loss: 0.2005 - accuracy: 0.7229 - val_loss: 0.1978 - val_accuracy: 0.7368\n",
      "Epoch 977/1000\n",
      "563/563 [==============================] - 1s 965us/step - loss: 0.1972 - accuracy: 0.7341 - val_loss: 0.1978 - val_accuracy: 0.7368\n",
      "Epoch 978/1000\n",
      "563/563 [==============================] - 1s 949us/step - loss: 0.1981 - accuracy: 0.7304 - val_loss: 0.1978 - val_accuracy: 0.7368\n",
      "Epoch 979/1000\n",
      "563/563 [==============================] - 1s 964us/step - loss: 0.1969 - accuracy: 0.7311 - val_loss: 0.1978 - val_accuracy: 0.7368\n",
      "Epoch 980/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1982 - accuracy: 0.7297 - val_loss: 0.1977 - val_accuracy: 0.7368\n",
      "Epoch 981/1000\n",
      "563/563 [==============================] - 1s 965us/step - loss: 0.1982 - accuracy: 0.7316 - val_loss: 0.1977 - val_accuracy: 0.7368\n",
      "Epoch 982/1000\n",
      "563/563 [==============================] - 1s 949us/step - loss: 0.2000 - accuracy: 0.7244 - val_loss: 0.1977 - val_accuracy: 0.7368\n",
      "Epoch 983/1000\n",
      "563/563 [==============================] - 1s 976us/step - loss: 0.1993 - accuracy: 0.7266 - val_loss: 0.1977 - val_accuracy: 0.7368\n",
      "Epoch 984/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1990 - accuracy: 0.7276 - val_loss: 0.1977 - val_accuracy: 0.7368\n",
      "Epoch 985/1000\n",
      "563/563 [==============================] - 1s 984us/step - loss: 0.1996 - accuracy: 0.7257 - val_loss: 0.1976 - val_accuracy: 0.7368\n",
      "Epoch 986/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1989 - accuracy: 0.7283 - val_loss: 0.1976 - val_accuracy: 0.7368\n",
      "Epoch 987/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1994 - accuracy: 0.7240 - val_loss: 0.1976 - val_accuracy: 0.7368\n",
      "Epoch 988/1000\n",
      "563/563 [==============================] - 1s 981us/step - loss: 0.1976 - accuracy: 0.7308 - val_loss: 0.1976 - val_accuracy: 0.7368\n",
      "Epoch 989/1000\n",
      "563/563 [==============================] - 1s 956us/step - loss: 0.1998 - accuracy: 0.7280 - val_loss: 0.1976 - val_accuracy: 0.7368\n",
      "Epoch 990/1000\n",
      "563/563 [==============================] - 1s 943us/step - loss: 0.1981 - accuracy: 0.7307 - val_loss: 0.1976 - val_accuracy: 0.7368\n",
      "Epoch 991/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1991 - accuracy: 0.7246 - val_loss: 0.1975 - val_accuracy: 0.7368\n",
      "Epoch 992/1000\n",
      "563/563 [==============================] - 1s 956us/step - loss: 0.1982 - accuracy: 0.7300 - val_loss: 0.1975 - val_accuracy: 0.7368\n",
      "Epoch 993/1000\n",
      "563/563 [==============================] - 1s 938us/step - loss: 0.2006 - accuracy: 0.7224 - val_loss: 0.1975 - val_accuracy: 0.7368\n",
      "Epoch 994/1000\n",
      "563/563 [==============================] - 1s 975us/step - loss: 0.1988 - accuracy: 0.7304 - val_loss: 0.1975 - val_accuracy: 0.7368\n",
      "Epoch 995/1000\n",
      "563/563 [==============================] - 1s 981us/step - loss: 0.1975 - accuracy: 0.7359 - val_loss: 0.1975 - val_accuracy: 0.7368\n",
      "Epoch 996/1000\n",
      "563/563 [==============================] - 1s 976us/step - loss: 0.1997 - accuracy: 0.7257 - val_loss: 0.1974 - val_accuracy: 0.7368\n",
      "Epoch 997/1000\n",
      "563/563 [==============================] - 1s 956us/step - loss: 0.1972 - accuracy: 0.7330 - val_loss: 0.1974 - val_accuracy: 0.7368\n",
      "Epoch 998/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1984 - accuracy: 0.7276 - val_loss: 0.1974 - val_accuracy: 0.7368\n",
      "Epoch 999/1000\n",
      "563/563 [==============================] - 1s 952us/step - loss: 0.2011 - accuracy: 0.7198 - val_loss: 0.1974 - val_accuracy: 0.7368\n",
      "Epoch 1000/1000\n",
      "563/563 [==============================] - 1s 952us/step - loss: 0.1984 - accuracy: 0.7290 - val_loss: 0.1974 - val_accuracy: 0.7368\n"
     ]
    }
   ],
   "source": [
    "# Fit the model using 50 epochs and the training data\n",
    "fit_model_A26 = nn_A26.fit(X_train_scaled, y_train, validation_split=0.3, epochs=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alternative Model 26 Results\n",
      "Loss: 0.19804491102695465, Accuracy: 0.7269970774650574\n"
     ]
    }
   ],
   "source": [
    "print(\"Alternative Model 26 Results\")\n",
    "# Evaluate the model loss and accuracy metrics using the evaluate method and the test data\n",
    "model_loss, model_accuracy = nn_A26.evaluate(X_test_scaled,y_test,verbose=0)\n",
    "# Display the model loss and accuracy results\n",
    "print(f\"Loss: {model_loss}, Accuracy: {model_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alternative Model 27+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "114"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the the number of inputs (features) to the model\n",
    "number_input_features = len(X_train.iloc[0])\n",
    "# Review the number of features\n",
    "number_input_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the number of neurons in the output layer\n",
    "number_output_neurons_A27 = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the number of hidden nodes for the first hidden layer\n",
    "hidden_nodes_layer1_A27 =  (number_input_features + number_output_neurons_A27) // 2\n",
    "# Review the number of hidden nodes in the first layer\n",
    "hidden_nodes_layer1_A27"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the number of hidden nodes for the second hidden layer\n",
    "hidden_nodes_layer2_A27 =  (hidden_nodes_layer1_A27 + number_output_neurons_A27) // 2\n",
    "# Review the number of hidden nodes in the second layer\n",
    "hidden_nodes_layer2_A27"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the number of hidden nodes for the third hidden layer\n",
    "hidden_nodes_layer3_A27 =  (hidden_nodes_layer2_A27 + number_output_neurons_A27) // 2\n",
    "# Review the number of hidden nodes in the third layer\n",
    "hidden_nodes_layer3_A27"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the number of hidden nodes for the fourth hidden layer\n",
    "hidden_nodes_layer4_A27 =  (hidden_nodes_layer3_A27 + number_output_neurons_A27) // 2\n",
    "# Review the number of hidden nodes in the fourth layer\n",
    "hidden_nodes_layer4_A27"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the number of hidden nodes for the fifth hidden layer\n",
    "hidden_nodes_layer5_A27 =  (hidden_nodes_layer4_A27 + number_output_neurons_A27) // 2\n",
    "# Review the number of hidden nodes in the fifth layer\n",
    "hidden_nodes_layer5_A27"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the number of hidden nodes for the Sixth hidden layer\n",
    "hidden_nodes_layer6_A27 =  (hidden_nodes_layer5_A27 + number_output_neurons_A27) // 2\n",
    "# Review the number of hidden nodes in the sixth layer\n",
    "hidden_nodes_layer6_A27"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_24 (Dense)             (None, 57)                6555      \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 29)                1682      \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (None, 15)                450       \n",
      "_________________________________________________________________\n",
      "dense_27 (Dense)             (None, 8)                 128       \n",
      "_________________________________________________________________\n",
      "dense_28 (Dense)             (None, 4)                 36        \n",
      "_________________________________________________________________\n",
      "dense_29 (Dense)             (None, 2)                 10        \n",
      "_________________________________________________________________\n",
      "dense_30 (Dense)             (None, 1)                 3         \n",
      "=================================================================\n",
      "Total params: 8,864\n",
      "Trainable params: 8,864\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Create and Display the Sequential Model Instance \n",
    "# for Model A27\n",
    "nn_A27 = Sequential() \n",
    "\n",
    "# Add the first hidden layer\n",
    "nn_A27.add(Dense(units=hidden_nodes_layer1_A27, input_dim=number_input_features, activation=\"relu\"))\n",
    "# Add the second hidden layer\n",
    "nn_A27.add(Dense(units=hidden_nodes_layer2_A27, input_dim=number_input_features, activation=\"relu\"))\n",
    "# Add the third hidden layer\n",
    "nn_A27.add(Dense(units=hidden_nodes_layer3_A27, input_dim=number_input_features, activation=\"relu\"))\n",
    "# Add the fourth hidden layer\n",
    "nn_A27.add(Dense(units=hidden_nodes_layer4_A27, input_dim=number_input_features, activation=\"relu\"))\n",
    "# Add the fifth hidden layer\n",
    "nn_A27.add(Dense(units=hidden_nodes_layer5_A27, input_dim=number_input_features, activation=\"relu\"))\n",
    "# Add the sixth hidden layer\n",
    "nn_A27.add(Dense(units=hidden_nodes_layer6_A27, activation=\"relu\"))\n",
    "\n",
    "# Add the output layer to the model specifying the number of output neurons and activation function\n",
    "nn_A27.add(Dense(units=number_output_neurons_A27, activation=\"sigmoid\"))\n",
    "\n",
    "# Display the Sequential model summary\n",
    "nn_A27.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the Sequential model\n",
    "nn_A27.compile(loss=\"mse\", optimizer=\"rmsprop\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2220 - accuracy: 0.6488 - val_loss: 0.1857 - val_accuracy: 0.7370\n",
      "Epoch 2/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1886 - accuracy: 0.7258 - val_loss: 0.1838 - val_accuracy: 0.7370\n",
      "Epoch 3/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1868 - accuracy: 0.7259 - val_loss: 0.1828 - val_accuracy: 0.7371\n",
      "Epoch 4/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1858 - accuracy: 0.7235 - val_loss: 0.1832 - val_accuracy: 0.7362\n",
      "Epoch 5/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1845 - accuracy: 0.7296 - val_loss: 0.1823 - val_accuracy: 0.7376\n",
      "Epoch 6/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1852 - accuracy: 0.7281 - val_loss: 0.1831 - val_accuracy: 0.7372\n",
      "Epoch 7/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1822 - accuracy: 0.7354 - val_loss: 0.1832 - val_accuracy: 0.7376\n",
      "Epoch 8/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1832 - accuracy: 0.7346 - val_loss: 0.1836 - val_accuracy: 0.7341\n",
      "Epoch 9/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1849 - accuracy: 0.7298 - val_loss: 0.1834 - val_accuracy: 0.7388\n",
      "Epoch 10/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1844 - accuracy: 0.7312 - val_loss: 0.1833 - val_accuracy: 0.7380\n",
      "Epoch 11/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1810 - accuracy: 0.7371 - val_loss: 0.1836 - val_accuracy: 0.7361\n",
      "Epoch 12/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1849 - accuracy: 0.7320 - val_loss: 0.1834 - val_accuracy: 0.7388\n",
      "Epoch 13/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1799 - accuracy: 0.7402 - val_loss: 0.1829 - val_accuracy: 0.7357\n",
      "Epoch 14/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1828 - accuracy: 0.7336 - val_loss: 0.1841 - val_accuracy: 0.7337\n",
      "Epoch 15/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1820 - accuracy: 0.7366 - val_loss: 0.1831 - val_accuracy: 0.7356\n",
      "Epoch 16/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1823 - accuracy: 0.7349 - val_loss: 0.1823 - val_accuracy: 0.7380\n",
      "Epoch 17/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1837 - accuracy: 0.7333 - val_loss: 0.1849 - val_accuracy: 0.7327\n",
      "Epoch 18/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1811 - accuracy: 0.7359 - val_loss: 0.1836 - val_accuracy: 0.7348\n",
      "Epoch 19/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1842 - accuracy: 0.7289 - val_loss: 0.1822 - val_accuracy: 0.7370\n",
      "Epoch 20/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1779 - accuracy: 0.7410 - val_loss: 0.1838 - val_accuracy: 0.7335\n",
      "Epoch 21/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1826 - accuracy: 0.7315 - val_loss: 0.1829 - val_accuracy: 0.7336\n",
      "Epoch 22/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1840 - accuracy: 0.7264 - val_loss: 0.1827 - val_accuracy: 0.7337\n",
      "Epoch 23/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1842 - accuracy: 0.7292 - val_loss: 0.1827 - val_accuracy: 0.7410\n",
      "Epoch 24/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1804 - accuracy: 0.7358 - val_loss: 0.1831 - val_accuracy: 0.7419\n",
      "Epoch 25/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1826 - accuracy: 0.7356 - val_loss: 0.1829 - val_accuracy: 0.7388\n",
      "Epoch 26/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1821 - accuracy: 0.7366 - val_loss: 0.1829 - val_accuracy: 0.7393\n",
      "Epoch 27/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1828 - accuracy: 0.7294 - val_loss: 0.1828 - val_accuracy: 0.7400\n",
      "Epoch 28/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1792 - accuracy: 0.7395 - val_loss: 0.1830 - val_accuracy: 0.7396\n",
      "Epoch 29/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1797 - accuracy: 0.7384 - val_loss: 0.1836 - val_accuracy: 0.7367\n",
      "Epoch 30/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1795 - accuracy: 0.7388 - val_loss: 0.1837 - val_accuracy: 0.7359\n",
      "Epoch 31/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1821 - accuracy: 0.7289 - val_loss: 0.1830 - val_accuracy: 0.7363\n",
      "Epoch 32/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1790 - accuracy: 0.7410 - val_loss: 0.1835 - val_accuracy: 0.7391\n",
      "Epoch 33/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1810 - accuracy: 0.7323 - val_loss: 0.1841 - val_accuracy: 0.7374\n",
      "Epoch 34/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1795 - accuracy: 0.7404 - val_loss: 0.1857 - val_accuracy: 0.7359\n",
      "Epoch 35/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1815 - accuracy: 0.7353 - val_loss: 0.1834 - val_accuracy: 0.7356\n",
      "Epoch 36/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1816 - accuracy: 0.7345 - val_loss: 0.1838 - val_accuracy: 0.7409\n",
      "Epoch 37/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1842 - accuracy: 0.7329 - val_loss: 0.1832 - val_accuracy: 0.7378\n",
      "Epoch 38/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1834 - accuracy: 0.7309 - val_loss: 0.1825 - val_accuracy: 0.7402\n",
      "Epoch 39/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1798 - accuracy: 0.7376 - val_loss: 0.1826 - val_accuracy: 0.7383\n",
      "Epoch 40/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1812 - accuracy: 0.7364 - val_loss: 0.1821 - val_accuracy: 0.7392\n",
      "Epoch 41/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1824 - accuracy: 0.7325 - val_loss: 0.1835 - val_accuracy: 0.7384\n",
      "Epoch 42/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1831 - accuracy: 0.7317 - val_loss: 0.1829 - val_accuracy: 0.7380\n",
      "Epoch 43/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1783 - accuracy: 0.7446 - val_loss: 0.1838 - val_accuracy: 0.7388\n",
      "Epoch 44/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1812 - accuracy: 0.7378 - val_loss: 0.1834 - val_accuracy: 0.7387\n",
      "Epoch 45/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1811 - accuracy: 0.7397 - val_loss: 0.1836 - val_accuracy: 0.7375\n",
      "Epoch 46/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1791 - accuracy: 0.7404 - val_loss: 0.1832 - val_accuracy: 0.7392\n",
      "Epoch 47/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1815 - accuracy: 0.7362 - val_loss: 0.1828 - val_accuracy: 0.7396\n",
      "Epoch 48/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1807 - accuracy: 0.7383 - val_loss: 0.1825 - val_accuracy: 0.7385\n",
      "Epoch 49/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1797 - accuracy: 0.7381 - val_loss: 0.1820 - val_accuracy: 0.7401\n",
      "Epoch 50/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1801 - accuracy: 0.7392 - val_loss: 0.1830 - val_accuracy: 0.7406\n",
      "Epoch 51/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1802 - accuracy: 0.7355 - val_loss: 0.1832 - val_accuracy: 0.7387\n",
      "Epoch 52/1000\n",
      "563/563 [==============================] - 1s 998us/step - loss: 0.1783 - accuracy: 0.7390 - val_loss: 0.1829 - val_accuracy: 0.7400\n",
      "Epoch 53/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1815 - accuracy: 0.7381 - val_loss: 0.1831 - val_accuracy: 0.7400\n",
      "Epoch 54/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1807 - accuracy: 0.7366 - val_loss: 0.1830 - val_accuracy: 0.7409\n",
      "Epoch 55/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1801 - accuracy: 0.7375 - val_loss: 0.1828 - val_accuracy: 0.7398\n",
      "Epoch 56/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1797 - accuracy: 0.7377 - val_loss: 0.1828 - val_accuracy: 0.7403\n",
      "Epoch 57/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1799 - accuracy: 0.7376 - val_loss: 0.1822 - val_accuracy: 0.7405\n",
      "Epoch 58/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1817 - accuracy: 0.7377 - val_loss: 0.1830 - val_accuracy: 0.7400\n",
      "Epoch 59/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1826 - accuracy: 0.7367 - val_loss: 0.1819 - val_accuracy: 0.7398\n",
      "Epoch 60/1000\n",
      "563/563 [==============================] - 1s 999us/step - loss: 0.1810 - accuracy: 0.7370 - val_loss: 0.1830 - val_accuracy: 0.7393\n",
      "Epoch 61/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1796 - accuracy: 0.7394 - val_loss: 0.1824 - val_accuracy: 0.7401\n",
      "Epoch 62/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1795 - accuracy: 0.7385 - val_loss: 0.1858 - val_accuracy: 0.7214\n",
      "Epoch 63/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1822 - accuracy: 0.7247 - val_loss: 0.1869 - val_accuracy: 0.7374\n",
      "Epoch 64/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1787 - accuracy: 0.7392 - val_loss: 0.1829 - val_accuracy: 0.7385\n",
      "Epoch 65/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1807 - accuracy: 0.7380 - val_loss: 0.1835 - val_accuracy: 0.7379\n",
      "Epoch 66/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1804 - accuracy: 0.7379 - val_loss: 0.1839 - val_accuracy: 0.7376\n",
      "Epoch 67/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1812 - accuracy: 0.7371 - val_loss: 0.1833 - val_accuracy: 0.7400\n",
      "Epoch 68/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1814 - accuracy: 0.7356 - val_loss: 0.1828 - val_accuracy: 0.7394\n",
      "Epoch 69/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1813 - accuracy: 0.7369 - val_loss: 0.1838 - val_accuracy: 0.7392\n",
      "Epoch 70/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1784 - accuracy: 0.7418 - val_loss: 0.1836 - val_accuracy: 0.7387\n",
      "Epoch 71/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1791 - accuracy: 0.7397 - val_loss: 0.1828 - val_accuracy: 0.7387\n",
      "Epoch 72/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1819 - accuracy: 0.7348 - val_loss: 0.1832 - val_accuracy: 0.7387\n",
      "Epoch 73/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1814 - accuracy: 0.7357 - val_loss: 0.1828 - val_accuracy: 0.7411\n",
      "Epoch 74/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1801 - accuracy: 0.7409 - val_loss: 0.1849 - val_accuracy: 0.7384\n",
      "Epoch 75/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1806 - accuracy: 0.7408 - val_loss: 0.1836 - val_accuracy: 0.7402\n",
      "Epoch 76/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1794 - accuracy: 0.7376 - val_loss: 0.1839 - val_accuracy: 0.7401\n",
      "Epoch 77/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1800 - accuracy: 0.7391 - val_loss: 0.1835 - val_accuracy: 0.7368\n",
      "Epoch 78/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1803 - accuracy: 0.7358 - val_loss: 0.1836 - val_accuracy: 0.7406\n",
      "Epoch 79/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1824 - accuracy: 0.7367 - val_loss: 0.1835 - val_accuracy: 0.7378\n",
      "Epoch 80/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1781 - accuracy: 0.7411 - val_loss: 0.1842 - val_accuracy: 0.7396\n",
      "Epoch 81/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1840 - accuracy: 0.7309 - val_loss: 0.1835 - val_accuracy: 0.7400\n",
      "Epoch 82/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1783 - accuracy: 0.7437 - val_loss: 0.1841 - val_accuracy: 0.7385\n",
      "Epoch 83/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1805 - accuracy: 0.7387 - val_loss: 0.1842 - val_accuracy: 0.7366\n",
      "Epoch 84/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1795 - accuracy: 0.7407 - val_loss: 0.1856 - val_accuracy: 0.7384\n",
      "Epoch 85/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1813 - accuracy: 0.7355 - val_loss: 0.1837 - val_accuracy: 0.7411\n",
      "Epoch 86/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1825 - accuracy: 0.7319 - val_loss: 0.1832 - val_accuracy: 0.7409\n",
      "Epoch 87/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1794 - accuracy: 0.7407 - val_loss: 0.1837 - val_accuracy: 0.7410\n",
      "Epoch 88/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1804 - accuracy: 0.7363 - val_loss: 0.1831 - val_accuracy: 0.7398\n",
      "Epoch 89/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1793 - accuracy: 0.7402 - val_loss: 0.1848 - val_accuracy: 0.7406\n",
      "Epoch 90/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1790 - accuracy: 0.7391 - val_loss: 0.1828 - val_accuracy: 0.7415\n",
      "Epoch 91/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1793 - accuracy: 0.7401 - val_loss: 0.1829 - val_accuracy: 0.7407\n",
      "Epoch 92/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1778 - accuracy: 0.7422 - val_loss: 0.1857 - val_accuracy: 0.7403\n",
      "Epoch 93/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1798 - accuracy: 0.7390 - val_loss: 0.1866 - val_accuracy: 0.7375\n",
      "Epoch 94/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1791 - accuracy: 0.7410 - val_loss: 0.1846 - val_accuracy: 0.7403\n",
      "Epoch 95/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1811 - accuracy: 0.7352 - val_loss: 0.1829 - val_accuracy: 0.7411\n",
      "Epoch 96/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1808 - accuracy: 0.7390 - val_loss: 0.1831 - val_accuracy: 0.7397\n",
      "Epoch 97/1000\n",
      "563/563 [==============================] - 1s 993us/step - loss: 0.1813 - accuracy: 0.7322 - val_loss: 0.1835 - val_accuracy: 0.7392\n",
      "Epoch 98/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1831 - accuracy: 0.7333 - val_loss: 0.1834 - val_accuracy: 0.7407\n",
      "Epoch 99/1000\n",
      "563/563 [==============================] - 1s 986us/step - loss: 0.1800 - accuracy: 0.7360 - val_loss: 0.1833 - val_accuracy: 0.7402\n",
      "Epoch 100/1000\n",
      "563/563 [==============================] - 1s 956us/step - loss: 0.1808 - accuracy: 0.7367 - val_loss: 0.1830 - val_accuracy: 0.7411\n",
      "Epoch 101/1000\n",
      "563/563 [==============================] - 1s 995us/step - loss: 0.1793 - accuracy: 0.7385 - val_loss: 0.1847 - val_accuracy: 0.7397\n",
      "Epoch 102/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1794 - accuracy: 0.7395 - val_loss: 0.1827 - val_accuracy: 0.7410\n",
      "Epoch 103/1000\n",
      "563/563 [==============================] - 1s 966us/step - loss: 0.1803 - accuracy: 0.7370 - val_loss: 0.1831 - val_accuracy: 0.7403\n",
      "Epoch 104/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1839 - accuracy: 0.7297 - val_loss: 0.1851 - val_accuracy: 0.7389\n",
      "Epoch 105/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1804 - accuracy: 0.7360 - val_loss: 0.1831 - val_accuracy: 0.7407\n",
      "Epoch 106/1000\n",
      "563/563 [==============================] - 1s 971us/step - loss: 0.1812 - accuracy: 0.7366 - val_loss: 0.1839 - val_accuracy: 0.7387\n",
      "Epoch 107/1000\n",
      "563/563 [==============================] - 1s 984us/step - loss: 0.1805 - accuracy: 0.7357 - val_loss: 0.1831 - val_accuracy: 0.7381\n",
      "Epoch 108/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1810 - accuracy: 0.7380 - val_loss: 0.1828 - val_accuracy: 0.7407\n",
      "Epoch 109/1000\n",
      "563/563 [==============================] - 1s 997us/step - loss: 0.1804 - accuracy: 0.7346 - val_loss: 0.1835 - val_accuracy: 0.7393\n",
      "Epoch 110/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1792 - accuracy: 0.7407 - val_loss: 0.1836 - val_accuracy: 0.7397\n",
      "Epoch 111/1000\n",
      "563/563 [==============================] - 1s 977us/step - loss: 0.1805 - accuracy: 0.7395 - val_loss: 0.1838 - val_accuracy: 0.7393\n",
      "Epoch 112/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1798 - accuracy: 0.7354 - val_loss: 0.1833 - val_accuracy: 0.7389\n",
      "Epoch 113/1000\n",
      "563/563 [==============================] - 1s 988us/step - loss: 0.1798 - accuracy: 0.7335 - val_loss: 0.1829 - val_accuracy: 0.7402\n",
      "Epoch 114/1000\n",
      "563/563 [==============================] - 1s 993us/step - loss: 0.1785 - accuracy: 0.7424 - val_loss: 0.1826 - val_accuracy: 0.7394\n",
      "Epoch 115/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1811 - accuracy: 0.7348 - val_loss: 0.1837 - val_accuracy: 0.7394\n",
      "Epoch 116/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1824 - accuracy: 0.7336 - val_loss: 0.1840 - val_accuracy: 0.7388\n",
      "Epoch 117/1000\n",
      "563/563 [==============================] - 1s 973us/step - loss: 0.1792 - accuracy: 0.7389 - val_loss: 0.1838 - val_accuracy: 0.7402\n",
      "Epoch 118/1000\n",
      "563/563 [==============================] - 1s 974us/step - loss: 0.1783 - accuracy: 0.7414 - val_loss: 0.1834 - val_accuracy: 0.7405\n",
      "Epoch 119/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1800 - accuracy: 0.7398 - val_loss: 0.1855 - val_accuracy: 0.7375\n",
      "Epoch 120/1000\n",
      "563/563 [==============================] - 1s 991us/step - loss: 0.1782 - accuracy: 0.7433 - val_loss: 0.1834 - val_accuracy: 0.7391\n",
      "Epoch 121/1000\n",
      "563/563 [==============================] - 1s 997us/step - loss: 0.1799 - accuracy: 0.7361 - val_loss: 0.1849 - val_accuracy: 0.7388\n",
      "Epoch 122/1000\n",
      "563/563 [==============================] - 1s 988us/step - loss: 0.1821 - accuracy: 0.7340 - val_loss: 0.1833 - val_accuracy: 0.7383\n",
      "Epoch 123/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1820 - accuracy: 0.7365 - val_loss: 0.1840 - val_accuracy: 0.7371\n",
      "Epoch 124/1000\n",
      "563/563 [==============================] - 1s 991us/step - loss: 0.1819 - accuracy: 0.7346 - val_loss: 0.1848 - val_accuracy: 0.7376\n",
      "Epoch 125/1000\n",
      "563/563 [==============================] - 1s 995us/step - loss: 0.1800 - accuracy: 0.7407 - val_loss: 0.1829 - val_accuracy: 0.7383\n",
      "Epoch 126/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1791 - accuracy: 0.7377 - val_loss: 0.1844 - val_accuracy: 0.7388\n",
      "Epoch 127/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1824 - accuracy: 0.7325 - val_loss: 0.1845 - val_accuracy: 0.7398\n",
      "Epoch 128/1000\n",
      "563/563 [==============================] - 1s 970us/step - loss: 0.1818 - accuracy: 0.7354 - val_loss: 0.1840 - val_accuracy: 0.7381\n",
      "Epoch 129/1000\n",
      "563/563 [==============================] - 1s 959us/step - loss: 0.1811 - accuracy: 0.7346 - val_loss: 0.1827 - val_accuracy: 0.7393\n",
      "Epoch 130/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1787 - accuracy: 0.7413 - val_loss: 0.1838 - val_accuracy: 0.7357\n",
      "Epoch 131/1000\n",
      "563/563 [==============================] - 1s 981us/step - loss: 0.1787 - accuracy: 0.7397 - val_loss: 0.1820 - val_accuracy: 0.7409\n",
      "Epoch 132/1000\n",
      "563/563 [==============================] - 1s 984us/step - loss: 0.1795 - accuracy: 0.7394 - val_loss: 0.1833 - val_accuracy: 0.7393\n",
      "Epoch 133/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1810 - accuracy: 0.7360 - val_loss: 0.1834 - val_accuracy: 0.7401\n",
      "Epoch 134/1000\n",
      "563/563 [==============================] - 1s 997us/step - loss: 0.1822 - accuracy: 0.7338 - val_loss: 0.1834 - val_accuracy: 0.7361\n",
      "Epoch 135/1000\n",
      "563/563 [==============================] - 1s 997us/step - loss: 0.1799 - accuracy: 0.7366 - val_loss: 0.1831 - val_accuracy: 0.7407\n",
      "Epoch 136/1000\n",
      "563/563 [==============================] - 1s 997us/step - loss: 0.1808 - accuracy: 0.7372 - val_loss: 0.1836 - val_accuracy: 0.7376\n",
      "Epoch 137/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1819 - accuracy: 0.7338 - val_loss: 0.1834 - val_accuracy: 0.7411\n",
      "Epoch 138/1000\n",
      "563/563 [==============================] - 1s 966us/step - loss: 0.1811 - accuracy: 0.7378 - val_loss: 0.1829 - val_accuracy: 0.7392\n",
      "Epoch 139/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1788 - accuracy: 0.7411 - val_loss: 0.1843 - val_accuracy: 0.7380\n",
      "Epoch 140/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1812 - accuracy: 0.7377 - val_loss: 0.1826 - val_accuracy: 0.7402\n",
      "Epoch 141/1000\n",
      "563/563 [==============================] - 1s 981us/step - loss: 0.1793 - accuracy: 0.7408 - val_loss: 0.1838 - val_accuracy: 0.7378\n",
      "Epoch 142/1000\n",
      "563/563 [==============================] - 1s 975us/step - loss: 0.1813 - accuracy: 0.7365 - val_loss: 0.1839 - val_accuracy: 0.7409\n",
      "Epoch 143/1000\n",
      "563/563 [==============================] - 1s 999us/step - loss: 0.1819 - accuracy: 0.7346 - val_loss: 0.1826 - val_accuracy: 0.7400\n",
      "Epoch 144/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1795 - accuracy: 0.7400 - val_loss: 0.1836 - val_accuracy: 0.7389\n",
      "Epoch 145/1000\n",
      "563/563 [==============================] - 1s 972us/step - loss: 0.1775 - accuracy: 0.7427 - val_loss: 0.1829 - val_accuracy: 0.7367\n",
      "Epoch 146/1000\n",
      "563/563 [==============================] - 1s 966us/step - loss: 0.1790 - accuracy: 0.7407 - val_loss: 0.1852 - val_accuracy: 0.7380\n",
      "Epoch 147/1000\n",
      "563/563 [==============================] - 1s 988us/step - loss: 0.1815 - accuracy: 0.7354 - val_loss: 0.1848 - val_accuracy: 0.7397\n",
      "Epoch 148/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1800 - accuracy: 0.7390 - val_loss: 0.1820 - val_accuracy: 0.7398\n",
      "Epoch 149/1000\n",
      "563/563 [==============================] - 1s 978us/step - loss: 0.1770 - accuracy: 0.7458 - val_loss: 0.1836 - val_accuracy: 0.7387\n",
      "Epoch 150/1000\n",
      "563/563 [==============================] - 1s 975us/step - loss: 0.1786 - accuracy: 0.7376 - val_loss: 0.1827 - val_accuracy: 0.7400\n",
      "Epoch 151/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1811 - accuracy: 0.7356 - val_loss: 0.1848 - val_accuracy: 0.7371\n",
      "Epoch 152/1000\n",
      "563/563 [==============================] - 1s 986us/step - loss: 0.1808 - accuracy: 0.7355 - val_loss: 0.1820 - val_accuracy: 0.7391\n",
      "Epoch 153/1000\n",
      "563/563 [==============================] - 1s 988us/step - loss: 0.1792 - accuracy: 0.7374 - val_loss: 0.1834 - val_accuracy: 0.7376\n",
      "Epoch 154/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1803 - accuracy: 0.7384 - val_loss: 0.1833 - val_accuracy: 0.7363\n",
      "Epoch 155/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1818 - accuracy: 0.7344 - val_loss: 0.1834 - val_accuracy: 0.7384\n",
      "Epoch 156/1000\n",
      "563/563 [==============================] - 1s 993us/step - loss: 0.1807 - accuracy: 0.7361 - val_loss: 0.1826 - val_accuracy: 0.7409\n",
      "Epoch 157/1000\n",
      "563/563 [==============================] - 1s 972us/step - loss: 0.1798 - accuracy: 0.7399 - val_loss: 0.1824 - val_accuracy: 0.7391\n",
      "Epoch 158/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1815 - accuracy: 0.7350 - val_loss: 0.1829 - val_accuracy: 0.7387\n",
      "Epoch 159/1000\n",
      "563/563 [==============================] - 1s 997us/step - loss: 0.1808 - accuracy: 0.7346 - val_loss: 0.1824 - val_accuracy: 0.7391\n",
      "Epoch 160/1000\n",
      "563/563 [==============================] - 1s 995us/step - loss: 0.1787 - accuracy: 0.7415 - val_loss: 0.1830 - val_accuracy: 0.7394\n",
      "Epoch 161/1000\n",
      "563/563 [==============================] - 1s 977us/step - loss: 0.1802 - accuracy: 0.7380 - val_loss: 0.1828 - val_accuracy: 0.7379\n",
      "Epoch 162/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1806 - accuracy: 0.7367 - val_loss: 0.1833 - val_accuracy: 0.7407\n",
      "Epoch 163/1000\n",
      "563/563 [==============================] - 1s 991us/step - loss: 0.1816 - accuracy: 0.7371 - val_loss: 0.1835 - val_accuracy: 0.7391\n",
      "Epoch 164/1000\n",
      "563/563 [==============================] - 1s 975us/step - loss: 0.1783 - accuracy: 0.7431 - val_loss: 0.1832 - val_accuracy: 0.7381\n",
      "Epoch 165/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1799 - accuracy: 0.7390 - val_loss: 0.1835 - val_accuracy: 0.7396\n",
      "Epoch 166/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1803 - accuracy: 0.7395 - val_loss: 0.1833 - val_accuracy: 0.7374\n",
      "Epoch 167/1000\n",
      "563/563 [==============================] - 1s 961us/step - loss: 0.1802 - accuracy: 0.7374 - val_loss: 0.1826 - val_accuracy: 0.7394\n",
      "Epoch 168/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1818 - accuracy: 0.7334 - val_loss: 0.1827 - val_accuracy: 0.7394\n",
      "Epoch 169/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1814 - accuracy: 0.7328 - val_loss: 0.1824 - val_accuracy: 0.7388\n",
      "Epoch 170/1000\n",
      "563/563 [==============================] - 1s 963us/step - loss: 0.1797 - accuracy: 0.7382 - val_loss: 0.1830 - val_accuracy: 0.7366\n",
      "Epoch 171/1000\n",
      "563/563 [==============================] - 1s 990us/step - loss: 0.1822 - accuracy: 0.7336 - val_loss: 0.1841 - val_accuracy: 0.7388\n",
      "Epoch 172/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1836 - accuracy: 0.7325 - val_loss: 0.1819 - val_accuracy: 0.7389\n",
      "Epoch 173/1000\n",
      "563/563 [==============================] - 1s 986us/step - loss: 0.1810 - accuracy: 0.7348 - val_loss: 0.1824 - val_accuracy: 0.7397\n",
      "Epoch 174/1000\n",
      "563/563 [==============================] - 1s 977us/step - loss: 0.1784 - accuracy: 0.7406 - val_loss: 0.1830 - val_accuracy: 0.7384\n",
      "Epoch 175/1000\n",
      "563/563 [==============================] - 1s 968us/step - loss: 0.1806 - accuracy: 0.7381 - val_loss: 0.1833 - val_accuracy: 0.7396\n",
      "Epoch 176/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1803 - accuracy: 0.7336 - val_loss: 0.1829 - val_accuracy: 0.7394\n",
      "Epoch 177/1000\n",
      "563/563 [==============================] - 1s 991us/step - loss: 0.1814 - accuracy: 0.7348 - val_loss: 0.1830 - val_accuracy: 0.7409\n",
      "Epoch 178/1000\n",
      "563/563 [==============================] - 1s 975us/step - loss: 0.1812 - accuracy: 0.7350 - val_loss: 0.1834 - val_accuracy: 0.7376\n",
      "Epoch 179/1000\n",
      "563/563 [==============================] - 1s 995us/step - loss: 0.1822 - accuracy: 0.7332 - val_loss: 0.1825 - val_accuracy: 0.7406\n",
      "Epoch 180/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1804 - accuracy: 0.7377 - val_loss: 0.1828 - val_accuracy: 0.7410\n",
      "Epoch 181/1000\n",
      "563/563 [==============================] - 1s 991us/step - loss: 0.1809 - accuracy: 0.7353 - val_loss: 0.1821 - val_accuracy: 0.7396\n",
      "Epoch 182/1000\n",
      "563/563 [==============================] - 1s 979us/step - loss: 0.1816 - accuracy: 0.7342 - val_loss: 0.1826 - val_accuracy: 0.7402\n",
      "Epoch 183/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1797 - accuracy: 0.7394 - val_loss: 0.1822 - val_accuracy: 0.7413\n",
      "Epoch 184/1000\n",
      "563/563 [==============================] - 1s 979us/step - loss: 0.1813 - accuracy: 0.7351 - val_loss: 0.1830 - val_accuracy: 0.7401\n",
      "Epoch 185/1000\n",
      "563/563 [==============================] - 1s 974us/step - loss: 0.1800 - accuracy: 0.7348 - val_loss: 0.1860 - val_accuracy: 0.7335\n",
      "Epoch 186/1000\n",
      "563/563 [==============================] - 1s 997us/step - loss: 0.1833 - accuracy: 0.7316 - val_loss: 0.1837 - val_accuracy: 0.7398\n",
      "Epoch 187/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1808 - accuracy: 0.7361 - val_loss: 0.1834 - val_accuracy: 0.7380\n",
      "Epoch 188/1000\n",
      "563/563 [==============================] - 1s 991us/step - loss: 0.1778 - accuracy: 0.7417 - val_loss: 0.1829 - val_accuracy: 0.7388\n",
      "Epoch 189/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1806 - accuracy: 0.7368 - val_loss: 0.1826 - val_accuracy: 0.7397\n",
      "Epoch 190/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1786 - accuracy: 0.7426 - val_loss: 0.1834 - val_accuracy: 0.7392\n",
      "Epoch 191/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1810 - accuracy: 0.7349 - val_loss: 0.1838 - val_accuracy: 0.7397\n",
      "Epoch 192/1000\n",
      "563/563 [==============================] - 1s 984us/step - loss: 0.1801 - accuracy: 0.7350 - val_loss: 0.1843 - val_accuracy: 0.7411\n",
      "Epoch 193/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1809 - accuracy: 0.7372 - val_loss: 0.1827 - val_accuracy: 0.7403\n",
      "Epoch 194/1000\n",
      "563/563 [==============================] - 1s 995us/step - loss: 0.1819 - accuracy: 0.7326 - val_loss: 0.1826 - val_accuracy: 0.7405\n",
      "Epoch 195/1000\n",
      "563/563 [==============================] - 1s 971us/step - loss: 0.1766 - accuracy: 0.7489 - val_loss: 0.1836 - val_accuracy: 0.7410\n",
      "Epoch 196/1000\n",
      "563/563 [==============================] - 1s 975us/step - loss: 0.1801 - accuracy: 0.7383 - val_loss: 0.1834 - val_accuracy: 0.7396\n",
      "Epoch 197/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1806 - accuracy: 0.7354 - val_loss: 0.1828 - val_accuracy: 0.7411\n",
      "Epoch 198/1000\n",
      "563/563 [==============================] - 1s 984us/step - loss: 0.1809 - accuracy: 0.7373 - val_loss: 0.1852 - val_accuracy: 0.7374\n",
      "Epoch 199/1000\n",
      "563/563 [==============================] - 1s 986us/step - loss: 0.1817 - accuracy: 0.7322 - val_loss: 0.1839 - val_accuracy: 0.7375\n",
      "Epoch 200/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1806 - accuracy: 0.7349 - val_loss: 0.1835 - val_accuracy: 0.7392\n",
      "Epoch 201/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1835 - accuracy: 0.7308 - val_loss: 0.1827 - val_accuracy: 0.7415\n",
      "Epoch 202/1000\n",
      "563/563 [==============================] - 1s 977us/step - loss: 0.1819 - accuracy: 0.7348 - val_loss: 0.1834 - val_accuracy: 0.7393\n",
      "Epoch 203/1000\n",
      "563/563 [==============================] - 1s 995us/step - loss: 0.1803 - accuracy: 0.7383 - val_loss: 0.1832 - val_accuracy: 0.7409\n",
      "Epoch 204/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1805 - accuracy: 0.7341 - val_loss: 0.1830 - val_accuracy: 0.7393\n",
      "Epoch 205/1000\n",
      "563/563 [==============================] - 1s 984us/step - loss: 0.1779 - accuracy: 0.7381 - val_loss: 0.1832 - val_accuracy: 0.7403\n",
      "Epoch 206/1000\n",
      "563/563 [==============================] - 1s 974us/step - loss: 0.1811 - accuracy: 0.7377 - val_loss: 0.1829 - val_accuracy: 0.7394\n",
      "Epoch 207/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1823 - accuracy: 0.7333 - val_loss: 0.1825 - val_accuracy: 0.7400\n",
      "Epoch 208/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1803 - accuracy: 0.7389 - val_loss: 0.1839 - val_accuracy: 0.7409\n",
      "Epoch 209/1000\n",
      "563/563 [==============================] - 1s 974us/step - loss: 0.1821 - accuracy: 0.7329 - val_loss: 0.1837 - val_accuracy: 0.7409\n",
      "Epoch 210/1000\n",
      "563/563 [==============================] - 1s 990us/step - loss: 0.1799 - accuracy: 0.7383 - val_loss: 0.1826 - val_accuracy: 0.7406\n",
      "Epoch 211/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1797 - accuracy: 0.7390 - val_loss: 0.1829 - val_accuracy: 0.7416\n",
      "Epoch 212/1000\n",
      "563/563 [==============================] - 1s 998us/step - loss: 0.1782 - accuracy: 0.7397 - val_loss: 0.1828 - val_accuracy: 0.7407\n",
      "Epoch 213/1000\n",
      "563/563 [==============================] - 1s 979us/step - loss: 0.1812 - accuracy: 0.7321 - val_loss: 0.1837 - val_accuracy: 0.7392\n",
      "Epoch 214/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1807 - accuracy: 0.7369 - val_loss: 0.1838 - val_accuracy: 0.7407\n",
      "Epoch 215/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1794 - accuracy: 0.7377 - val_loss: 0.1842 - val_accuracy: 0.7406\n",
      "Epoch 216/1000\n",
      "563/563 [==============================] - 1s 984us/step - loss: 0.1804 - accuracy: 0.7388 - val_loss: 0.1839 - val_accuracy: 0.7398\n",
      "Epoch 217/1000\n",
      "563/563 [==============================] - 1s 959us/step - loss: 0.1804 - accuracy: 0.7349 - val_loss: 0.1830 - val_accuracy: 0.7383\n",
      "Epoch 218/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1815 - accuracy: 0.7330 - val_loss: 0.1849 - val_accuracy: 0.7389\n",
      "Epoch 219/1000\n",
      "563/563 [==============================] - 1s 995us/step - loss: 0.1794 - accuracy: 0.7413 - val_loss: 0.1840 - val_accuracy: 0.7405\n",
      "Epoch 220/1000\n",
      "563/563 [==============================] - 1s 970us/step - loss: 0.1783 - accuracy: 0.7420 - val_loss: 0.1832 - val_accuracy: 0.7411\n",
      "Epoch 221/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1803 - accuracy: 0.7375 - val_loss: 0.1824 - val_accuracy: 0.7411\n",
      "Epoch 222/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1781 - accuracy: 0.7417 - val_loss: 0.1836 - val_accuracy: 0.7391\n",
      "Epoch 223/1000\n",
      "563/563 [==============================] - 1s 963us/step - loss: 0.1812 - accuracy: 0.7315 - val_loss: 0.1830 - val_accuracy: 0.7411\n",
      "Epoch 224/1000\n",
      "563/563 [==============================] - 1s 961us/step - loss: 0.1790 - accuracy: 0.7380 - val_loss: 0.1838 - val_accuracy: 0.7394\n",
      "Epoch 225/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1822 - accuracy: 0.7335 - val_loss: 0.1836 - val_accuracy: 0.7397\n",
      "Epoch 226/1000\n",
      "563/563 [==============================] - 1s 997us/step - loss: 0.1798 - accuracy: 0.7402 - val_loss: 0.1827 - val_accuracy: 0.7411\n",
      "Epoch 227/1000\n",
      "563/563 [==============================] - 1s 961us/step - loss: 0.1819 - accuracy: 0.7316 - val_loss: 0.1828 - val_accuracy: 0.7411\n",
      "Epoch 228/1000\n",
      "563/563 [==============================] - 1s 968us/step - loss: 0.1788 - accuracy: 0.7398 - val_loss: 0.1830 - val_accuracy: 0.7403\n",
      "Epoch 229/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1801 - accuracy: 0.7400 - val_loss: 0.1837 - val_accuracy: 0.7414\n",
      "Epoch 230/1000\n",
      "563/563 [==============================] - 1s 986us/step - loss: 0.1801 - accuracy: 0.7380 - val_loss: 0.1835 - val_accuracy: 0.7409\n",
      "Epoch 231/1000\n",
      "563/563 [==============================] - 1s 975us/step - loss: 0.1794 - accuracy: 0.7409 - val_loss: 0.1840 - val_accuracy: 0.7403\n",
      "Epoch 232/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1784 - accuracy: 0.7417 - val_loss: 0.1832 - val_accuracy: 0.7415\n",
      "Epoch 233/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1808 - accuracy: 0.7374 - val_loss: 0.1831 - val_accuracy: 0.7422\n",
      "Epoch 234/1000\n",
      "563/563 [==============================] - 1s 989us/step - loss: 0.1791 - accuracy: 0.7398 - val_loss: 0.1848 - val_accuracy: 0.7354\n",
      "Epoch 235/1000\n",
      "563/563 [==============================] - 1s 956us/step - loss: 0.1798 - accuracy: 0.7404 - val_loss: 0.1833 - val_accuracy: 0.7401\n",
      "Epoch 236/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1780 - accuracy: 0.7430 - val_loss: 0.1838 - val_accuracy: 0.7398\n",
      "Epoch 237/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1803 - accuracy: 0.7395 - val_loss: 0.1828 - val_accuracy: 0.7405\n",
      "Epoch 238/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1783 - accuracy: 0.7424 - val_loss: 0.1828 - val_accuracy: 0.7413\n",
      "Epoch 239/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1777 - accuracy: 0.7408 - val_loss: 0.1829 - val_accuracy: 0.7407\n",
      "Epoch 240/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1808 - accuracy: 0.7362 - val_loss: 0.1831 - val_accuracy: 0.7409\n",
      "Epoch 241/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1797 - accuracy: 0.7373 - val_loss: 0.1832 - val_accuracy: 0.7410\n",
      "Epoch 242/1000\n",
      "563/563 [==============================] - 1s 994us/step - loss: 0.1770 - accuracy: 0.7450 - val_loss: 0.1835 - val_accuracy: 0.7410\n",
      "Epoch 243/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1781 - accuracy: 0.7425 - val_loss: 0.1850 - val_accuracy: 0.7387\n",
      "Epoch 244/1000\n",
      "563/563 [==============================] - 1s 975us/step - loss: 0.1813 - accuracy: 0.7349 - val_loss: 0.1847 - val_accuracy: 0.7388\n",
      "Epoch 245/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1821 - accuracy: 0.7324 - val_loss: 0.1824 - val_accuracy: 0.7403\n",
      "Epoch 246/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1799 - accuracy: 0.7384 - val_loss: 0.1837 - val_accuracy: 0.7418\n",
      "Epoch 247/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1793 - accuracy: 0.7383 - val_loss: 0.1822 - val_accuracy: 0.7410\n",
      "Epoch 248/1000\n",
      "563/563 [==============================] - 1s 965us/step - loss: 0.1814 - accuracy: 0.7349 - val_loss: 0.1836 - val_accuracy: 0.7397\n",
      "Epoch 249/1000\n",
      "563/563 [==============================] - 1s 963us/step - loss: 0.1835 - accuracy: 0.7285 - val_loss: 0.1834 - val_accuracy: 0.7397\n",
      "Epoch 250/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1809 - accuracy: 0.7357 - val_loss: 0.1830 - val_accuracy: 0.7415\n",
      "Epoch 251/1000\n",
      "563/563 [==============================] - 1s 961us/step - loss: 0.1783 - accuracy: 0.7404 - val_loss: 0.1850 - val_accuracy: 0.7407\n",
      "Epoch 252/1000\n",
      "563/563 [==============================] - 1s 974us/step - loss: 0.1811 - accuracy: 0.7366 - val_loss: 0.1834 - val_accuracy: 0.7409\n",
      "Epoch 253/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1804 - accuracy: 0.7390 - val_loss: 0.1837 - val_accuracy: 0.7385\n",
      "Epoch 254/1000\n",
      "563/563 [==============================] - 1s 991us/step - loss: 0.1803 - accuracy: 0.7364 - val_loss: 0.1846 - val_accuracy: 0.7391\n",
      "Epoch 255/1000\n",
      "563/563 [==============================] - 1s 972us/step - loss: 0.1787 - accuracy: 0.7401 - val_loss: 0.1831 - val_accuracy: 0.7416\n",
      "Epoch 256/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1818 - accuracy: 0.7345 - val_loss: 0.1850 - val_accuracy: 0.7380\n",
      "Epoch 257/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1830 - accuracy: 0.7302 - val_loss: 0.1835 - val_accuracy: 0.7415\n",
      "Epoch 258/1000\n",
      "563/563 [==============================] - 1s 972us/step - loss: 0.1796 - accuracy: 0.7391 - val_loss: 0.1835 - val_accuracy: 0.7394\n",
      "Epoch 259/1000\n",
      "563/563 [==============================] - 1s 979us/step - loss: 0.1807 - accuracy: 0.7366 - val_loss: 0.1839 - val_accuracy: 0.7397\n",
      "Epoch 260/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1807 - accuracy: 0.7363 - val_loss: 0.1832 - val_accuracy: 0.7402\n",
      "Epoch 261/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1813 - accuracy: 0.7377 - val_loss: 0.1837 - val_accuracy: 0.7409\n",
      "Epoch 262/1000\n",
      "563/563 [==============================] - 1s 965us/step - loss: 0.1816 - accuracy: 0.7329 - val_loss: 0.1833 - val_accuracy: 0.7397\n",
      "Epoch 263/1000\n",
      "563/563 [==============================] - 1s 979us/step - loss: 0.1829 - accuracy: 0.7342 - val_loss: 0.1831 - val_accuracy: 0.7411\n",
      "Epoch 264/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1817 - accuracy: 0.7339 - val_loss: 0.1855 - val_accuracy: 0.7398\n",
      "Epoch 265/1000\n",
      "563/563 [==============================] - 1s 990us/step - loss: 0.1788 - accuracy: 0.7370 - val_loss: 0.1839 - val_accuracy: 0.7405\n",
      "Epoch 266/1000\n",
      "563/563 [==============================] - 1s 966us/step - loss: 0.1792 - accuracy: 0.7374 - val_loss: 0.1827 - val_accuracy: 0.7400\n",
      "Epoch 267/1000\n",
      "563/563 [==============================] - 1s 974us/step - loss: 0.1768 - accuracy: 0.7458 - val_loss: 0.1827 - val_accuracy: 0.7402\n",
      "Epoch 268/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1822 - accuracy: 0.7309 - val_loss: 0.1832 - val_accuracy: 0.7410\n",
      "Epoch 269/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1783 - accuracy: 0.7426 - val_loss: 0.1843 - val_accuracy: 0.7402\n",
      "Epoch 270/1000\n",
      "563/563 [==============================] - 1s 974us/step - loss: 0.1812 - accuracy: 0.7337 - val_loss: 0.1832 - val_accuracy: 0.7380\n",
      "Epoch 271/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1808 - accuracy: 0.7405 - val_loss: 0.1842 - val_accuracy: 0.7392\n",
      "Epoch 272/1000\n",
      "563/563 [==============================] - 1s 975us/step - loss: 0.1784 - accuracy: 0.7408 - val_loss: 0.1835 - val_accuracy: 0.7391\n",
      "Epoch 273/1000\n",
      "563/563 [==============================] - 1s 998us/step - loss: 0.1788 - accuracy: 0.7432 - val_loss: 0.1847 - val_accuracy: 0.7365\n",
      "Epoch 274/1000\n",
      "563/563 [==============================] - 1s 981us/step - loss: 0.1788 - accuracy: 0.7385 - val_loss: 0.1826 - val_accuracy: 0.7409\n",
      "Epoch 275/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1794 - accuracy: 0.7390 - val_loss: 0.1834 - val_accuracy: 0.7389\n",
      "Epoch 276/1000\n",
      "563/563 [==============================] - 1s 961us/step - loss: 0.1821 - accuracy: 0.7350 - val_loss: 0.1824 - val_accuracy: 0.7409\n",
      "Epoch 277/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1803 - accuracy: 0.7354 - val_loss: 0.1823 - val_accuracy: 0.7383\n",
      "Epoch 278/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1783 - accuracy: 0.7426 - val_loss: 0.1845 - val_accuracy: 0.7383\n",
      "Epoch 279/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1835 - accuracy: 0.7286 - val_loss: 0.1820 - val_accuracy: 0.7388\n",
      "Epoch 280/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1805 - accuracy: 0.7360 - val_loss: 0.1844 - val_accuracy: 0.7387\n",
      "Epoch 281/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1819 - accuracy: 0.7331 - val_loss: 0.1829 - val_accuracy: 0.7400\n",
      "Epoch 282/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1787 - accuracy: 0.7413 - val_loss: 0.1846 - val_accuracy: 0.7381\n",
      "Epoch 283/1000\n",
      "563/563 [==============================] - 1s 954us/step - loss: 0.1830 - accuracy: 0.7322 - val_loss: 0.1843 - val_accuracy: 0.7367\n",
      "Epoch 284/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1787 - accuracy: 0.7394 - val_loss: 0.1847 - val_accuracy: 0.7376\n",
      "Epoch 285/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1796 - accuracy: 0.7400 - val_loss: 0.1836 - val_accuracy: 0.7384\n",
      "Epoch 286/1000\n",
      "563/563 [==============================] - 1s 982us/step - loss: 0.1814 - accuracy: 0.7326 - val_loss: 0.1857 - val_accuracy: 0.7379\n",
      "Epoch 287/1000\n",
      "563/563 [==============================] - 1s 959us/step - loss: 0.1802 - accuracy: 0.7382 - val_loss: 0.1850 - val_accuracy: 0.7372\n",
      "Epoch 288/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1800 - accuracy: 0.7378 - val_loss: 0.1836 - val_accuracy: 0.7366\n",
      "Epoch 289/1000\n",
      "563/563 [==============================] - 1s 965us/step - loss: 0.1791 - accuracy: 0.7386 - val_loss: 0.1825 - val_accuracy: 0.7407\n",
      "Epoch 290/1000\n",
      "563/563 [==============================] - 1s 970us/step - loss: 0.1820 - accuracy: 0.7333 - val_loss: 0.1829 - val_accuracy: 0.7384\n",
      "Epoch 291/1000\n",
      "563/563 [==============================] - 1s 965us/step - loss: 0.1789 - accuracy: 0.7391 - val_loss: 0.1828 - val_accuracy: 0.7383\n",
      "Epoch 292/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1789 - accuracy: 0.7402 - val_loss: 0.1837 - val_accuracy: 0.7389\n",
      "Epoch 293/1000\n",
      "563/563 [==============================] - 1s 974us/step - loss: 0.1816 - accuracy: 0.7335 - val_loss: 0.1849 - val_accuracy: 0.7391\n",
      "Epoch 294/1000\n",
      "563/563 [==============================] - 1s 970us/step - loss: 0.1774 - accuracy: 0.7421 - val_loss: 0.1853 - val_accuracy: 0.7402\n",
      "Epoch 295/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1808 - accuracy: 0.7347 - val_loss: 0.1835 - val_accuracy: 0.7385\n",
      "Epoch 296/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1795 - accuracy: 0.7351 - val_loss: 0.1840 - val_accuracy: 0.7396\n",
      "Epoch 297/1000\n",
      "563/563 [==============================] - 1s 993us/step - loss: 0.1829 - accuracy: 0.7316 - val_loss: 0.1832 - val_accuracy: 0.7413\n",
      "Epoch 298/1000\n",
      "563/563 [==============================] - 1s 998us/step - loss: 0.1814 - accuracy: 0.7344 - val_loss: 0.1833 - val_accuracy: 0.7401\n",
      "Epoch 299/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1779 - accuracy: 0.7430 - val_loss: 0.1851 - val_accuracy: 0.7396\n",
      "Epoch 300/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1799 - accuracy: 0.7378 - val_loss: 0.1842 - val_accuracy: 0.7366\n",
      "Epoch 301/1000\n",
      "563/563 [==============================] - 1s 977us/step - loss: 0.1807 - accuracy: 0.7348 - val_loss: 0.1848 - val_accuracy: 0.7375\n",
      "Epoch 302/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1800 - accuracy: 0.7414 - val_loss: 0.1848 - val_accuracy: 0.7393\n",
      "Epoch 303/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1789 - accuracy: 0.7393 - val_loss: 0.1859 - val_accuracy: 0.7379\n",
      "Epoch 304/1000\n",
      "563/563 [==============================] - 1s 970us/step - loss: 0.1787 - accuracy: 0.7403 - val_loss: 0.1845 - val_accuracy: 0.7388\n",
      "Epoch 305/1000\n",
      "563/563 [==============================] - 1s 979us/step - loss: 0.1800 - accuracy: 0.7362 - val_loss: 0.1845 - val_accuracy: 0.7402\n",
      "Epoch 306/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1797 - accuracy: 0.7394 - val_loss: 0.1844 - val_accuracy: 0.7400\n",
      "Epoch 307/1000\n",
      "563/563 [==============================] - 1s 961us/step - loss: 0.1815 - accuracy: 0.7352 - val_loss: 0.1842 - val_accuracy: 0.7383\n",
      "Epoch 308/1000\n",
      "563/563 [==============================] - 1s 970us/step - loss: 0.1801 - accuracy: 0.7375 - val_loss: 0.1834 - val_accuracy: 0.7402\n",
      "Epoch 309/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1808 - accuracy: 0.7344 - val_loss: 0.1832 - val_accuracy: 0.7383\n",
      "Epoch 310/1000\n",
      "563/563 [==============================] - 1s 997us/step - loss: 0.1786 - accuracy: 0.7374 - val_loss: 0.1842 - val_accuracy: 0.7398\n",
      "Epoch 311/1000\n",
      "563/563 [==============================] - 1s 991us/step - loss: 0.1796 - accuracy: 0.7367 - val_loss: 0.1838 - val_accuracy: 0.7385\n",
      "Epoch 312/1000\n",
      "563/563 [==============================] - 1s 984us/step - loss: 0.1803 - accuracy: 0.7383 - val_loss: 0.1845 - val_accuracy: 0.7374\n",
      "Epoch 313/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1802 - accuracy: 0.7389 - val_loss: 0.1839 - val_accuracy: 0.7413\n",
      "Epoch 314/1000\n",
      "563/563 [==============================] - 1s 965us/step - loss: 0.1817 - accuracy: 0.7341 - val_loss: 0.1840 - val_accuracy: 0.7402\n",
      "Epoch 315/1000\n",
      "563/563 [==============================] - 1s 990us/step - loss: 0.1789 - accuracy: 0.7421 - val_loss: 0.1857 - val_accuracy: 0.7357\n",
      "Epoch 316/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1802 - accuracy: 0.7390 - val_loss: 0.1832 - val_accuracy: 0.7392\n",
      "Epoch 317/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1785 - accuracy: 0.7415 - val_loss: 0.1837 - val_accuracy: 0.7397\n",
      "Epoch 318/1000\n",
      "563/563 [==============================] - 1s 987us/step - loss: 0.1768 - accuracy: 0.7452 - val_loss: 0.1828 - val_accuracy: 0.7400\n",
      "Epoch 319/1000\n",
      "563/563 [==============================] - 1s 975us/step - loss: 0.1823 - accuracy: 0.7337 - val_loss: 0.1829 - val_accuracy: 0.7393\n",
      "Epoch 320/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1803 - accuracy: 0.7370 - val_loss: 0.1851 - val_accuracy: 0.7381\n",
      "Epoch 321/1000\n",
      "563/563 [==============================] - 1s 991us/step - loss: 0.1797 - accuracy: 0.7390 - val_loss: 0.1881 - val_accuracy: 0.7323\n",
      "Epoch 322/1000\n",
      "563/563 [==============================] - 1s 982us/step - loss: 0.1785 - accuracy: 0.7399 - val_loss: 0.1841 - val_accuracy: 0.7403\n",
      "Epoch 323/1000\n",
      "563/563 [==============================] - 1s 995us/step - loss: 0.1786 - accuracy: 0.7387 - val_loss: 0.1832 - val_accuracy: 0.7394\n",
      "Epoch 324/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1800 - accuracy: 0.7383 - val_loss: 0.1844 - val_accuracy: 0.7402\n",
      "Epoch 325/1000\n",
      "563/563 [==============================] - 1s 961us/step - loss: 0.1802 - accuracy: 0.7376 - val_loss: 0.1840 - val_accuracy: 0.7380\n",
      "Epoch 326/1000\n",
      "563/563 [==============================] - 1s 965us/step - loss: 0.1804 - accuracy: 0.7360 - val_loss: 0.1839 - val_accuracy: 0.7388\n",
      "Epoch 327/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1799 - accuracy: 0.7386 - val_loss: 0.1836 - val_accuracy: 0.7389\n",
      "Epoch 328/1000\n",
      "563/563 [==============================] - 1s 984us/step - loss: 0.1777 - accuracy: 0.7409 - val_loss: 0.1839 - val_accuracy: 0.7368\n",
      "Epoch 329/1000\n",
      "563/563 [==============================] - 1s 975us/step - loss: 0.1798 - accuracy: 0.7381 - val_loss: 0.1868 - val_accuracy: 0.7356\n",
      "Epoch 330/1000\n",
      "563/563 [==============================] - 1s 993us/step - loss: 0.1835 - accuracy: 0.7311 - val_loss: 0.1843 - val_accuracy: 0.7389\n",
      "Epoch 331/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1822 - accuracy: 0.7352 - val_loss: 0.1837 - val_accuracy: 0.7403\n",
      "Epoch 332/1000\n",
      "563/563 [==============================] - 1s 979us/step - loss: 0.1816 - accuracy: 0.7346 - val_loss: 0.1851 - val_accuracy: 0.7359\n",
      "Epoch 333/1000\n",
      "563/563 [==============================] - 1s 959us/step - loss: 0.1815 - accuracy: 0.7351 - val_loss: 0.1846 - val_accuracy: 0.7410\n",
      "Epoch 334/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1791 - accuracy: 0.7366 - val_loss: 0.1839 - val_accuracy: 0.7407\n",
      "Epoch 335/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1819 - accuracy: 0.7345 - val_loss: 0.1830 - val_accuracy: 0.7398\n",
      "Epoch 336/1000\n",
      "563/563 [==============================] - 1s 972us/step - loss: 0.1805 - accuracy: 0.7357 - val_loss: 0.1843 - val_accuracy: 0.7378\n",
      "Epoch 337/1000\n",
      "563/563 [==============================] - 1s 974us/step - loss: 0.1825 - accuracy: 0.7334 - val_loss: 0.1838 - val_accuracy: 0.7396\n",
      "Epoch 338/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1815 - accuracy: 0.7340 - val_loss: 0.1842 - val_accuracy: 0.7391\n",
      "Epoch 339/1000\n",
      "563/563 [==============================] - 1s 977us/step - loss: 0.1817 - accuracy: 0.7331 - val_loss: 0.1848 - val_accuracy: 0.7388\n",
      "Epoch 340/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1808 - accuracy: 0.7369 - val_loss: 0.1844 - val_accuracy: 0.7358\n",
      "Epoch 341/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1824 - accuracy: 0.7335 - val_loss: 0.1838 - val_accuracy: 0.7380\n",
      "Epoch 342/1000\n",
      "563/563 [==============================] - 1s 995us/step - loss: 0.1813 - accuracy: 0.7359 - val_loss: 0.1850 - val_accuracy: 0.7414\n",
      "Epoch 343/1000\n",
      "563/563 [==============================] - 1s 984us/step - loss: 0.1811 - accuracy: 0.7391 - val_loss: 0.1857 - val_accuracy: 0.7397\n",
      "Epoch 344/1000\n",
      "563/563 [==============================] - 1s 963us/step - loss: 0.1815 - accuracy: 0.7336 - val_loss: 0.1848 - val_accuracy: 0.7398\n",
      "Epoch 345/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1825 - accuracy: 0.7341 - val_loss: 0.1849 - val_accuracy: 0.7402\n",
      "Epoch 346/1000\n",
      "563/563 [==============================] - 1s 997us/step - loss: 0.1803 - accuracy: 0.7373 - val_loss: 0.1854 - val_accuracy: 0.7396\n",
      "Epoch 347/1000\n",
      "563/563 [==============================] - 1s 982us/step - loss: 0.1784 - accuracy: 0.7418 - val_loss: 0.1823 - val_accuracy: 0.7401\n",
      "Epoch 348/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1800 - accuracy: 0.7356 - val_loss: 0.1861 - val_accuracy: 0.7414\n",
      "Epoch 349/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1796 - accuracy: 0.7434 - val_loss: 0.1857 - val_accuracy: 0.7391\n",
      "Epoch 350/1000\n",
      "563/563 [==============================] - 1s 979us/step - loss: 0.1812 - accuracy: 0.7370 - val_loss: 0.1822 - val_accuracy: 0.7405\n",
      "Epoch 351/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1831 - accuracy: 0.7343 - val_loss: 0.1840 - val_accuracy: 0.7389\n",
      "Epoch 352/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1825 - accuracy: 0.7341 - val_loss: 0.1841 - val_accuracy: 0.7391\n",
      "Epoch 353/1000\n",
      "563/563 [==============================] - 1s 991us/step - loss: 0.1811 - accuracy: 0.7350 - val_loss: 0.1831 - val_accuracy: 0.7414\n",
      "Epoch 354/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1807 - accuracy: 0.7369 - val_loss: 0.1822 - val_accuracy: 0.7405\n",
      "Epoch 355/1000\n",
      "563/563 [==============================] - 1s 982us/step - loss: 0.1802 - accuracy: 0.7372 - val_loss: 0.1824 - val_accuracy: 0.7410\n",
      "Epoch 356/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1792 - accuracy: 0.7380 - val_loss: 0.1837 - val_accuracy: 0.7411\n",
      "Epoch 357/1000\n",
      "563/563 [==============================] - 1s 966us/step - loss: 0.1815 - accuracy: 0.7338 - val_loss: 0.1842 - val_accuracy: 0.7409\n",
      "Epoch 358/1000\n",
      "563/563 [==============================] - 1s 982us/step - loss: 0.1811 - accuracy: 0.7332 - val_loss: 0.1829 - val_accuracy: 0.7432\n",
      "Epoch 359/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1796 - accuracy: 0.7362 - val_loss: 0.1840 - val_accuracy: 0.7394\n",
      "Epoch 360/1000\n",
      "563/563 [==============================] - 1s 972us/step - loss: 0.1814 - accuracy: 0.7371 - val_loss: 0.1844 - val_accuracy: 0.7409\n",
      "Epoch 361/1000\n",
      "563/563 [==============================] - 1s 958us/step - loss: 0.1805 - accuracy: 0.7382 - val_loss: 0.1833 - val_accuracy: 0.7397\n",
      "Epoch 362/1000\n",
      "563/563 [==============================] - 1s 990us/step - loss: 0.1803 - accuracy: 0.7355 - val_loss: 0.1840 - val_accuracy: 0.7406\n",
      "Epoch 363/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1829 - accuracy: 0.7336 - val_loss: 0.1842 - val_accuracy: 0.7397\n",
      "Epoch 364/1000\n",
      "563/563 [==============================] - 1s 972us/step - loss: 0.1807 - accuracy: 0.7360 - val_loss: 0.1843 - val_accuracy: 0.7411\n",
      "Epoch 365/1000\n",
      "563/563 [==============================] - 1s 986us/step - loss: 0.1812 - accuracy: 0.7333 - val_loss: 0.1839 - val_accuracy: 0.7401\n",
      "Epoch 366/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1787 - accuracy: 0.7405 - val_loss: 0.1838 - val_accuracy: 0.7393\n",
      "Epoch 367/1000\n",
      "563/563 [==============================] - 1s 981us/step - loss: 0.1805 - accuracy: 0.7377 - val_loss: 0.1828 - val_accuracy: 0.7413\n",
      "Epoch 368/1000\n",
      "563/563 [==============================] - 1s 959us/step - loss: 0.1823 - accuracy: 0.7346 - val_loss: 0.1840 - val_accuracy: 0.7405\n",
      "Epoch 369/1000\n",
      "563/563 [==============================] - 1s 961us/step - loss: 0.1814 - accuracy: 0.7359 - val_loss: 0.1844 - val_accuracy: 0.7397\n",
      "Epoch 370/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1817 - accuracy: 0.7353 - val_loss: 0.1830 - val_accuracy: 0.7407\n",
      "Epoch 371/1000\n",
      "563/563 [==============================] - 1s 963us/step - loss: 0.1828 - accuracy: 0.7334 - val_loss: 0.1828 - val_accuracy: 0.7389\n",
      "Epoch 372/1000\n",
      "563/563 [==============================] - 1s 986us/step - loss: 0.1796 - accuracy: 0.7351 - val_loss: 0.1843 - val_accuracy: 0.7391\n",
      "Epoch 373/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1784 - accuracy: 0.7370 - val_loss: 0.1839 - val_accuracy: 0.7392\n",
      "Epoch 374/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1800 - accuracy: 0.7391 - val_loss: 0.1836 - val_accuracy: 0.7385\n",
      "Epoch 375/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1796 - accuracy: 0.7373 - val_loss: 0.1834 - val_accuracy: 0.7393\n",
      "Epoch 376/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1809 - accuracy: 0.7367 - val_loss: 0.1838 - val_accuracy: 0.7394\n",
      "Epoch 377/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1825 - accuracy: 0.7332 - val_loss: 0.1839 - val_accuracy: 0.7400\n",
      "Epoch 378/1000\n",
      "563/563 [==============================] - 1s 986us/step - loss: 0.1814 - accuracy: 0.7350 - val_loss: 0.1832 - val_accuracy: 0.7396\n",
      "Epoch 379/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1788 - accuracy: 0.7407 - val_loss: 0.1843 - val_accuracy: 0.7389\n",
      "Epoch 380/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1779 - accuracy: 0.7411 - val_loss: 0.1841 - val_accuracy: 0.7392\n",
      "Epoch 381/1000\n",
      "563/563 [==============================] - 1s 993us/step - loss: 0.1799 - accuracy: 0.7374 - val_loss: 0.1839 - val_accuracy: 0.7403\n",
      "Epoch 382/1000\n",
      "563/563 [==============================] - 1s 968us/step - loss: 0.1750 - accuracy: 0.7479 - val_loss: 0.1842 - val_accuracy: 0.7393\n",
      "Epoch 383/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1815 - accuracy: 0.7312 - val_loss: 0.1851 - val_accuracy: 0.7384\n",
      "Epoch 384/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1797 - accuracy: 0.7380 - val_loss: 0.1837 - val_accuracy: 0.7391\n",
      "Epoch 385/1000\n",
      "563/563 [==============================] - 1s 968us/step - loss: 0.1795 - accuracy: 0.7389 - val_loss: 0.1848 - val_accuracy: 0.7391\n",
      "Epoch 386/1000\n",
      "563/563 [==============================] - 1s 988us/step - loss: 0.1826 - accuracy: 0.7298 - val_loss: 0.1844 - val_accuracy: 0.7394\n",
      "Epoch 387/1000\n",
      "563/563 [==============================] - 1s 991us/step - loss: 0.1804 - accuracy: 0.7373 - val_loss: 0.1850 - val_accuracy: 0.7368\n",
      "Epoch 388/1000\n",
      "563/563 [==============================] - 1s 991us/step - loss: 0.1804 - accuracy: 0.7351 - val_loss: 0.1843 - val_accuracy: 0.7371\n",
      "Epoch 389/1000\n",
      "563/563 [==============================] - 1s 977us/step - loss: 0.1817 - accuracy: 0.7338 - val_loss: 0.1832 - val_accuracy: 0.7391\n",
      "Epoch 390/1000\n",
      "563/563 [==============================] - 1s 970us/step - loss: 0.1817 - accuracy: 0.7359 - val_loss: 0.1853 - val_accuracy: 0.7376\n",
      "Epoch 391/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1806 - accuracy: 0.7379 - val_loss: 0.1839 - val_accuracy: 0.7383\n",
      "Epoch 392/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1794 - accuracy: 0.7359 - val_loss: 0.1837 - val_accuracy: 0.7379\n",
      "Epoch 393/1000\n",
      "563/563 [==============================] - 1s 986us/step - loss: 0.1754 - accuracy: 0.7428 - val_loss: 0.1840 - val_accuracy: 0.7393\n",
      "Epoch 394/1000\n",
      "563/563 [==============================] - 1s 995us/step - loss: 0.1844 - accuracy: 0.7287 - val_loss: 0.1848 - val_accuracy: 0.7387\n",
      "Epoch 395/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1791 - accuracy: 0.7392 - val_loss: 0.1833 - val_accuracy: 0.7391\n",
      "Epoch 396/1000\n",
      "563/563 [==============================] - 1s 974us/step - loss: 0.1779 - accuracy: 0.7404 - val_loss: 0.1842 - val_accuracy: 0.7387\n",
      "Epoch 397/1000\n",
      "563/563 [==============================] - 1s 966us/step - loss: 0.1814 - accuracy: 0.7329 - val_loss: 0.1869 - val_accuracy: 0.7401\n",
      "Epoch 398/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1817 - accuracy: 0.7333 - val_loss: 0.1851 - val_accuracy: 0.7379\n",
      "Epoch 399/1000\n",
      "563/563 [==============================] - 1s 997us/step - loss: 0.1810 - accuracy: 0.7353 - val_loss: 0.1873 - val_accuracy: 0.7345\n",
      "Epoch 400/1000\n",
      "563/563 [==============================] - 1s 959us/step - loss: 0.1808 - accuracy: 0.7362 - val_loss: 0.1830 - val_accuracy: 0.7397\n",
      "Epoch 401/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1800 - accuracy: 0.7401 - val_loss: 0.1848 - val_accuracy: 0.7387\n",
      "Epoch 402/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1830 - accuracy: 0.7343 - val_loss: 0.1836 - val_accuracy: 0.7396\n",
      "Epoch 403/1000\n",
      "563/563 [==============================] - 1s 974us/step - loss: 0.1825 - accuracy: 0.7359 - val_loss: 0.1833 - val_accuracy: 0.7398\n",
      "Epoch 404/1000\n",
      "563/563 [==============================] - 1s 985us/step - loss: 0.1792 - accuracy: 0.7379 - val_loss: 0.1848 - val_accuracy: 0.7394\n",
      "Epoch 405/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1807 - accuracy: 0.7378 - val_loss: 0.1836 - val_accuracy: 0.7396\n",
      "Epoch 406/1000\n",
      "563/563 [==============================] - 1s 990us/step - loss: 0.1794 - accuracy: 0.7420 - val_loss: 0.1848 - val_accuracy: 0.7409\n",
      "Epoch 407/1000\n",
      "563/563 [==============================] - 1s 995us/step - loss: 0.1802 - accuracy: 0.7343 - val_loss: 0.1825 - val_accuracy: 0.7406\n",
      "Epoch 408/1000\n",
      "563/563 [==============================] - 1s 991us/step - loss: 0.1820 - accuracy: 0.7372 - val_loss: 0.1843 - val_accuracy: 0.7393\n",
      "Epoch 409/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1807 - accuracy: 0.7352 - val_loss: 0.1831 - val_accuracy: 0.7400\n",
      "Epoch 410/1000\n",
      "563/563 [==============================] - 1s 977us/step - loss: 0.1815 - accuracy: 0.7323 - val_loss: 0.1830 - val_accuracy: 0.7400\n",
      "Epoch 411/1000\n",
      "563/563 [==============================] - 1s 984us/step - loss: 0.1763 - accuracy: 0.7457 - val_loss: 0.1834 - val_accuracy: 0.7406\n",
      "Epoch 412/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1801 - accuracy: 0.7377 - val_loss: 0.1844 - val_accuracy: 0.7406\n",
      "Epoch 413/1000\n",
      "563/563 [==============================] - 1s 982us/step - loss: 0.1790 - accuracy: 0.7402 - val_loss: 0.1837 - val_accuracy: 0.7359\n",
      "Epoch 414/1000\n",
      "563/563 [==============================] - 1s 986us/step - loss: 0.1801 - accuracy: 0.7353 - val_loss: 0.1882 - val_accuracy: 0.7371\n",
      "Epoch 415/1000\n",
      "563/563 [==============================] - 1s 968us/step - loss: 0.1823 - accuracy: 0.7346 - val_loss: 0.1838 - val_accuracy: 0.7402\n",
      "Epoch 416/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1797 - accuracy: 0.7393 - val_loss: 0.1853 - val_accuracy: 0.7378\n",
      "Epoch 417/1000\n",
      "563/563 [==============================] - 1s 977us/step - loss: 0.1784 - accuracy: 0.7432 - val_loss: 0.1843 - val_accuracy: 0.7415\n",
      "Epoch 418/1000\n",
      "563/563 [==============================] - 1s 950us/step - loss: 0.1791 - accuracy: 0.7381 - val_loss: 0.1851 - val_accuracy: 0.7400\n",
      "Epoch 419/1000\n",
      "563/563 [==============================] - 1s 993us/step - loss: 0.1795 - accuracy: 0.7377 - val_loss: 0.1840 - val_accuracy: 0.7414\n",
      "Epoch 420/1000\n",
      "563/563 [==============================] - 1s 995us/step - loss: 0.1783 - accuracy: 0.7388 - val_loss: 0.1835 - val_accuracy: 0.7392\n",
      "Epoch 421/1000\n",
      "563/563 [==============================] - 1s 990us/step - loss: 0.1824 - accuracy: 0.7314 - val_loss: 0.1827 - val_accuracy: 0.7411\n",
      "Epoch 422/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1807 - accuracy: 0.7386 - val_loss: 0.1842 - val_accuracy: 0.7397\n",
      "Epoch 423/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1806 - accuracy: 0.7349 - val_loss: 0.1835 - val_accuracy: 0.7391\n",
      "Epoch 424/1000\n",
      "563/563 [==============================] - 1s 966us/step - loss: 0.1812 - accuracy: 0.7363 - val_loss: 0.1832 - val_accuracy: 0.7400\n",
      "Epoch 425/1000\n",
      "563/563 [==============================] - 1s 995us/step - loss: 0.1795 - accuracy: 0.7408 - val_loss: 0.1839 - val_accuracy: 0.7413\n",
      "Epoch 426/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1811 - accuracy: 0.7382 - val_loss: 0.1850 - val_accuracy: 0.7398\n",
      "Epoch 427/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1786 - accuracy: 0.7405 - val_loss: 0.1832 - val_accuracy: 0.7394\n",
      "Epoch 428/1000\n",
      "563/563 [==============================] - 1s 979us/step - loss: 0.1811 - accuracy: 0.7386 - val_loss: 0.1835 - val_accuracy: 0.7416\n",
      "Epoch 429/1000\n",
      "563/563 [==============================] - 1s 952us/step - loss: 0.1816 - accuracy: 0.7337 - val_loss: 0.1825 - val_accuracy: 0.7410\n",
      "Epoch 430/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1799 - accuracy: 0.7379 - val_loss: 0.1841 - val_accuracy: 0.7397\n",
      "Epoch 431/1000\n",
      "563/563 [==============================] - 1s 981us/step - loss: 0.1802 - accuracy: 0.7382 - val_loss: 0.1833 - val_accuracy: 0.7402\n",
      "Epoch 432/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1810 - accuracy: 0.7363 - val_loss: 0.1828 - val_accuracy: 0.7400\n",
      "Epoch 433/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1793 - accuracy: 0.7413 - val_loss: 0.1831 - val_accuracy: 0.7401\n",
      "Epoch 434/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1793 - accuracy: 0.7400 - val_loss: 0.1839 - val_accuracy: 0.7397\n",
      "Epoch 435/1000\n",
      "563/563 [==============================] - 1s 949us/step - loss: 0.1822 - accuracy: 0.7344 - val_loss: 0.1892 - val_accuracy: 0.7359\n",
      "Epoch 436/1000\n",
      "563/563 [==============================] - 1s 966us/step - loss: 0.1824 - accuracy: 0.7309 - val_loss: 0.1835 - val_accuracy: 0.7396\n",
      "Epoch 437/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1812 - accuracy: 0.7357 - val_loss: 0.1850 - val_accuracy: 0.7380\n",
      "Epoch 438/1000\n",
      "563/563 [==============================] - 1s 961us/step - loss: 0.1814 - accuracy: 0.7333 - val_loss: 0.1848 - val_accuracy: 0.7389\n",
      "Epoch 439/1000\n",
      "563/563 [==============================] - 1s 970us/step - loss: 0.1782 - accuracy: 0.7437 - val_loss: 0.1836 - val_accuracy: 0.7376\n",
      "Epoch 440/1000\n",
      "563/563 [==============================] - 1s 970us/step - loss: 0.1805 - accuracy: 0.7376 - val_loss: 0.1846 - val_accuracy: 0.7398\n",
      "Epoch 441/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1790 - accuracy: 0.7385 - val_loss: 0.1847 - val_accuracy: 0.7380\n",
      "Epoch 442/1000\n",
      "563/563 [==============================] - 1s 968us/step - loss: 0.1817 - accuracy: 0.7322 - val_loss: 0.1850 - val_accuracy: 0.7403\n",
      "Epoch 443/1000\n",
      "563/563 [==============================] - 1s 981us/step - loss: 0.1782 - accuracy: 0.7420 - val_loss: 0.1829 - val_accuracy: 0.7391\n",
      "Epoch 444/1000\n",
      "563/563 [==============================] - 1s 995us/step - loss: 0.1841 - accuracy: 0.7308 - val_loss: 0.1841 - val_accuracy: 0.7371\n",
      "Epoch 445/1000\n",
      "563/563 [==============================] - 1s 986us/step - loss: 0.1817 - accuracy: 0.7335 - val_loss: 0.1841 - val_accuracy: 0.7383\n",
      "Epoch 446/1000\n",
      "563/563 [==============================] - 1s 974us/step - loss: 0.1789 - accuracy: 0.7407 - val_loss: 0.1831 - val_accuracy: 0.7401\n",
      "Epoch 447/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1804 - accuracy: 0.7362 - val_loss: 0.1873 - val_accuracy: 0.7362\n",
      "Epoch 448/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1835 - accuracy: 0.7321 - val_loss: 0.1834 - val_accuracy: 0.7407\n",
      "Epoch 449/1000\n",
      "563/563 [==============================] - 1s 968us/step - loss: 0.1798 - accuracy: 0.7382 - val_loss: 0.1858 - val_accuracy: 0.7370\n",
      "Epoch 450/1000\n",
      "563/563 [==============================] - 1s 961us/step - loss: 0.1802 - accuracy: 0.7348 - val_loss: 0.1847 - val_accuracy: 0.7371\n",
      "Epoch 451/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1817 - accuracy: 0.7374 - val_loss: 0.1838 - val_accuracy: 0.7405\n",
      "Epoch 452/1000\n",
      "563/563 [==============================] - 1s 975us/step - loss: 0.1799 - accuracy: 0.7387 - val_loss: 0.1836 - val_accuracy: 0.7406\n",
      "Epoch 453/1000\n",
      "563/563 [==============================] - 1s 970us/step - loss: 0.1807 - accuracy: 0.7359 - val_loss: 0.1828 - val_accuracy: 0.7406\n",
      "Epoch 454/1000\n",
      "563/563 [==============================] - 1s 991us/step - loss: 0.1827 - accuracy: 0.7319 - val_loss: 0.1835 - val_accuracy: 0.7407\n",
      "Epoch 455/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1815 - accuracy: 0.7352 - val_loss: 0.1843 - val_accuracy: 0.7403\n",
      "Epoch 456/1000\n",
      "563/563 [==============================] - 1s 956us/step - loss: 0.1827 - accuracy: 0.7317 - val_loss: 0.1847 - val_accuracy: 0.7381\n",
      "Epoch 457/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1831 - accuracy: 0.7320 - val_loss: 0.1850 - val_accuracy: 0.7349\n",
      "Epoch 458/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1807 - accuracy: 0.7371 - val_loss: 0.1830 - val_accuracy: 0.7407\n",
      "Epoch 459/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1782 - accuracy: 0.7410 - val_loss: 0.1831 - val_accuracy: 0.7394\n",
      "Epoch 460/1000\n",
      "563/563 [==============================] - 1s 941us/step - loss: 0.1837 - accuracy: 0.7291 - val_loss: 0.1855 - val_accuracy: 0.7378\n",
      "Epoch 461/1000\n",
      "563/563 [==============================] - 1s 960us/step - loss: 0.1815 - accuracy: 0.7386 - val_loss: 0.1849 - val_accuracy: 0.7368\n",
      "Epoch 462/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1819 - accuracy: 0.7344 - val_loss: 0.1835 - val_accuracy: 0.7389\n",
      "Epoch 463/1000\n",
      "563/563 [==============================] - 1s 975us/step - loss: 0.1798 - accuracy: 0.7402 - val_loss: 0.1852 - val_accuracy: 0.7384\n",
      "Epoch 464/1000\n",
      "563/563 [==============================] - 1s 984us/step - loss: 0.1775 - accuracy: 0.7411 - val_loss: 0.1843 - val_accuracy: 0.7403\n",
      "Epoch 465/1000\n",
      "563/563 [==============================] - 1s 970us/step - loss: 0.1791 - accuracy: 0.7384 - val_loss: 0.1865 - val_accuracy: 0.7383\n",
      "Epoch 466/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1801 - accuracy: 0.7360 - val_loss: 0.1840 - val_accuracy: 0.7378\n",
      "Epoch 467/1000\n",
      "563/563 [==============================] - 1s 981us/step - loss: 0.1818 - accuracy: 0.7331 - val_loss: 0.1831 - val_accuracy: 0.7389\n",
      "Epoch 468/1000\n",
      "563/563 [==============================] - 1s 954us/step - loss: 0.1783 - accuracy: 0.7414 - val_loss: 0.1849 - val_accuracy: 0.7363\n",
      "Epoch 469/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1812 - accuracy: 0.7315 - val_loss: 0.1836 - val_accuracy: 0.7397\n",
      "Epoch 470/1000\n",
      "563/563 [==============================] - 1s 991us/step - loss: 0.1806 - accuracy: 0.7388 - val_loss: 0.1835 - val_accuracy: 0.7384\n",
      "Epoch 471/1000\n",
      "563/563 [==============================] - 1s 979us/step - loss: 0.1800 - accuracy: 0.7357 - val_loss: 0.1841 - val_accuracy: 0.7392\n",
      "Epoch 472/1000\n",
      "563/563 [==============================] - 1s 993us/step - loss: 0.1828 - accuracy: 0.7301 - val_loss: 0.1853 - val_accuracy: 0.7379\n",
      "Epoch 473/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1800 - accuracy: 0.7367 - val_loss: 0.1842 - val_accuracy: 0.7374\n",
      "Epoch 474/1000\n",
      "563/563 [==============================] - 1s 966us/step - loss: 0.1815 - accuracy: 0.7332 - val_loss: 0.1839 - val_accuracy: 0.7380\n",
      "Epoch 475/1000\n",
      "563/563 [==============================] - 1s 963us/step - loss: 0.1816 - accuracy: 0.7343 - val_loss: 0.1856 - val_accuracy: 0.7378\n",
      "Epoch 476/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1766 - accuracy: 0.7460 - val_loss: 0.1833 - val_accuracy: 0.7391\n",
      "Epoch 477/1000\n",
      "563/563 [==============================] - 1s 993us/step - loss: 0.1799 - accuracy: 0.7399 - val_loss: 0.1848 - val_accuracy: 0.7380\n",
      "Epoch 478/1000\n",
      "563/563 [==============================] - 1s 974us/step - loss: 0.1810 - accuracy: 0.7349 - val_loss: 0.1851 - val_accuracy: 0.7370\n",
      "Epoch 479/1000\n",
      "563/563 [==============================] - 1s 991us/step - loss: 0.1816 - accuracy: 0.7338 - val_loss: 0.1840 - val_accuracy: 0.7376\n",
      "Epoch 480/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1819 - accuracy: 0.7333 - val_loss: 0.1854 - val_accuracy: 0.7384\n",
      "Epoch 481/1000\n",
      "563/563 [==============================] - 1s 998us/step - loss: 0.1820 - accuracy: 0.7348 - val_loss: 0.1853 - val_accuracy: 0.7396\n",
      "Epoch 482/1000\n",
      "563/563 [==============================] - 1s 982us/step - loss: 0.1790 - accuracy: 0.7418 - val_loss: 0.1842 - val_accuracy: 0.7384\n",
      "Epoch 483/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1803 - accuracy: 0.7386 - val_loss: 0.1840 - val_accuracy: 0.7383\n",
      "Epoch 484/1000\n",
      "563/563 [==============================] - 1s 991us/step - loss: 0.1794 - accuracy: 0.7407 - val_loss: 0.1833 - val_accuracy: 0.7379\n",
      "Epoch 485/1000\n",
      "563/563 [==============================] - 1s 963us/step - loss: 0.1791 - accuracy: 0.7412 - val_loss: 0.1849 - val_accuracy: 0.7378\n",
      "Epoch 486/1000\n",
      "563/563 [==============================] - 1s 965us/step - loss: 0.1790 - accuracy: 0.7383 - val_loss: 0.1833 - val_accuracy: 0.7385\n",
      "Epoch 487/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1802 - accuracy: 0.7362 - val_loss: 0.1848 - val_accuracy: 0.7376\n",
      "Epoch 488/1000\n",
      "563/563 [==============================] - 1s 972us/step - loss: 0.1850 - accuracy: 0.7280 - val_loss: 0.1856 - val_accuracy: 0.7380\n",
      "Epoch 489/1000\n",
      "563/563 [==============================] - 1s 977us/step - loss: 0.1777 - accuracy: 0.7419 - val_loss: 0.1859 - val_accuracy: 0.7407\n",
      "Epoch 490/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1784 - accuracy: 0.7417 - val_loss: 0.1846 - val_accuracy: 0.7392\n",
      "Epoch 491/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1808 - accuracy: 0.7368 - val_loss: 0.1845 - val_accuracy: 0.7372\n",
      "Epoch 492/1000\n",
      "563/563 [==============================] - 1s 966us/step - loss: 0.1811 - accuracy: 0.7379 - val_loss: 0.1863 - val_accuracy: 0.7375\n",
      "Epoch 493/1000\n",
      "563/563 [==============================] - 1s 986us/step - loss: 0.1787 - accuracy: 0.7402 - val_loss: 0.1848 - val_accuracy: 0.7380\n",
      "Epoch 494/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1782 - accuracy: 0.7416 - val_loss: 0.1857 - val_accuracy: 0.7354\n",
      "Epoch 495/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1810 - accuracy: 0.7368 - val_loss: 0.1839 - val_accuracy: 0.7379\n",
      "Epoch 496/1000\n",
      "563/563 [==============================] - 1s 974us/step - loss: 0.1832 - accuracy: 0.7331 - val_loss: 0.1851 - val_accuracy: 0.7375\n",
      "Epoch 497/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1811 - accuracy: 0.7349 - val_loss: 0.1853 - val_accuracy: 0.7371\n",
      "Epoch 498/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1806 - accuracy: 0.7367 - val_loss: 0.1841 - val_accuracy: 0.7391\n",
      "Epoch 499/1000\n",
      "563/563 [==============================] - 1s 991us/step - loss: 0.1803 - accuracy: 0.7372 - val_loss: 0.1841 - val_accuracy: 0.7385\n",
      "Epoch 500/1000\n",
      "563/563 [==============================] - 1s 966us/step - loss: 0.1801 - accuracy: 0.7379 - val_loss: 0.1840 - val_accuracy: 0.7393\n",
      "Epoch 501/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1789 - accuracy: 0.7413 - val_loss: 0.1845 - val_accuracy: 0.7385\n",
      "Epoch 502/1000\n",
      "563/563 [==============================] - 1s 963us/step - loss: 0.1813 - accuracy: 0.7359 - val_loss: 0.1854 - val_accuracy: 0.7396\n",
      "Epoch 503/1000\n",
      "563/563 [==============================] - 1s 995us/step - loss: 0.1813 - accuracy: 0.7354 - val_loss: 0.1843 - val_accuracy: 0.7405\n",
      "Epoch 504/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1829 - accuracy: 0.7311 - val_loss: 0.1842 - val_accuracy: 0.7394\n",
      "Epoch 505/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1786 - accuracy: 0.7435 - val_loss: 0.1850 - val_accuracy: 0.7400\n",
      "Epoch 506/1000\n",
      "563/563 [==============================] - 1s 973us/step - loss: 0.1796 - accuracy: 0.7372 - val_loss: 0.1844 - val_accuracy: 0.7393\n",
      "Epoch 507/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1800 - accuracy: 0.7375 - val_loss: 0.1864 - val_accuracy: 0.7385\n",
      "Epoch 508/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1809 - accuracy: 0.7355 - val_loss: 0.1831 - val_accuracy: 0.7411\n",
      "Epoch 509/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1795 - accuracy: 0.7364 - val_loss: 0.1849 - val_accuracy: 0.7394\n",
      "Epoch 510/1000\n",
      "563/563 [==============================] - 1s 981us/step - loss: 0.1809 - accuracy: 0.7380 - val_loss: 0.1833 - val_accuracy: 0.7410\n",
      "Epoch 511/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1819 - accuracy: 0.7336 - val_loss: 0.1846 - val_accuracy: 0.7402\n",
      "Epoch 512/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1833 - accuracy: 0.7320 - val_loss: 0.1845 - val_accuracy: 0.7393\n",
      "Epoch 513/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1819 - accuracy: 0.7363 - val_loss: 0.1833 - val_accuracy: 0.7394\n",
      "Epoch 514/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1784 - accuracy: 0.7394 - val_loss: 0.1833 - val_accuracy: 0.7402\n",
      "Epoch 515/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1842 - accuracy: 0.7298 - val_loss: 0.1838 - val_accuracy: 0.7397\n",
      "Epoch 516/1000\n",
      "563/563 [==============================] - 1s 972us/step - loss: 0.1823 - accuracy: 0.7366 - val_loss: 0.1845 - val_accuracy: 0.7385\n",
      "Epoch 517/1000\n",
      "563/563 [==============================] - 1s 961us/step - loss: 0.1821 - accuracy: 0.7302 - val_loss: 0.1871 - val_accuracy: 0.7393\n",
      "Epoch 518/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1853 - accuracy: 0.7320 - val_loss: 0.1864 - val_accuracy: 0.7402\n",
      "Epoch 519/1000\n",
      "563/563 [==============================] - 1s 975us/step - loss: 0.1813 - accuracy: 0.7384 - val_loss: 0.1868 - val_accuracy: 0.7371\n",
      "Epoch 520/1000\n",
      "563/563 [==============================] - 1s 974us/step - loss: 0.1847 - accuracy: 0.7331 - val_loss: 0.1852 - val_accuracy: 0.7400\n",
      "Epoch 521/1000\n",
      "563/563 [==============================] - 1s 965us/step - loss: 0.1818 - accuracy: 0.7375 - val_loss: 0.1856 - val_accuracy: 0.7403\n",
      "Epoch 522/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1838 - accuracy: 0.7377 - val_loss: 0.1873 - val_accuracy: 0.7400\n",
      "Epoch 523/1000\n",
      "563/563 [==============================] - 1s 970us/step - loss: 0.1811 - accuracy: 0.7400 - val_loss: 0.1858 - val_accuracy: 0.7403\n",
      "Epoch 524/1000\n",
      "563/563 [==============================] - 1s 968us/step - loss: 0.1825 - accuracy: 0.7391 - val_loss: 0.1858 - val_accuracy: 0.7401\n",
      "Epoch 525/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1811 - accuracy: 0.7405 - val_loss: 0.1868 - val_accuracy: 0.7392\n",
      "Epoch 526/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1820 - accuracy: 0.7378 - val_loss: 0.1869 - val_accuracy: 0.7396\n",
      "Epoch 527/1000\n",
      "563/563 [==============================] - 1s 995us/step - loss: 0.1805 - accuracy: 0.7390 - val_loss: 0.1858 - val_accuracy: 0.7385\n",
      "Epoch 528/1000\n",
      "563/563 [==============================] - 1s 974us/step - loss: 0.1833 - accuracy: 0.7331 - val_loss: 0.1862 - val_accuracy: 0.7391\n",
      "Epoch 529/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1847 - accuracy: 0.7299 - val_loss: 0.1857 - val_accuracy: 0.7387\n",
      "Epoch 530/1000\n",
      "563/563 [==============================] - 1s 970us/step - loss: 0.1837 - accuracy: 0.7307 - val_loss: 0.1854 - val_accuracy: 0.7392\n",
      "Epoch 531/1000\n",
      "563/563 [==============================] - 1s 968us/step - loss: 0.1800 - accuracy: 0.7413 - val_loss: 0.1848 - val_accuracy: 0.7396\n",
      "Epoch 532/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1811 - accuracy: 0.7387 - val_loss: 0.1853 - val_accuracy: 0.7398\n",
      "Epoch 533/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1841 - accuracy: 0.7317 - val_loss: 0.1850 - val_accuracy: 0.7400\n",
      "Epoch 534/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1822 - accuracy: 0.7353 - val_loss: 0.1868 - val_accuracy: 0.7391\n",
      "Epoch 535/1000\n",
      "563/563 [==============================] - 1s 972us/step - loss: 0.1810 - accuracy: 0.7419 - val_loss: 0.1850 - val_accuracy: 0.7392\n",
      "Epoch 536/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1803 - accuracy: 0.7396 - val_loss: 0.1832 - val_accuracy: 0.7387\n",
      "Epoch 537/1000\n",
      "563/563 [==============================] - 1s 950us/step - loss: 0.1833 - accuracy: 0.7307 - val_loss: 0.1851 - val_accuracy: 0.7391\n",
      "Epoch 538/1000\n",
      "563/563 [==============================] - 1s 958us/step - loss: 0.1808 - accuracy: 0.7369 - val_loss: 0.1830 - val_accuracy: 0.7402\n",
      "Epoch 539/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1807 - accuracy: 0.7348 - val_loss: 0.1846 - val_accuracy: 0.7402\n",
      "Epoch 540/1000\n",
      "563/563 [==============================] - 1s 998us/step - loss: 0.1820 - accuracy: 0.7361 - val_loss: 0.1853 - val_accuracy: 0.7394\n",
      "Epoch 541/1000\n",
      "563/563 [==============================] - 1s 974us/step - loss: 0.1815 - accuracy: 0.7345 - val_loss: 0.1846 - val_accuracy: 0.7387\n",
      "Epoch 542/1000\n",
      "563/563 [==============================] - 1s 972us/step - loss: 0.1799 - accuracy: 0.7377 - val_loss: 0.1871 - val_accuracy: 0.7367\n",
      "Epoch 543/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1799 - accuracy: 0.7378 - val_loss: 0.1838 - val_accuracy: 0.7387\n",
      "Epoch 544/1000\n",
      "563/563 [==============================] - 1s 975us/step - loss: 0.1834 - accuracy: 0.7311 - val_loss: 0.1850 - val_accuracy: 0.7383\n",
      "Epoch 545/1000\n",
      "563/563 [==============================] - 1s 958us/step - loss: 0.1798 - accuracy: 0.7389 - val_loss: 0.1852 - val_accuracy: 0.7407\n",
      "Epoch 546/1000\n",
      "563/563 [==============================] - 1s 974us/step - loss: 0.1821 - accuracy: 0.7325 - val_loss: 0.1843 - val_accuracy: 0.7400\n",
      "Epoch 547/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1812 - accuracy: 0.7355 - val_loss: 0.1858 - val_accuracy: 0.7388\n",
      "Epoch 548/1000\n",
      "563/563 [==============================] - 1s 979us/step - loss: 0.1790 - accuracy: 0.7423 - val_loss: 0.1847 - val_accuracy: 0.7389\n",
      "Epoch 549/1000\n",
      "563/563 [==============================] - 1s 968us/step - loss: 0.1773 - accuracy: 0.7446 - val_loss: 0.1857 - val_accuracy: 0.7383\n",
      "Epoch 550/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1820 - accuracy: 0.7332 - val_loss: 0.1846 - val_accuracy: 0.7387\n",
      "Epoch 551/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1819 - accuracy: 0.7354 - val_loss: 0.1862 - val_accuracy: 0.7383\n",
      "Epoch 552/1000\n",
      "563/563 [==============================] - 1s 969us/step - loss: 0.1833 - accuracy: 0.7314 - val_loss: 0.1848 - val_accuracy: 0.7378\n",
      "Epoch 553/1000\n",
      "563/563 [==============================] - 1s 965us/step - loss: 0.1802 - accuracy: 0.7386 - val_loss: 0.1838 - val_accuracy: 0.7379\n",
      "Epoch 554/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1782 - accuracy: 0.7410 - val_loss: 0.1860 - val_accuracy: 0.7383\n",
      "Epoch 555/1000\n",
      "563/563 [==============================] - 1s 966us/step - loss: 0.1822 - accuracy: 0.7347 - val_loss: 0.1862 - val_accuracy: 0.7388\n",
      "Epoch 556/1000\n",
      "563/563 [==============================] - 1s 965us/step - loss: 0.1827 - accuracy: 0.7352 - val_loss: 0.1839 - val_accuracy: 0.7387\n",
      "Epoch 557/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1803 - accuracy: 0.7356 - val_loss: 0.1855 - val_accuracy: 0.7380\n",
      "Epoch 558/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1811 - accuracy: 0.7363 - val_loss: 0.1848 - val_accuracy: 0.7398\n",
      "Epoch 559/1000\n",
      "563/563 [==============================] - 1s 967us/step - loss: 0.1796 - accuracy: 0.7411 - val_loss: 0.1839 - val_accuracy: 0.7383\n",
      "Epoch 560/1000\n",
      "563/563 [==============================] - 1s 975us/step - loss: 0.1845 - accuracy: 0.7308 - val_loss: 0.1846 - val_accuracy: 0.7392\n",
      "Epoch 561/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1807 - accuracy: 0.7353 - val_loss: 0.1837 - val_accuracy: 0.7391\n",
      "Epoch 562/1000\n",
      "563/563 [==============================] - 1s 979us/step - loss: 0.1816 - accuracy: 0.7358 - val_loss: 0.1843 - val_accuracy: 0.7384\n",
      "Epoch 563/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1808 - accuracy: 0.7387 - val_loss: 0.1847 - val_accuracy: 0.7365\n",
      "Epoch 564/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1807 - accuracy: 0.7360 - val_loss: 0.1851 - val_accuracy: 0.7385\n",
      "Epoch 565/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1824 - accuracy: 0.7349 - val_loss: 0.1845 - val_accuracy: 0.7394\n",
      "Epoch 566/1000\n",
      "563/563 [==============================] - 1s 963us/step - loss: 0.1808 - accuracy: 0.7350 - val_loss: 0.1881 - val_accuracy: 0.7398\n",
      "Epoch 567/1000\n",
      "563/563 [==============================] - 1s 968us/step - loss: 0.1816 - accuracy: 0.7377 - val_loss: 0.1846 - val_accuracy: 0.7393\n",
      "Epoch 568/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1822 - accuracy: 0.7361 - val_loss: 0.1840 - val_accuracy: 0.7392\n",
      "Epoch 569/1000\n",
      "563/563 [==============================] - 1s 970us/step - loss: 0.1795 - accuracy: 0.7385 - val_loss: 0.1836 - val_accuracy: 0.7384\n",
      "Epoch 570/1000\n",
      "563/563 [==============================] - 1s 968us/step - loss: 0.1797 - accuracy: 0.7383 - val_loss: 0.1852 - val_accuracy: 0.7381\n",
      "Epoch 571/1000\n",
      "563/563 [==============================] - 1s 970us/step - loss: 0.1808 - accuracy: 0.7352 - val_loss: 0.1833 - val_accuracy: 0.7391\n",
      "Epoch 572/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1810 - accuracy: 0.7385 - val_loss: 0.1851 - val_accuracy: 0.7378\n",
      "Epoch 573/1000\n",
      "563/563 [==============================] - 1s 970us/step - loss: 0.1789 - accuracy: 0.7433 - val_loss: 0.1848 - val_accuracy: 0.7372\n",
      "Epoch 574/1000\n",
      "563/563 [==============================] - 1s 958us/step - loss: 0.1819 - accuracy: 0.7332 - val_loss: 0.1847 - val_accuracy: 0.7387\n",
      "Epoch 575/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1795 - accuracy: 0.7379 - val_loss: 0.1848 - val_accuracy: 0.7383\n",
      "Epoch 576/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1807 - accuracy: 0.7357 - val_loss: 0.1864 - val_accuracy: 0.7378\n",
      "Epoch 577/1000\n",
      "563/563 [==============================] - 1s 976us/step - loss: 0.1795 - accuracy: 0.7402 - val_loss: 0.1845 - val_accuracy: 0.7380\n",
      "Epoch 578/1000\n",
      "563/563 [==============================] - 1s 954us/step - loss: 0.1837 - accuracy: 0.7292 - val_loss: 0.1836 - val_accuracy: 0.7396\n",
      "Epoch 579/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1812 - accuracy: 0.7363 - val_loss: 0.1851 - val_accuracy: 0.7372\n",
      "Epoch 580/1000\n",
      "563/563 [==============================] - 1s 961us/step - loss: 0.1809 - accuracy: 0.7394 - val_loss: 0.1863 - val_accuracy: 0.7394\n",
      "Epoch 581/1000\n",
      "563/563 [==============================] - 1s 950us/step - loss: 0.1814 - accuracy: 0.7331 - val_loss: 0.1844 - val_accuracy: 0.7400\n",
      "Epoch 582/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1805 - accuracy: 0.7373 - val_loss: 0.1886 - val_accuracy: 0.7361\n",
      "Epoch 583/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1841 - accuracy: 0.7320 - val_loss: 0.1846 - val_accuracy: 0.7381\n",
      "Epoch 584/1000\n",
      "563/563 [==============================] - 1s 972us/step - loss: 0.1810 - accuracy: 0.7373 - val_loss: 0.1835 - val_accuracy: 0.7394\n",
      "Epoch 585/1000\n",
      "563/563 [==============================] - 1s 974us/step - loss: 0.1829 - accuracy: 0.7334 - val_loss: 0.1842 - val_accuracy: 0.7389\n",
      "Epoch 586/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1807 - accuracy: 0.7372 - val_loss: 0.1869 - val_accuracy: 0.7393\n",
      "Epoch 587/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1808 - accuracy: 0.7412 - val_loss: 0.1837 - val_accuracy: 0.7391\n",
      "Epoch 588/1000\n",
      "563/563 [==============================] - 1s 981us/step - loss: 0.1803 - accuracy: 0.7365 - val_loss: 0.1842 - val_accuracy: 0.7392\n",
      "Epoch 589/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1810 - accuracy: 0.7377 - val_loss: 0.1846 - val_accuracy: 0.7383\n",
      "Epoch 590/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1812 - accuracy: 0.7365 - val_loss: 0.1868 - val_accuracy: 0.7396\n",
      "Epoch 591/1000\n",
      "563/563 [==============================] - 1s 963us/step - loss: 0.1792 - accuracy: 0.7404 - val_loss: 0.1839 - val_accuracy: 0.7411\n",
      "Epoch 592/1000\n",
      "563/563 [==============================] - 1s 956us/step - loss: 0.1819 - accuracy: 0.7329 - val_loss: 0.1831 - val_accuracy: 0.7405\n",
      "Epoch 593/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1832 - accuracy: 0.7350 - val_loss: 0.1850 - val_accuracy: 0.7392\n",
      "Epoch 594/1000\n",
      "563/563 [==============================] - 1s 961us/step - loss: 0.1824 - accuracy: 0.7338 - val_loss: 0.1845 - val_accuracy: 0.7405\n",
      "Epoch 595/1000\n",
      "563/563 [==============================] - 1s 956us/step - loss: 0.1811 - accuracy: 0.7378 - val_loss: 0.1848 - val_accuracy: 0.7393\n",
      "Epoch 596/1000\n",
      "563/563 [==============================] - 1s 961us/step - loss: 0.1831 - accuracy: 0.7324 - val_loss: 0.1852 - val_accuracy: 0.7383\n",
      "Epoch 597/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1800 - accuracy: 0.7374 - val_loss: 0.1887 - val_accuracy: 0.7396\n",
      "Epoch 598/1000\n",
      "563/563 [==============================] - 1s 968us/step - loss: 0.1816 - accuracy: 0.7347 - val_loss: 0.1838 - val_accuracy: 0.7387\n",
      "Epoch 599/1000\n",
      "563/563 [==============================] - 1s 974us/step - loss: 0.1830 - accuracy: 0.7340 - val_loss: 0.1844 - val_accuracy: 0.7387\n",
      "Epoch 600/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1815 - accuracy: 0.7372 - val_loss: 0.1840 - val_accuracy: 0.7402\n",
      "Epoch 601/1000\n",
      "563/563 [==============================] - 1s 998us/step - loss: 0.1816 - accuracy: 0.7343 - val_loss: 0.1844 - val_accuracy: 0.7394\n",
      "Epoch 602/1000\n",
      "563/563 [==============================] - 1s 956us/step - loss: 0.1812 - accuracy: 0.7356 - val_loss: 0.1860 - val_accuracy: 0.7384\n",
      "Epoch 603/1000\n",
      "563/563 [==============================] - 1s 954us/step - loss: 0.1825 - accuracy: 0.7330 - val_loss: 0.1846 - val_accuracy: 0.7389\n",
      "Epoch 604/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1823 - accuracy: 0.7350 - val_loss: 0.1847 - val_accuracy: 0.7401\n",
      "Epoch 605/1000\n",
      "563/563 [==============================] - 1s 965us/step - loss: 0.1810 - accuracy: 0.7355 - val_loss: 0.1834 - val_accuracy: 0.7409\n",
      "Epoch 606/1000\n",
      "563/563 [==============================] - 1s 968us/step - loss: 0.1834 - accuracy: 0.7293 - val_loss: 0.1847 - val_accuracy: 0.7384\n",
      "Epoch 607/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1811 - accuracy: 0.7367 - val_loss: 0.1861 - val_accuracy: 0.7384\n",
      "Epoch 608/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1831 - accuracy: 0.7315 - val_loss: 0.1849 - val_accuracy: 0.7387\n",
      "Epoch 609/1000\n",
      "563/563 [==============================] - 1s 970us/step - loss: 0.1804 - accuracy: 0.7376 - val_loss: 0.1854 - val_accuracy: 0.7387\n",
      "Epoch 610/1000\n",
      "563/563 [==============================] - 1s 979us/step - loss: 0.1844 - accuracy: 0.7292 - val_loss: 0.1856 - val_accuracy: 0.7397\n",
      "Epoch 611/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1803 - accuracy: 0.7363 - val_loss: 0.1849 - val_accuracy: 0.7403\n",
      "Epoch 612/1000\n",
      "563/563 [==============================] - 1s 975us/step - loss: 0.1826 - accuracy: 0.7340 - val_loss: 0.1857 - val_accuracy: 0.7397\n",
      "Epoch 613/1000\n",
      "563/563 [==============================] - 1s 982us/step - loss: 0.1821 - accuracy: 0.7358 - val_loss: 0.1847 - val_accuracy: 0.7415\n",
      "Epoch 614/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1828 - accuracy: 0.7337 - val_loss: 0.1853 - val_accuracy: 0.7400\n",
      "Epoch 615/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1792 - accuracy: 0.7397 - val_loss: 0.1855 - val_accuracy: 0.7397\n",
      "Epoch 616/1000\n",
      "563/563 [==============================] - 1s 958us/step - loss: 0.1794 - accuracy: 0.7395 - val_loss: 0.1863 - val_accuracy: 0.7398\n",
      "Epoch 617/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1816 - accuracy: 0.7354 - val_loss: 0.1865 - val_accuracy: 0.7378\n",
      "Epoch 618/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1805 - accuracy: 0.7381 - val_loss: 0.1860 - val_accuracy: 0.7372\n",
      "Epoch 619/1000\n",
      "563/563 [==============================] - 1s 975us/step - loss: 0.1811 - accuracy: 0.7349 - val_loss: 0.1857 - val_accuracy: 0.7394\n",
      "Epoch 620/1000\n",
      "563/563 [==============================] - 1s 959us/step - loss: 0.1806 - accuracy: 0.7372 - val_loss: 0.1861 - val_accuracy: 0.7358\n",
      "Epoch 621/1000\n",
      "563/563 [==============================] - 1s 998us/step - loss: 0.1847 - accuracy: 0.7296 - val_loss: 0.1859 - val_accuracy: 0.7357\n",
      "Epoch 622/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1832 - accuracy: 0.7342 - val_loss: 0.1854 - val_accuracy: 0.7393\n",
      "Epoch 623/1000\n",
      "563/563 [==============================] - 1s 965us/step - loss: 0.1798 - accuracy: 0.7393 - val_loss: 0.1850 - val_accuracy: 0.7400\n",
      "Epoch 624/1000\n",
      "563/563 [==============================] - 1s 972us/step - loss: 0.1795 - accuracy: 0.7396 - val_loss: 0.1851 - val_accuracy: 0.7384\n",
      "Epoch 625/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1826 - accuracy: 0.7333 - val_loss: 0.1841 - val_accuracy: 0.7384\n",
      "Epoch 626/1000\n",
      "563/563 [==============================] - 1s 986us/step - loss: 0.1785 - accuracy: 0.7414 - val_loss: 0.1855 - val_accuracy: 0.7383\n",
      "Epoch 627/1000\n",
      "563/563 [==============================] - 1s 954us/step - loss: 0.1829 - accuracy: 0.7334 - val_loss: 0.1842 - val_accuracy: 0.7391\n",
      "Epoch 628/1000\n",
      "563/563 [==============================] - 1s 970us/step - loss: 0.1845 - accuracy: 0.7281 - val_loss: 0.1841 - val_accuracy: 0.7393\n",
      "Epoch 629/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1821 - accuracy: 0.7353 - val_loss: 0.1864 - val_accuracy: 0.7383\n",
      "Epoch 630/1000\n",
      "563/563 [==============================] - 1s 970us/step - loss: 0.1826 - accuracy: 0.7347 - val_loss: 0.1847 - val_accuracy: 0.7376\n",
      "Epoch 631/1000\n",
      "563/563 [==============================] - 1s 986us/step - loss: 0.1802 - accuracy: 0.7387 - val_loss: 0.1860 - val_accuracy: 0.7379\n",
      "Epoch 632/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1818 - accuracy: 0.7350 - val_loss: 0.1885 - val_accuracy: 0.7380\n",
      "Epoch 633/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1815 - accuracy: 0.7373 - val_loss: 0.1845 - val_accuracy: 0.7380\n",
      "Epoch 634/1000\n",
      "563/563 [==============================] - 1s 974us/step - loss: 0.1815 - accuracy: 0.7350 - val_loss: 0.1849 - val_accuracy: 0.7367\n",
      "Epoch 635/1000\n",
      "563/563 [==============================] - 1s 952us/step - loss: 0.1804 - accuracy: 0.7360 - val_loss: 0.1845 - val_accuracy: 0.7380\n",
      "Epoch 636/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1807 - accuracy: 0.7387 - val_loss: 0.1854 - val_accuracy: 0.7385\n",
      "Epoch 637/1000\n",
      "563/563 [==============================] - 1s 958us/step - loss: 0.1808 - accuracy: 0.7372 - val_loss: 0.1873 - val_accuracy: 0.7380\n",
      "Epoch 638/1000\n",
      "563/563 [==============================] - 1s 956us/step - loss: 0.1826 - accuracy: 0.7329 - val_loss: 0.1855 - val_accuracy: 0.7379\n",
      "Epoch 639/1000\n",
      "563/563 [==============================] - 1s 985us/step - loss: 0.1802 - accuracy: 0.7394 - val_loss: 0.1883 - val_accuracy: 0.7385\n",
      "Epoch 640/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1799 - accuracy: 0.7422 - val_loss: 0.1858 - val_accuracy: 0.7368\n",
      "Epoch 641/1000\n",
      "563/563 [==============================] - 1s 997us/step - loss: 0.1799 - accuracy: 0.7374 - val_loss: 0.1865 - val_accuracy: 0.7380\n",
      "Epoch 642/1000\n",
      "563/563 [==============================] - 1s 979us/step - loss: 0.1809 - accuracy: 0.7365 - val_loss: 0.1858 - val_accuracy: 0.7376\n",
      "Epoch 643/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1789 - accuracy: 0.7437 - val_loss: 0.1863 - val_accuracy: 0.7387\n",
      "Epoch 644/1000\n",
      "563/563 [==============================] - 1s 972us/step - loss: 0.1808 - accuracy: 0.7351 - val_loss: 0.1863 - val_accuracy: 0.7381\n",
      "Epoch 645/1000\n",
      "563/563 [==============================] - 1s 943us/step - loss: 0.1809 - accuracy: 0.7342 - val_loss: 0.1851 - val_accuracy: 0.7387\n",
      "Epoch 646/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1819 - accuracy: 0.7366 - val_loss: 0.1852 - val_accuracy: 0.7379\n",
      "Epoch 647/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1818 - accuracy: 0.7359 - val_loss: 0.1853 - val_accuracy: 0.7385\n",
      "Epoch 648/1000\n",
      "563/563 [==============================] - 1s 986us/step - loss: 0.1780 - accuracy: 0.7416 - val_loss: 0.1858 - val_accuracy: 0.7359\n",
      "Epoch 649/1000\n",
      "563/563 [==============================] - 1s 978us/step - loss: 0.1790 - accuracy: 0.7384 - val_loss: 0.1846 - val_accuracy: 0.7380\n",
      "Epoch 650/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1817 - accuracy: 0.7377 - val_loss: 0.1854 - val_accuracy: 0.7387\n",
      "Epoch 651/1000\n",
      "563/563 [==============================] - 1s 974us/step - loss: 0.1820 - accuracy: 0.7327 - val_loss: 0.1849 - val_accuracy: 0.7378\n",
      "Epoch 652/1000\n",
      "563/563 [==============================] - 1s 963us/step - loss: 0.1794 - accuracy: 0.7403 - val_loss: 0.1879 - val_accuracy: 0.7371\n",
      "Epoch 653/1000\n",
      "563/563 [==============================] - 1s 991us/step - loss: 0.1819 - accuracy: 0.7364 - val_loss: 0.1855 - val_accuracy: 0.7372\n",
      "Epoch 654/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1819 - accuracy: 0.7370 - val_loss: 0.1862 - val_accuracy: 0.7371\n",
      "Epoch 655/1000\n",
      "563/563 [==============================] - 1s 974us/step - loss: 0.1804 - accuracy: 0.7403 - val_loss: 0.1871 - val_accuracy: 0.7379\n",
      "Epoch 656/1000\n",
      "563/563 [==============================] - 1s 972us/step - loss: 0.1802 - accuracy: 0.7355 - val_loss: 0.1860 - val_accuracy: 0.7375\n",
      "Epoch 657/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1811 - accuracy: 0.7380 - val_loss: 0.1863 - val_accuracy: 0.7372\n",
      "Epoch 658/1000\n",
      "563/563 [==============================] - 1s 991us/step - loss: 0.1810 - accuracy: 0.7385 - val_loss: 0.1853 - val_accuracy: 0.7359\n",
      "Epoch 659/1000\n",
      "563/563 [==============================] - 1s 982us/step - loss: 0.1799 - accuracy: 0.7388 - val_loss: 0.1871 - val_accuracy: 0.7374\n",
      "Epoch 660/1000\n",
      "563/563 [==============================] - 1s 982us/step - loss: 0.1799 - accuracy: 0.7400 - val_loss: 0.1877 - val_accuracy: 0.7374\n",
      "Epoch 661/1000\n",
      "563/563 [==============================] - ETA: 0s - loss: 0.1807 - accuracy: 0.73 - 1s 1ms/step - loss: 0.1807 - accuracy: 0.7372 - val_loss: 0.1894 - val_accuracy: 0.7379\n",
      "Epoch 662/1000\n",
      "563/563 [==============================] - 1s 972us/step - loss: 0.1806 - accuracy: 0.7402 - val_loss: 0.1857 - val_accuracy: 0.7380\n",
      "Epoch 663/1000\n",
      "563/563 [==============================] - 1s 979us/step - loss: 0.1820 - accuracy: 0.7330 - val_loss: 0.1848 - val_accuracy: 0.7376\n",
      "Epoch 664/1000\n",
      "563/563 [==============================] - 1s 993us/step - loss: 0.1818 - accuracy: 0.7356 - val_loss: 0.1862 - val_accuracy: 0.7375\n",
      "Epoch 665/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1828 - accuracy: 0.7333 - val_loss: 0.1885 - val_accuracy: 0.7372\n",
      "Epoch 666/1000\n",
      "563/563 [==============================] - 1s 968us/step - loss: 0.1799 - accuracy: 0.7377 - val_loss: 0.1868 - val_accuracy: 0.7379\n",
      "Epoch 667/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1801 - accuracy: 0.7398 - val_loss: 0.1849 - val_accuracy: 0.7378\n",
      "Epoch 668/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1823 - accuracy: 0.7331 - val_loss: 0.1865 - val_accuracy: 0.7380\n",
      "Epoch 669/1000\n",
      "563/563 [==============================] - 1s 965us/step - loss: 0.1797 - accuracy: 0.7428 - val_loss: 0.1901 - val_accuracy: 0.7327\n",
      "Epoch 670/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1784 - accuracy: 0.7408 - val_loss: 0.1861 - val_accuracy: 0.7352\n",
      "Epoch 671/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1824 - accuracy: 0.7324 - val_loss: 0.1854 - val_accuracy: 0.7378\n",
      "Epoch 672/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1806 - accuracy: 0.7356 - val_loss: 0.1852 - val_accuracy: 0.7372\n",
      "Epoch 673/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1831 - accuracy: 0.7341 - val_loss: 0.1844 - val_accuracy: 0.7374\n",
      "Epoch 674/1000\n",
      "563/563 [==============================] - 1s 975us/step - loss: 0.1817 - accuracy: 0.7372 - val_loss: 0.1850 - val_accuracy: 0.7380\n",
      "Epoch 675/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1806 - accuracy: 0.7339 - val_loss: 0.1869 - val_accuracy: 0.7348\n",
      "Epoch 676/1000\n",
      "563/563 [==============================] - 1s 966us/step - loss: 0.1803 - accuracy: 0.7374 - val_loss: 0.1894 - val_accuracy: 0.7389\n",
      "Epoch 677/1000\n",
      "563/563 [==============================] - 1s 963us/step - loss: 0.1791 - accuracy: 0.7443 - val_loss: 0.1866 - val_accuracy: 0.7346\n",
      "Epoch 678/1000\n",
      "563/563 [==============================] - 1s 977us/step - loss: 0.1804 - accuracy: 0.7378 - val_loss: 0.1839 - val_accuracy: 0.7383\n",
      "Epoch 679/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1792 - accuracy: 0.7406 - val_loss: 0.1851 - val_accuracy: 0.7362\n",
      "Epoch 680/1000\n",
      "563/563 [==============================] - 1s 966us/step - loss: 0.1823 - accuracy: 0.7319 - val_loss: 0.1844 - val_accuracy: 0.7397\n",
      "Epoch 681/1000\n",
      "563/563 [==============================] - 1s 963us/step - loss: 0.1812 - accuracy: 0.7340 - val_loss: 0.1862 - val_accuracy: 0.7349\n",
      "Epoch 682/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1814 - accuracy: 0.7347 - val_loss: 0.1847 - val_accuracy: 0.7371\n",
      "Epoch 683/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1798 - accuracy: 0.7354 - val_loss: 0.1846 - val_accuracy: 0.7381\n",
      "Epoch 684/1000\n",
      "563/563 [==============================] - 1s 968us/step - loss: 0.1815 - accuracy: 0.7341 - val_loss: 0.1851 - val_accuracy: 0.7393\n",
      "Epoch 685/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1809 - accuracy: 0.7339 - val_loss: 0.1852 - val_accuracy: 0.7387\n",
      "Epoch 686/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1784 - accuracy: 0.7405 - val_loss: 0.1844 - val_accuracy: 0.7384\n",
      "Epoch 687/1000\n",
      "563/563 [==============================] - 1s 963us/step - loss: 0.1809 - accuracy: 0.7347 - val_loss: 0.1868 - val_accuracy: 0.7383\n",
      "Epoch 688/1000\n",
      "563/563 [==============================] - 1s 954us/step - loss: 0.1841 - accuracy: 0.7313 - val_loss: 0.1867 - val_accuracy: 0.7376\n",
      "Epoch 689/1000\n",
      "563/563 [==============================] - 1s 981us/step - loss: 0.1819 - accuracy: 0.7367 - val_loss: 0.1839 - val_accuracy: 0.7388\n",
      "Epoch 690/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1807 - accuracy: 0.7368 - val_loss: 0.1845 - val_accuracy: 0.7371\n",
      "Epoch 691/1000\n",
      "563/563 [==============================] - 1s 966us/step - loss: 0.1830 - accuracy: 0.7293 - val_loss: 0.1845 - val_accuracy: 0.7378\n",
      "Epoch 692/1000\n",
      "563/563 [==============================] - 1s 961us/step - loss: 0.1848 - accuracy: 0.7273 - val_loss: 0.1877 - val_accuracy: 0.7383\n",
      "Epoch 693/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1834 - accuracy: 0.7334 - val_loss: 0.1849 - val_accuracy: 0.7371\n",
      "Epoch 694/1000\n",
      "563/563 [==============================] - 1s 981us/step - loss: 0.1814 - accuracy: 0.7341 - val_loss: 0.1840 - val_accuracy: 0.7379\n",
      "Epoch 695/1000\n",
      "563/563 [==============================] - 1s 970us/step - loss: 0.1815 - accuracy: 0.7334 - val_loss: 0.1913 - val_accuracy: 0.7375\n",
      "Epoch 696/1000\n",
      "563/563 [==============================] - 1s 997us/step - loss: 0.1845 - accuracy: 0.7309 - val_loss: 0.1850 - val_accuracy: 0.7371\n",
      "Epoch 697/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1797 - accuracy: 0.7374 - val_loss: 0.1847 - val_accuracy: 0.7363\n",
      "Epoch 698/1000\n",
      "563/563 [==============================] - 1s 963us/step - loss: 0.1799 - accuracy: 0.7390 - val_loss: 0.1833 - val_accuracy: 0.7383\n",
      "Epoch 699/1000\n",
      "563/563 [==============================] - 1s 975us/step - loss: 0.1803 - accuracy: 0.7400 - val_loss: 0.1866 - val_accuracy: 0.7371\n",
      "Epoch 700/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1842 - accuracy: 0.7284 - val_loss: 0.1840 - val_accuracy: 0.7378\n",
      "Epoch 701/1000\n",
      "563/563 [==============================] - 1s 974us/step - loss: 0.1816 - accuracy: 0.7349 - val_loss: 0.1844 - val_accuracy: 0.7387\n",
      "Epoch 702/1000\n",
      "563/563 [==============================] - 1s 956us/step - loss: 0.1849 - accuracy: 0.7274 - val_loss: 0.1836 - val_accuracy: 0.7376\n",
      "Epoch 703/1000\n",
      "563/563 [==============================] - 1s 979us/step - loss: 0.1839 - accuracy: 0.7264 - val_loss: 0.1840 - val_accuracy: 0.7378\n",
      "Epoch 704/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1802 - accuracy: 0.7412 - val_loss: 0.1839 - val_accuracy: 0.7367\n",
      "Epoch 705/1000\n",
      "563/563 [==============================] - 1s 972us/step - loss: 0.1812 - accuracy: 0.7367 - val_loss: 0.1837 - val_accuracy: 0.7374\n",
      "Epoch 706/1000\n",
      "563/563 [==============================] - 1s 968us/step - loss: 0.1848 - accuracy: 0.7263 - val_loss: 0.1844 - val_accuracy: 0.7354\n",
      "Epoch 707/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1841 - accuracy: 0.7286 - val_loss: 0.1847 - val_accuracy: 0.7388\n",
      "Epoch 708/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1830 - accuracy: 0.7292 - val_loss: 0.1846 - val_accuracy: 0.7396\n",
      "Epoch 709/1000\n",
      "563/563 [==============================] - 1s 952us/step - loss: 0.1821 - accuracy: 0.7331 - val_loss: 0.1833 - val_accuracy: 0.7398\n",
      "Epoch 710/1000\n",
      "563/563 [==============================] - 1s 982us/step - loss: 0.1822 - accuracy: 0.7348 - val_loss: 0.1858 - val_accuracy: 0.7406\n",
      "Epoch 711/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1852 - accuracy: 0.7253 - val_loss: 0.1821 - val_accuracy: 0.7407\n",
      "Epoch 712/1000\n",
      "563/563 [==============================] - 1s 987us/step - loss: 0.1824 - accuracy: 0.7321 - val_loss: 0.1839 - val_accuracy: 0.7384\n",
      "Epoch 713/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1845 - accuracy: 0.7238 - val_loss: 0.1837 - val_accuracy: 0.7385\n",
      "Epoch 714/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1797 - accuracy: 0.7398 - val_loss: 0.1838 - val_accuracy: 0.7375\n",
      "Epoch 715/1000\n",
      "563/563 [==============================] - 1s 979us/step - loss: 0.1842 - accuracy: 0.7266 - val_loss: 0.1839 - val_accuracy: 0.7385\n",
      "Epoch 716/1000\n",
      "563/563 [==============================] - 1s 965us/step - loss: 0.1815 - accuracy: 0.7326 - val_loss: 0.1848 - val_accuracy: 0.7350\n",
      "Epoch 717/1000\n",
      "563/563 [==============================] - 1s 984us/step - loss: 0.1795 - accuracy: 0.7313 - val_loss: 0.1839 - val_accuracy: 0.7357\n",
      "Epoch 718/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1821 - accuracy: 0.7303 - val_loss: 0.1863 - val_accuracy: 0.7350\n",
      "Epoch 719/1000\n",
      "563/563 [==============================] - 1s 972us/step - loss: 0.1850 - accuracy: 0.7244 - val_loss: 0.1865 - val_accuracy: 0.7304\n",
      "Epoch 720/1000\n",
      "563/563 [==============================] - 1s 959us/step - loss: 0.1820 - accuracy: 0.7293 - val_loss: 0.1847 - val_accuracy: 0.7295\n",
      "Epoch 721/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1843 - accuracy: 0.7257 - val_loss: 0.1850 - val_accuracy: 0.7284\n",
      "Epoch 722/1000\n",
      "563/563 [==============================] - 1s 988us/step - loss: 0.1834 - accuracy: 0.7289 - val_loss: 0.1863 - val_accuracy: 0.7282\n",
      "Epoch 723/1000\n",
      "563/563 [==============================] - 1s 968us/step - loss: 0.1853 - accuracy: 0.7284 - val_loss: 0.1841 - val_accuracy: 0.7326\n",
      "Epoch 724/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1828 - accuracy: 0.7342 - val_loss: 0.1846 - val_accuracy: 0.7288\n",
      "Epoch 725/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1838 - accuracy: 0.7279 - val_loss: 0.1844 - val_accuracy: 0.7330\n",
      "Epoch 726/1000\n",
      "563/563 [==============================] - 1s 964us/step - loss: 0.1837 - accuracy: 0.7279 - val_loss: 0.1848 - val_accuracy: 0.7280\n",
      "Epoch 727/1000\n",
      "563/563 [==============================] - 1s 949us/step - loss: 0.1841 - accuracy: 0.7268 - val_loss: 0.1846 - val_accuracy: 0.7300\n",
      "Epoch 728/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1843 - accuracy: 0.7283 - val_loss: 0.1835 - val_accuracy: 0.7327\n",
      "Epoch 729/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1867 - accuracy: 0.7221 - val_loss: 0.1846 - val_accuracy: 0.7322\n",
      "Epoch 730/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1832 - accuracy: 0.7316 - val_loss: 0.1856 - val_accuracy: 0.7299\n",
      "Epoch 731/1000\n",
      "563/563 [==============================] - 1s 990us/step - loss: 0.1850 - accuracy: 0.7314 - val_loss: 0.1842 - val_accuracy: 0.7289\n",
      "Epoch 732/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1852 - accuracy: 0.7263 - val_loss: 0.1847 - val_accuracy: 0.7318\n",
      "Epoch 733/1000\n",
      "563/563 [==============================] - 1s 975us/step - loss: 0.1834 - accuracy: 0.7313 - val_loss: 0.1848 - val_accuracy: 0.7323\n",
      "Epoch 734/1000\n",
      "563/563 [==============================] - 1s 970us/step - loss: 0.1831 - accuracy: 0.7288 - val_loss: 0.1835 - val_accuracy: 0.7328\n",
      "Epoch 735/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1842 - accuracy: 0.7303 - val_loss: 0.1848 - val_accuracy: 0.7313\n",
      "Epoch 736/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1837 - accuracy: 0.7290 - val_loss: 0.1858 - val_accuracy: 0.7337\n",
      "Epoch 737/1000\n",
      "563/563 [==============================] - 1s 970us/step - loss: 0.1844 - accuracy: 0.7322 - val_loss: 0.1842 - val_accuracy: 0.7341\n",
      "Epoch 738/1000\n",
      "563/563 [==============================] - 1s 977us/step - loss: 0.1853 - accuracy: 0.7268 - val_loss: 0.1837 - val_accuracy: 0.7331\n",
      "Epoch 739/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1799 - accuracy: 0.7366 - val_loss: 0.1846 - val_accuracy: 0.7334\n",
      "Epoch 740/1000\n",
      "563/563 [==============================] - 1s 964us/step - loss: 0.1851 - accuracy: 0.7270 - val_loss: 0.1835 - val_accuracy: 0.7346\n",
      "Epoch 741/1000\n",
      "563/563 [==============================] - 1s 977us/step - loss: 0.1824 - accuracy: 0.7342 - val_loss: 0.1830 - val_accuracy: 0.7344\n",
      "Epoch 742/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1806 - accuracy: 0.7371 - val_loss: 0.1838 - val_accuracy: 0.7330\n",
      "Epoch 743/1000\n",
      "563/563 [==============================] - 1s 997us/step - loss: 0.1825 - accuracy: 0.7334 - val_loss: 0.1857 - val_accuracy: 0.7339\n",
      "Epoch 744/1000\n",
      "563/563 [==============================] - 1s 968us/step - loss: 0.1861 - accuracy: 0.7271 - val_loss: 0.1835 - val_accuracy: 0.7348\n",
      "Epoch 745/1000\n",
      "563/563 [==============================] - 1s 954us/step - loss: 0.1851 - accuracy: 0.7288 - val_loss: 0.1841 - val_accuracy: 0.7334\n",
      "Epoch 746/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1831 - accuracy: 0.7301 - val_loss: 0.1840 - val_accuracy: 0.7340\n",
      "Epoch 747/1000\n",
      "563/563 [==============================] - 1s 979us/step - loss: 0.1825 - accuracy: 0.7343 - val_loss: 0.1868 - val_accuracy: 0.7305\n",
      "Epoch 748/1000\n",
      "563/563 [==============================] - 1s 972us/step - loss: 0.1856 - accuracy: 0.7281 - val_loss: 0.1857 - val_accuracy: 0.7317\n",
      "Epoch 749/1000\n",
      "563/563 [==============================] - 1s 968us/step - loss: 0.1842 - accuracy: 0.7324 - val_loss: 0.1847 - val_accuracy: 0.7332\n",
      "Epoch 750/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1859 - accuracy: 0.7276 - val_loss: 0.1840 - val_accuracy: 0.7348\n",
      "Epoch 751/1000\n",
      "563/563 [==============================] - 1s 963us/step - loss: 0.1857 - accuracy: 0.7258 - val_loss: 0.1837 - val_accuracy: 0.7346\n",
      "Epoch 752/1000\n",
      "563/563 [==============================] - 1s 977us/step - loss: 0.1824 - accuracy: 0.7367 - val_loss: 0.1882 - val_accuracy: 0.7319\n",
      "Epoch 753/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1839 - accuracy: 0.7326 - val_loss: 0.1839 - val_accuracy: 0.7330\n",
      "Epoch 754/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1836 - accuracy: 0.7300 - val_loss: 0.1853 - val_accuracy: 0.7315\n",
      "Epoch 755/1000\n",
      "563/563 [==============================] - 1s 950us/step - loss: 0.1849 - accuracy: 0.7330 - val_loss: 0.1843 - val_accuracy: 0.7339\n",
      "Epoch 756/1000\n",
      "563/563 [==============================] - 1s 967us/step - loss: 0.1831 - accuracy: 0.7364 - val_loss: 0.1842 - val_accuracy: 0.7341\n",
      "Epoch 757/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1848 - accuracy: 0.7292 - val_loss: 0.1861 - val_accuracy: 0.7315\n",
      "Epoch 758/1000\n",
      "563/563 [==============================] - 1s 965us/step - loss: 0.1844 - accuracy: 0.7333 - val_loss: 0.1857 - val_accuracy: 0.7295\n",
      "Epoch 759/1000\n",
      "563/563 [==============================] - 1s 963us/step - loss: 0.1856 - accuracy: 0.7268 - val_loss: 0.1865 - val_accuracy: 0.7286\n",
      "Epoch 760/1000\n",
      "563/563 [==============================] - 1s 997us/step - loss: 0.1842 - accuracy: 0.7321 - val_loss: 0.1846 - val_accuracy: 0.7321\n",
      "Epoch 761/1000\n",
      "563/563 [==============================] - 1s 991us/step - loss: 0.1807 - accuracy: 0.7410 - val_loss: 0.1859 - val_accuracy: 0.7306\n",
      "Epoch 762/1000\n",
      "563/563 [==============================] - 1s 956us/step - loss: 0.1844 - accuracy: 0.7316 - val_loss: 0.1858 - val_accuracy: 0.7297\n",
      "Epoch 763/1000\n",
      "563/563 [==============================] - 1s 963us/step - loss: 0.1853 - accuracy: 0.7317 - val_loss: 0.1848 - val_accuracy: 0.7328\n",
      "Epoch 764/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1834 - accuracy: 0.7335 - val_loss: 0.1849 - val_accuracy: 0.7304\n",
      "Epoch 765/1000\n",
      "563/563 [==============================] - 1s 990us/step - loss: 0.1862 - accuracy: 0.7284 - val_loss: 0.1862 - val_accuracy: 0.7340\n",
      "Epoch 766/1000\n",
      "563/563 [==============================] - 1s 974us/step - loss: 0.1865 - accuracy: 0.7265 - val_loss: 0.1842 - val_accuracy: 0.7340\n",
      "Epoch 767/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1834 - accuracy: 0.7359 - val_loss: 0.1849 - val_accuracy: 0.7341\n",
      "Epoch 768/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1827 - accuracy: 0.7371 - val_loss: 0.1870 - val_accuracy: 0.7311\n",
      "Epoch 769/1000\n",
      "563/563 [==============================] - 1s 958us/step - loss: 0.1852 - accuracy: 0.7317 - val_loss: 0.1868 - val_accuracy: 0.7311\n",
      "Epoch 770/1000\n",
      "563/563 [==============================] - 1s 961us/step - loss: 0.1854 - accuracy: 0.7288 - val_loss: 0.1891 - val_accuracy: 0.7262\n",
      "Epoch 771/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1864 - accuracy: 0.7294 - val_loss: 0.1882 - val_accuracy: 0.7258\n",
      "Epoch 772/1000\n",
      "563/563 [==============================] - 1s 975us/step - loss: 0.1846 - accuracy: 0.7323 - val_loss: 0.1847 - val_accuracy: 0.7346\n",
      "Epoch 773/1000\n",
      "563/563 [==============================] - 1s 958us/step - loss: 0.1835 - accuracy: 0.7346 - val_loss: 0.1847 - val_accuracy: 0.7341\n",
      "Epoch 774/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1832 - accuracy: 0.7334 - val_loss: 0.1849 - val_accuracy: 0.7367\n",
      "Epoch 775/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1829 - accuracy: 0.7347 - val_loss: 0.1864 - val_accuracy: 0.7287\n",
      "Epoch 776/1000\n",
      "563/563 [==============================] - 1s 954us/step - loss: 0.1839 - accuracy: 0.7343 - val_loss: 0.1853 - val_accuracy: 0.7340\n",
      "Epoch 777/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1832 - accuracy: 0.7308 - val_loss: 0.1877 - val_accuracy: 0.7265\n",
      "Epoch 778/1000\n",
      "563/563 [==============================] - 1s 994us/step - loss: 0.1835 - accuracy: 0.7324 - val_loss: 0.1840 - val_accuracy: 0.7354\n",
      "Epoch 779/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1851 - accuracy: 0.7312 - val_loss: 0.1849 - val_accuracy: 0.7341\n",
      "Epoch 780/1000\n",
      "563/563 [==============================] - 1s 958us/step - loss: 0.1821 - accuracy: 0.7369 - val_loss: 0.1849 - val_accuracy: 0.7322\n",
      "Epoch 781/1000\n",
      "563/563 [==============================] - 1s 961us/step - loss: 0.1863 - accuracy: 0.7285 - val_loss: 0.1919 - val_accuracy: 0.7253\n",
      "Epoch 782/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1837 - accuracy: 0.7327 - val_loss: 0.1854 - val_accuracy: 0.7331\n",
      "Epoch 783/1000\n",
      "563/563 [==============================] - 1s 959us/step - loss: 0.1826 - accuracy: 0.7393 - val_loss: 0.1848 - val_accuracy: 0.7314\n",
      "Epoch 784/1000\n",
      "563/563 [==============================] - 1s 963us/step - loss: 0.1850 - accuracy: 0.7302 - val_loss: 0.1852 - val_accuracy: 0.7339\n",
      "Epoch 785/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1859 - accuracy: 0.7330 - val_loss: 0.1849 - val_accuracy: 0.7345\n",
      "Epoch 786/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1850 - accuracy: 0.7323 - val_loss: 0.1853 - val_accuracy: 0.7310\n",
      "Epoch 787/1000\n",
      "563/563 [==============================] - 1s 962us/step - loss: 0.1868 - accuracy: 0.7288 - val_loss: 0.1869 - val_accuracy: 0.7279\n",
      "Epoch 788/1000\n",
      "563/563 [==============================] - 1s 961us/step - loss: 0.1824 - accuracy: 0.7382 - val_loss: 0.1847 - val_accuracy: 0.7349\n",
      "Epoch 789/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1850 - accuracy: 0.7329 - val_loss: 0.1848 - val_accuracy: 0.7317\n",
      "Epoch 790/1000\n",
      "563/563 [==============================] - 1s 984us/step - loss: 0.1839 - accuracy: 0.7332 - val_loss: 0.1859 - val_accuracy: 0.7301\n",
      "Epoch 791/1000\n",
      "563/563 [==============================] - 1s 966us/step - loss: 0.1852 - accuracy: 0.7310 - val_loss: 0.1860 - val_accuracy: 0.7326\n",
      "Epoch 792/1000\n",
      "563/563 [==============================] - 1s 991us/step - loss: 0.1844 - accuracy: 0.7312 - val_loss: 0.1872 - val_accuracy: 0.7299\n",
      "Epoch 793/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1863 - accuracy: 0.7287 - val_loss: 0.1877 - val_accuracy: 0.7324\n",
      "Epoch 794/1000\n",
      "563/563 [==============================] - 1s 950us/step - loss: 0.1854 - accuracy: 0.7308 - val_loss: 0.1858 - val_accuracy: 0.7349\n",
      "Epoch 795/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1855 - accuracy: 0.7301 - val_loss: 0.1855 - val_accuracy: 0.7346\n",
      "Epoch 796/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1858 - accuracy: 0.7288 - val_loss: 0.1858 - val_accuracy: 0.7323\n",
      "Epoch 797/1000\n",
      "563/563 [==============================] - 1s 961us/step - loss: 0.1831 - accuracy: 0.7360 - val_loss: 0.1855 - val_accuracy: 0.7334\n",
      "Epoch 798/1000\n",
      "563/563 [==============================] - 1s 959us/step - loss: 0.1842 - accuracy: 0.7315 - val_loss: 0.1883 - val_accuracy: 0.7313\n",
      "Epoch 799/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1842 - accuracy: 0.7333 - val_loss: 0.1850 - val_accuracy: 0.7350\n",
      "Epoch 800/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1848 - accuracy: 0.7304 - val_loss: 0.1850 - val_accuracy: 0.7322\n",
      "Epoch 801/1000\n",
      "563/563 [==============================] - 1s 988us/step - loss: 0.1844 - accuracy: 0.7349 - val_loss: 0.1848 - val_accuracy: 0.7361\n",
      "Epoch 802/1000\n",
      "563/563 [==============================] - 1s 963us/step - loss: 0.1836 - accuracy: 0.7339 - val_loss: 0.1849 - val_accuracy: 0.7350\n",
      "Epoch 803/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1849 - accuracy: 0.7343 - val_loss: 0.1847 - val_accuracy: 0.7361\n",
      "Epoch 804/1000\n",
      "563/563 [==============================] - 1s 982us/step - loss: 0.1830 - accuracy: 0.7344 - val_loss: 0.1854 - val_accuracy: 0.7356\n",
      "Epoch 805/1000\n",
      "563/563 [==============================] - 1s 965us/step - loss: 0.1843 - accuracy: 0.7338 - val_loss: 0.1869 - val_accuracy: 0.7324\n",
      "Epoch 806/1000\n",
      "563/563 [==============================] - 1s 956us/step - loss: 0.1861 - accuracy: 0.7334 - val_loss: 0.1864 - val_accuracy: 0.7324\n",
      "Epoch 807/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1847 - accuracy: 0.7332 - val_loss: 0.1840 - val_accuracy: 0.7354\n",
      "Epoch 808/1000\n",
      "563/563 [==============================] - 1s 965us/step - loss: 0.1860 - accuracy: 0.7319 - val_loss: 0.1854 - val_accuracy: 0.7322\n",
      "Epoch 809/1000\n",
      "563/563 [==============================] - 1s 970us/step - loss: 0.1827 - accuracy: 0.7372 - val_loss: 0.1861 - val_accuracy: 0.7311\n",
      "Epoch 810/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1836 - accuracy: 0.7328 - val_loss: 0.1845 - val_accuracy: 0.7361\n",
      "Epoch 811/1000\n",
      "563/563 [==============================] - 1s 972us/step - loss: 0.1819 - accuracy: 0.7396 - val_loss: 0.1863 - val_accuracy: 0.7318\n",
      "Epoch 812/1000\n",
      "563/563 [==============================] - 1s 974us/step - loss: 0.1868 - accuracy: 0.7292 - val_loss: 0.1847 - val_accuracy: 0.7346\n",
      "Epoch 813/1000\n",
      "563/563 [==============================] - 1s 972us/step - loss: 0.1831 - accuracy: 0.7344 - val_loss: 0.1862 - val_accuracy: 0.7310\n",
      "Epoch 814/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1837 - accuracy: 0.7321 - val_loss: 0.1857 - val_accuracy: 0.7332\n",
      "Epoch 815/1000\n",
      "563/563 [==============================] - 1s 958us/step - loss: 0.1826 - accuracy: 0.7387 - val_loss: 0.1848 - val_accuracy: 0.7346\n",
      "Epoch 816/1000\n",
      "563/563 [==============================] - 1s 965us/step - loss: 0.1853 - accuracy: 0.7338 - val_loss: 0.1851 - val_accuracy: 0.7365\n",
      "Epoch 817/1000\n",
      "563/563 [==============================] - 1s 998us/step - loss: 0.1836 - accuracy: 0.7339 - val_loss: 0.1855 - val_accuracy: 0.7352\n",
      "Epoch 818/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1827 - accuracy: 0.7377 - val_loss: 0.1850 - val_accuracy: 0.7349\n",
      "Epoch 819/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1846 - accuracy: 0.7343 - val_loss: 0.1849 - val_accuracy: 0.7328\n",
      "Epoch 820/1000\n",
      "563/563 [==============================] - 1s 986us/step - loss: 0.1837 - accuracy: 0.7320 - val_loss: 0.1863 - val_accuracy: 0.7322\n",
      "Epoch 821/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1856 - accuracy: 0.7299 - val_loss: 0.1855 - val_accuracy: 0.7309\n",
      "Epoch 822/1000\n",
      "563/563 [==============================] - 1s 963us/step - loss: 0.1842 - accuracy: 0.7333 - val_loss: 0.1858 - val_accuracy: 0.7346\n",
      "Epoch 823/1000\n",
      "563/563 [==============================] - 1s 963us/step - loss: 0.1846 - accuracy: 0.7328 - val_loss: 0.1853 - val_accuracy: 0.7343\n",
      "Epoch 824/1000\n",
      "563/563 [==============================] - 1s 982us/step - loss: 0.1822 - accuracy: 0.7358 - val_loss: 0.1856 - val_accuracy: 0.7340\n",
      "Epoch 825/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1849 - accuracy: 0.7292 - val_loss: 0.1848 - val_accuracy: 0.7357\n",
      "Epoch 826/1000\n",
      "563/563 [==============================] - 1s 963us/step - loss: 0.1857 - accuracy: 0.7287 - val_loss: 0.1863 - val_accuracy: 0.7332\n",
      "Epoch 827/1000\n",
      "563/563 [==============================] - 1s 954us/step - loss: 0.1836 - accuracy: 0.7324 - val_loss: 0.1859 - val_accuracy: 0.7354\n",
      "Epoch 828/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1818 - accuracy: 0.7392 - val_loss: 0.1850 - val_accuracy: 0.7332\n",
      "Epoch 829/1000\n",
      "563/563 [==============================] - 1s 997us/step - loss: 0.1835 - accuracy: 0.7354 - val_loss: 0.1868 - val_accuracy: 0.7309\n",
      "Epoch 830/1000\n",
      "563/563 [==============================] - 1s 970us/step - loss: 0.1852 - accuracy: 0.7298 - val_loss: 0.1859 - val_accuracy: 0.7309\n",
      "Epoch 831/1000\n",
      "563/563 [==============================] - 1s 958us/step - loss: 0.1832 - accuracy: 0.7378 - val_loss: 0.1848 - val_accuracy: 0.7356\n",
      "Epoch 832/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1840 - accuracy: 0.7320 - val_loss: 0.1853 - val_accuracy: 0.7358\n",
      "Epoch 833/1000\n",
      "563/563 [==============================] - 1s 947us/step - loss: 0.1859 - accuracy: 0.7298 - val_loss: 0.1848 - val_accuracy: 0.7330\n",
      "Epoch 834/1000\n",
      "563/563 [==============================] - 1s 975us/step - loss: 0.1871 - accuracy: 0.7260 - val_loss: 0.1888 - val_accuracy: 0.7261\n",
      "Epoch 835/1000\n",
      "563/563 [==============================] - 1s 988us/step - loss: 0.1844 - accuracy: 0.7322 - val_loss: 0.1853 - val_accuracy: 0.7341\n",
      "Epoch 836/1000\n",
      "563/563 [==============================] - 1s 993us/step - loss: 0.1857 - accuracy: 0.7333 - val_loss: 0.1851 - val_accuracy: 0.7330\n",
      "Epoch 837/1000\n",
      "563/563 [==============================] - 1s 954us/step - loss: 0.1829 - accuracy: 0.7377 - val_loss: 0.1846 - val_accuracy: 0.7356\n",
      "Epoch 838/1000\n",
      "563/563 [==============================] - 1s 972us/step - loss: 0.1844 - accuracy: 0.7335 - val_loss: 0.1855 - val_accuracy: 0.7340\n",
      "Epoch 839/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1864 - accuracy: 0.7284 - val_loss: 0.1852 - val_accuracy: 0.7348\n",
      "Epoch 840/1000\n",
      "563/563 [==============================] - 1s 984us/step - loss: 0.1829 - accuracy: 0.7367 - val_loss: 0.1869 - val_accuracy: 0.7344\n",
      "Epoch 841/1000\n",
      "563/563 [==============================] - 1s 966us/step - loss: 0.1842 - accuracy: 0.7368 - val_loss: 0.1859 - val_accuracy: 0.7343\n",
      "Epoch 842/1000\n",
      "563/563 [==============================] - 1s 984us/step - loss: 0.1841 - accuracy: 0.7339 - val_loss: 0.1857 - val_accuracy: 0.7336\n",
      "Epoch 843/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1859 - accuracy: 0.7326 - val_loss: 0.1870 - val_accuracy: 0.7287\n",
      "Epoch 844/1000\n",
      "563/563 [==============================] - 1s 952us/step - loss: 0.1847 - accuracy: 0.7321 - val_loss: 0.1847 - val_accuracy: 0.7344\n",
      "Epoch 845/1000\n",
      "563/563 [==============================] - 1s 959us/step - loss: 0.1824 - accuracy: 0.7388 - val_loss: 0.1847 - val_accuracy: 0.7339\n",
      "Epoch 846/1000\n",
      "563/563 [==============================] - 1s 993us/step - loss: 0.1851 - accuracy: 0.7312 - val_loss: 0.1859 - val_accuracy: 0.7319\n",
      "Epoch 847/1000\n",
      "563/563 [==============================] - 1s 981us/step - loss: 0.1838 - accuracy: 0.7312 - val_loss: 0.1860 - val_accuracy: 0.7311\n",
      "Epoch 848/1000\n",
      "563/563 [==============================] - 1s 954us/step - loss: 0.1842 - accuracy: 0.7326 - val_loss: 0.1860 - val_accuracy: 0.7339\n",
      "Epoch 849/1000\n",
      "563/563 [==============================] - 1s 974us/step - loss: 0.1829 - accuracy: 0.7360 - val_loss: 0.1869 - val_accuracy: 0.7308\n",
      "Epoch 850/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1836 - accuracy: 0.7337 - val_loss: 0.1849 - val_accuracy: 0.7341\n",
      "Epoch 851/1000\n",
      "563/563 [==============================] - 1s 956us/step - loss: 0.1840 - accuracy: 0.7351 - val_loss: 0.1859 - val_accuracy: 0.7343\n",
      "Epoch 852/1000\n",
      "563/563 [==============================] - 1s 954us/step - loss: 0.1814 - accuracy: 0.7410 - val_loss: 0.1876 - val_accuracy: 0.7328\n",
      "Epoch 853/1000\n",
      "563/563 [==============================] - 1s 995us/step - loss: 0.1853 - accuracy: 0.7311 - val_loss: 0.1850 - val_accuracy: 0.7324\n",
      "Epoch 854/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1834 - accuracy: 0.7351 - val_loss: 0.1853 - val_accuracy: 0.7330\n",
      "Epoch 855/1000\n",
      "563/563 [==============================] - 1s 976us/step - loss: 0.1833 - accuracy: 0.7345 - val_loss: 0.1852 - val_accuracy: 0.7339\n",
      "Epoch 856/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1820 - accuracy: 0.7365 - val_loss: 0.1845 - val_accuracy: 0.7334\n",
      "Epoch 857/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1841 - accuracy: 0.7338 - val_loss: 0.1857 - val_accuracy: 0.7324\n",
      "Epoch 858/1000\n",
      "563/563 [==============================] - 1s 990us/step - loss: 0.1836 - accuracy: 0.7352 - val_loss: 0.1854 - val_accuracy: 0.7318\n",
      "Epoch 859/1000\n",
      "563/563 [==============================] - 1s 963us/step - loss: 0.1844 - accuracy: 0.7359 - val_loss: 0.1850 - val_accuracy: 0.7334\n",
      "Epoch 860/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1825 - accuracy: 0.7347 - val_loss: 0.1855 - val_accuracy: 0.7336\n",
      "Epoch 861/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1825 - accuracy: 0.7355 - val_loss: 0.1868 - val_accuracy: 0.7331\n",
      "Epoch 862/1000\n",
      "563/563 [==============================] - 1s 965us/step - loss: 0.1845 - accuracy: 0.7330 - val_loss: 0.1869 - val_accuracy: 0.7309\n",
      "Epoch 863/1000\n",
      "563/563 [==============================] - 1s 954us/step - loss: 0.1860 - accuracy: 0.7284 - val_loss: 0.1860 - val_accuracy: 0.7327\n",
      "Epoch 864/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1816 - accuracy: 0.7411 - val_loss: 0.1854 - val_accuracy: 0.7327\n",
      "Epoch 865/1000\n",
      "563/563 [==============================] - 1s 968us/step - loss: 0.1801 - accuracy: 0.7390 - val_loss: 0.1860 - val_accuracy: 0.7300\n",
      "Epoch 866/1000\n",
      "563/563 [==============================] - 1s 981us/step - loss: 0.1846 - accuracy: 0.7340 - val_loss: 0.1849 - val_accuracy: 0.7315\n",
      "Epoch 867/1000\n",
      "563/563 [==============================] - 1s 991us/step - loss: 0.1835 - accuracy: 0.7331 - val_loss: 0.1852 - val_accuracy: 0.7330\n",
      "Epoch 868/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1828 - accuracy: 0.7357 - val_loss: 0.1865 - val_accuracy: 0.7335\n",
      "Epoch 869/1000\n",
      "563/563 [==============================] - 1s 961us/step - loss: 0.1834 - accuracy: 0.7310 - val_loss: 0.1868 - val_accuracy: 0.7332\n",
      "Epoch 870/1000\n",
      "563/563 [==============================] - 1s 972us/step - loss: 0.1823 - accuracy: 0.7370 - val_loss: 0.1863 - val_accuracy: 0.7348\n",
      "Epoch 871/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1846 - accuracy: 0.7309 - val_loss: 0.1851 - val_accuracy: 0.7346\n",
      "Epoch 872/1000\n",
      "563/563 [==============================] - 1s 985us/step - loss: 0.1826 - accuracy: 0.7387 - val_loss: 0.1878 - val_accuracy: 0.7280\n",
      "Epoch 873/1000\n",
      "563/563 [==============================] - 1s 968us/step - loss: 0.1826 - accuracy: 0.7320 - val_loss: 0.1951 - val_accuracy: 0.7245\n",
      "Epoch 874/1000\n",
      "563/563 [==============================] - 1s 986us/step - loss: 0.1864 - accuracy: 0.7279 - val_loss: 0.1861 - val_accuracy: 0.7322\n",
      "Epoch 875/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1846 - accuracy: 0.7331 - val_loss: 0.1862 - val_accuracy: 0.7334\n",
      "Epoch 876/1000\n",
      "563/563 [==============================] - 1s 954us/step - loss: 0.1854 - accuracy: 0.7323 - val_loss: 0.1850 - val_accuracy: 0.7362\n",
      "Epoch 877/1000\n",
      "563/563 [==============================] - 1s 972us/step - loss: 0.1846 - accuracy: 0.7373 - val_loss: 0.1881 - val_accuracy: 0.7289\n",
      "Epoch 878/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1830 - accuracy: 0.7351 - val_loss: 0.1851 - val_accuracy: 0.7354\n",
      "Epoch 879/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1828 - accuracy: 0.7364 - val_loss: 0.1851 - val_accuracy: 0.7340\n",
      "Epoch 880/1000\n",
      "563/563 [==============================] - 1s 952us/step - loss: 0.1829 - accuracy: 0.7346 - val_loss: 0.1858 - val_accuracy: 0.7314\n",
      "Epoch 881/1000\n",
      "563/563 [==============================] - 1s 981us/step - loss: 0.1853 - accuracy: 0.7316 - val_loss: 0.1862 - val_accuracy: 0.7340\n",
      "Epoch 882/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1854 - accuracy: 0.7297 - val_loss: 0.1854 - val_accuracy: 0.7340\n",
      "Epoch 883/1000\n",
      "563/563 [==============================] - 1s 975us/step - loss: 0.1840 - accuracy: 0.7328 - val_loss: 0.1854 - val_accuracy: 0.7330\n",
      "Epoch 884/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1829 - accuracy: 0.7340 - val_loss: 0.1866 - val_accuracy: 0.7331\n",
      "Epoch 885/1000\n",
      "563/563 [==============================] - 1s 986us/step - loss: 0.1818 - accuracy: 0.7405 - val_loss: 0.1866 - val_accuracy: 0.7280\n",
      "Epoch 886/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1832 - accuracy: 0.7343 - val_loss: 0.1880 - val_accuracy: 0.7309\n",
      "Epoch 887/1000\n",
      "563/563 [==============================] - 1s 965us/step - loss: 0.1859 - accuracy: 0.7296 - val_loss: 0.1872 - val_accuracy: 0.7306\n",
      "Epoch 888/1000\n",
      "563/563 [==============================] - 1s 966us/step - loss: 0.1849 - accuracy: 0.7330 - val_loss: 0.1850 - val_accuracy: 0.7322\n",
      "Epoch 889/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1821 - accuracy: 0.7375 - val_loss: 0.1885 - val_accuracy: 0.7275\n",
      "Epoch 890/1000\n",
      "563/563 [==============================] - 1s 961us/step - loss: 0.1828 - accuracy: 0.7334 - val_loss: 0.1887 - val_accuracy: 0.7293\n",
      "Epoch 891/1000\n",
      "563/563 [==============================] - 1s 952us/step - loss: 0.1832 - accuracy: 0.7353 - val_loss: 0.1863 - val_accuracy: 0.7296\n",
      "Epoch 892/1000\n",
      "563/563 [==============================] - 1s 981us/step - loss: 0.1846 - accuracy: 0.7286 - val_loss: 0.1861 - val_accuracy: 0.7314\n",
      "Epoch 893/1000\n",
      "563/563 [==============================] - 1s 991us/step - loss: 0.1856 - accuracy: 0.7319 - val_loss: 0.1849 - val_accuracy: 0.7350\n",
      "Epoch 894/1000\n",
      "563/563 [==============================] - 1s 952us/step - loss: 0.1870 - accuracy: 0.7285 - val_loss: 0.1888 - val_accuracy: 0.7251\n",
      "Epoch 895/1000\n",
      "563/563 [==============================] - 1s 977us/step - loss: 0.1846 - accuracy: 0.7285 - val_loss: 0.1868 - val_accuracy: 0.7327\n",
      "Epoch 896/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1834 - accuracy: 0.7343 - val_loss: 0.1854 - val_accuracy: 0.7343\n",
      "Epoch 897/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1848 - accuracy: 0.7322 - val_loss: 0.1905 - val_accuracy: 0.7226\n",
      "Epoch 898/1000\n",
      "563/563 [==============================] - 1s 968us/step - loss: 0.1840 - accuracy: 0.7306 - val_loss: 0.1856 - val_accuracy: 0.7319\n",
      "Epoch 899/1000\n",
      "563/563 [==============================] - 1s 953us/step - loss: 0.1845 - accuracy: 0.7342 - val_loss: 0.1852 - val_accuracy: 0.7330\n",
      "Epoch 900/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1816 - accuracy: 0.7399 - val_loss: 0.1851 - val_accuracy: 0.7328\n",
      "Epoch 901/1000\n",
      "563/563 [==============================] - 1s 991us/step - loss: 0.1857 - accuracy: 0.7298 - val_loss: 0.1858 - val_accuracy: 0.7324\n",
      "Epoch 902/1000\n",
      "563/563 [==============================] - 1s 958us/step - loss: 0.1862 - accuracy: 0.7308 - val_loss: 0.1858 - val_accuracy: 0.7326\n",
      "Epoch 903/1000\n",
      "563/563 [==============================] - 1s 994us/step - loss: 0.1845 - accuracy: 0.7296 - val_loss: 0.1870 - val_accuracy: 0.7318\n",
      "Epoch 904/1000\n",
      "563/563 [==============================] - 1s 998us/step - loss: 0.1837 - accuracy: 0.7320 - val_loss: 0.1857 - val_accuracy: 0.7301\n",
      "Epoch 905/1000\n",
      "563/563 [==============================] - 1s 947us/step - loss: 0.1860 - accuracy: 0.7305 - val_loss: 0.1874 - val_accuracy: 0.7326\n",
      "Epoch 906/1000\n",
      "563/563 [==============================] - 1s 954us/step - loss: 0.1837 - accuracy: 0.7347 - val_loss: 0.1857 - val_accuracy: 0.7315\n",
      "Epoch 907/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1862 - accuracy: 0.7313 - val_loss: 0.1874 - val_accuracy: 0.7256\n",
      "Epoch 908/1000\n",
      "563/563 [==============================] - 1s 995us/step - loss: 0.1873 - accuracy: 0.7243 - val_loss: 0.1848 - val_accuracy: 0.7328\n",
      "Epoch 909/1000\n",
      "563/563 [==============================] - 1s 950us/step - loss: 0.1859 - accuracy: 0.7279 - val_loss: 0.1855 - val_accuracy: 0.7324\n",
      "Epoch 910/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1829 - accuracy: 0.7376 - val_loss: 0.1859 - val_accuracy: 0.7326\n",
      "Epoch 911/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1824 - accuracy: 0.7343 - val_loss: 0.1853 - val_accuracy: 0.7332\n",
      "Epoch 912/1000\n",
      "563/563 [==============================] - 1s 972us/step - loss: 0.1831 - accuracy: 0.7381 - val_loss: 0.1861 - val_accuracy: 0.7299\n",
      "Epoch 913/1000\n",
      "563/563 [==============================] - 1s 972us/step - loss: 0.1847 - accuracy: 0.7316 - val_loss: 0.1875 - val_accuracy: 0.7265\n",
      "Epoch 914/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1862 - accuracy: 0.7291 - val_loss: 0.1879 - val_accuracy: 0.7321\n",
      "Epoch 915/1000\n",
      "563/563 [==============================] - 1s 981us/step - loss: 0.1867 - accuracy: 0.7275 - val_loss: 0.1857 - val_accuracy: 0.7326\n",
      "Epoch 916/1000\n",
      "563/563 [==============================] - 1s 963us/step - loss: 0.1838 - accuracy: 0.7376 - val_loss: 0.1851 - val_accuracy: 0.7341\n",
      "Epoch 917/1000\n",
      "563/563 [==============================] - 1s 961us/step - loss: 0.1845 - accuracy: 0.7332 - val_loss: 0.1849 - val_accuracy: 0.7330\n",
      "Epoch 918/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1823 - accuracy: 0.7353 - val_loss: 0.1868 - val_accuracy: 0.7328\n",
      "Epoch 919/1000\n",
      "563/563 [==============================] - 1s 972us/step - loss: 0.1829 - accuracy: 0.7373 - val_loss: 0.1923 - val_accuracy: 0.7240\n",
      "Epoch 920/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1822 - accuracy: 0.7393 - val_loss: 0.1889 - val_accuracy: 0.7284\n",
      "Epoch 921/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1846 - accuracy: 0.7304 - val_loss: 0.1864 - val_accuracy: 0.7318\n",
      "Epoch 922/1000\n",
      "563/563 [==============================] - 1s 988us/step - loss: 0.1843 - accuracy: 0.7340 - val_loss: 0.1876 - val_accuracy: 0.7350\n",
      "Epoch 923/1000\n",
      "563/563 [==============================] - 1s 979us/step - loss: 0.1869 - accuracy: 0.7272 - val_loss: 0.1859 - val_accuracy: 0.7330\n",
      "Epoch 924/1000\n",
      "563/563 [==============================] - 1s 950us/step - loss: 0.1832 - accuracy: 0.7356 - val_loss: 0.1850 - val_accuracy: 0.7350\n",
      "Epoch 925/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1824 - accuracy: 0.7394 - val_loss: 0.1857 - val_accuracy: 0.7300\n",
      "Epoch 926/1000\n",
      "563/563 [==============================] - 1s 947us/step - loss: 0.1860 - accuracy: 0.7274 - val_loss: 0.1868 - val_accuracy: 0.7328\n",
      "Epoch 927/1000\n",
      "563/563 [==============================] - 1s 979us/step - loss: 0.1842 - accuracy: 0.7323 - val_loss: 0.1864 - val_accuracy: 0.7324\n",
      "Epoch 928/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1854 - accuracy: 0.7297 - val_loss: 0.1855 - val_accuracy: 0.7327\n",
      "Epoch 929/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1859 - accuracy: 0.7302 - val_loss: 0.1847 - val_accuracy: 0.7341\n",
      "Epoch 930/1000\n",
      "563/563 [==============================] - 1s 977us/step - loss: 0.1840 - accuracy: 0.7352 - val_loss: 0.1880 - val_accuracy: 0.7271\n",
      "Epoch 931/1000\n",
      "563/563 [==============================] - 1s 970us/step - loss: 0.1847 - accuracy: 0.7295 - val_loss: 0.1854 - val_accuracy: 0.7327\n",
      "Epoch 932/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1830 - accuracy: 0.7345 - val_loss: 0.1860 - val_accuracy: 0.7321\n",
      "Epoch 933/1000\n",
      "563/563 [==============================] - 1s 963us/step - loss: 0.1840 - accuracy: 0.7308 - val_loss: 0.1859 - val_accuracy: 0.7353\n",
      "Epoch 934/1000\n",
      "563/563 [==============================] - 1s 961us/step - loss: 0.1824 - accuracy: 0.7367 - val_loss: 0.1848 - val_accuracy: 0.7354\n",
      "Epoch 935/1000\n",
      "563/563 [==============================] - 1s 991us/step - loss: 0.1832 - accuracy: 0.7330 - val_loss: 0.1849 - val_accuracy: 0.7337\n",
      "Epoch 936/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1854 - accuracy: 0.7311 - val_loss: 0.1868 - val_accuracy: 0.7293\n",
      "Epoch 937/1000\n",
      "563/563 [==============================] - 1s 959us/step - loss: 0.1852 - accuracy: 0.7296 - val_loss: 0.1857 - val_accuracy: 0.7344\n",
      "Epoch 938/1000\n",
      "563/563 [==============================] - 1s 973us/step - loss: 0.1825 - accuracy: 0.7377 - val_loss: 0.1848 - val_accuracy: 0.7349\n",
      "Epoch 939/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1846 - accuracy: 0.7312 - val_loss: 0.1855 - val_accuracy: 0.7339\n",
      "Epoch 940/1000\n",
      "563/563 [==============================] - 1s 965us/step - loss: 0.1840 - accuracy: 0.7325 - val_loss: 0.1880 - val_accuracy: 0.7299\n",
      "Epoch 941/1000\n",
      "563/563 [==============================] - 1s 974us/step - loss: 0.1841 - accuracy: 0.7349 - val_loss: 0.1856 - val_accuracy: 0.7332\n",
      "Epoch 942/1000\n",
      "563/563 [==============================] - 1s 970us/step - loss: 0.1853 - accuracy: 0.7302 - val_loss: 0.1929 - val_accuracy: 0.7243\n",
      "Epoch 943/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1858 - accuracy: 0.7318 - val_loss: 0.1886 - val_accuracy: 0.7348\n",
      "Epoch 944/1000\n",
      "563/563 [==============================] - 1s 963us/step - loss: 0.1846 - accuracy: 0.7326 - val_loss: 0.1853 - val_accuracy: 0.7339\n",
      "Epoch 945/1000\n",
      "563/563 [==============================] - 1s 962us/step - loss: 0.1830 - accuracy: 0.7378 - val_loss: 0.1849 - val_accuracy: 0.7349\n",
      "Epoch 946/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1856 - accuracy: 0.7323 - val_loss: 0.1859 - val_accuracy: 0.7358\n",
      "Epoch 947/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1816 - accuracy: 0.7369 - val_loss: 0.1850 - val_accuracy: 0.7353\n",
      "Epoch 948/1000\n",
      "563/563 [==============================] - 1s 971us/step - loss: 0.1839 - accuracy: 0.7361 - val_loss: 0.1866 - val_accuracy: 0.7345\n",
      "Epoch 949/1000\n",
      "563/563 [==============================] - 1s 970us/step - loss: 0.1836 - accuracy: 0.7394 - val_loss: 0.1857 - val_accuracy: 0.7361\n",
      "Epoch 950/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1831 - accuracy: 0.7370 - val_loss: 0.1851 - val_accuracy: 0.7357\n",
      "Epoch 951/1000\n",
      "563/563 [==============================] - 1s 959us/step - loss: 0.1846 - accuracy: 0.7310 - val_loss: 0.1865 - val_accuracy: 0.7321\n",
      "Epoch 952/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1846 - accuracy: 0.7325 - val_loss: 0.1864 - val_accuracy: 0.7292\n",
      "Epoch 953/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1829 - accuracy: 0.7336 - val_loss: 0.1851 - val_accuracy: 0.7353\n",
      "Epoch 954/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1846 - accuracy: 0.7330 - val_loss: 0.1855 - val_accuracy: 0.7350\n",
      "Epoch 955/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1844 - accuracy: 0.7331 - val_loss: 0.1861 - val_accuracy: 0.7332\n",
      "Epoch 956/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1833 - accuracy: 0.7351 - val_loss: 0.1868 - val_accuracy: 0.7304\n",
      "Epoch 957/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1825 - accuracy: 0.7377 - val_loss: 0.1863 - val_accuracy: 0.7311\n",
      "Epoch 958/1000\n",
      "563/563 [==============================] - 1s 981us/step - loss: 0.1858 - accuracy: 0.7316 - val_loss: 0.1891 - val_accuracy: 0.7253\n",
      "Epoch 959/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1828 - accuracy: 0.7380 - val_loss: 0.1860 - val_accuracy: 0.7352\n",
      "Epoch 960/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1809 - accuracy: 0.7400 - val_loss: 0.1868 - val_accuracy: 0.7341\n",
      "Epoch 961/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1821 - accuracy: 0.7383 - val_loss: 0.1910 - val_accuracy: 0.7254\n",
      "Epoch 962/1000\n",
      "563/563 [==============================] - 1s 975us/step - loss: 0.1826 - accuracy: 0.7333 - val_loss: 0.1858 - val_accuracy: 0.7343\n",
      "Epoch 963/1000\n",
      "563/563 [==============================] - 1s 957us/step - loss: 0.1850 - accuracy: 0.7309 - val_loss: 0.1916 - val_accuracy: 0.7236\n",
      "Epoch 964/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1827 - accuracy: 0.7368 - val_loss: 0.1868 - val_accuracy: 0.7344\n",
      "Epoch 965/1000\n",
      "563/563 [==============================] - 1s 979us/step - loss: 0.1841 - accuracy: 0.7331 - val_loss: 0.1865 - val_accuracy: 0.7343\n",
      "Epoch 966/1000\n",
      "563/563 [==============================] - 1s 979us/step - loss: 0.1824 - accuracy: 0.7372 - val_loss: 0.1874 - val_accuracy: 0.7336\n",
      "Epoch 967/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1842 - accuracy: 0.7303 - val_loss: 0.1867 - val_accuracy: 0.7341\n",
      "Epoch 968/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1858 - accuracy: 0.7281 - val_loss: 0.1861 - val_accuracy: 0.7348\n",
      "Epoch 969/1000\n",
      "563/563 [==============================] - 1s 983us/step - loss: 0.1855 - accuracy: 0.7301 - val_loss: 0.1855 - val_accuracy: 0.7352\n",
      "Epoch 970/1000\n",
      "563/563 [==============================] - 1s 977us/step - loss: 0.1843 - accuracy: 0.7327 - val_loss: 0.1858 - val_accuracy: 0.7336\n",
      "Epoch 971/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1861 - accuracy: 0.7323 - val_loss: 0.1881 - val_accuracy: 0.7336\n",
      "Epoch 972/1000\n",
      "563/563 [==============================] - 1s 958us/step - loss: 0.1841 - accuracy: 0.7341 - val_loss: 0.1893 - val_accuracy: 0.7287\n",
      "Epoch 973/1000\n",
      "563/563 [==============================] - 1s 970us/step - loss: 0.1876 - accuracy: 0.7287 - val_loss: 0.1890 - val_accuracy: 0.7319\n",
      "Epoch 974/1000\n",
      "563/563 [==============================] - 1s 991us/step - loss: 0.1840 - accuracy: 0.7333 - val_loss: 0.1870 - val_accuracy: 0.7339\n",
      "Epoch 975/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1862 - accuracy: 0.7306 - val_loss: 0.1882 - val_accuracy: 0.7324\n",
      "Epoch 976/1000\n",
      "563/563 [==============================] - 1s 965us/step - loss: 0.1862 - accuracy: 0.7316 - val_loss: 0.1868 - val_accuracy: 0.7336\n",
      "Epoch 977/1000\n",
      "563/563 [==============================] - 1s 982us/step - loss: 0.1836 - accuracy: 0.7386 - val_loss: 0.1874 - val_accuracy: 0.7344\n",
      "Epoch 978/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1838 - accuracy: 0.7361 - val_loss: 0.1869 - val_accuracy: 0.7323\n",
      "Epoch 979/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1880 - accuracy: 0.7261 - val_loss: 0.1877 - val_accuracy: 0.7331\n",
      "Epoch 980/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1839 - accuracy: 0.7358 - val_loss: 0.1879 - val_accuracy: 0.7306\n",
      "Epoch 981/1000\n",
      "563/563 [==============================] - 1s 997us/step - loss: 0.1838 - accuracy: 0.7347 - val_loss: 0.1917 - val_accuracy: 0.7225\n",
      "Epoch 982/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1856 - accuracy: 0.7306 - val_loss: 0.1893 - val_accuracy: 0.7271\n",
      "Epoch 983/1000\n",
      "563/563 [==============================] - 1s 947us/step - loss: 0.1857 - accuracy: 0.7279 - val_loss: 0.1888 - val_accuracy: 0.7300\n",
      "Epoch 984/1000\n",
      "563/563 [==============================] - 1s 988us/step - loss: 0.1833 - accuracy: 0.7370 - val_loss: 0.1884 - val_accuracy: 0.7315\n",
      "Epoch 985/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1842 - accuracy: 0.7337 - val_loss: 0.1883 - val_accuracy: 0.7323\n",
      "Epoch 986/1000\n",
      "563/563 [==============================] - 1s 995us/step - loss: 0.1855 - accuracy: 0.7300 - val_loss: 0.1913 - val_accuracy: 0.7223\n",
      "Epoch 987/1000\n",
      "563/563 [==============================] - 1s 993us/step - loss: 0.1853 - accuracy: 0.7318 - val_loss: 0.1861 - val_accuracy: 0.7343\n",
      "Epoch 988/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1843 - accuracy: 0.7360 - val_loss: 0.1878 - val_accuracy: 0.7331\n",
      "Epoch 989/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1837 - accuracy: 0.7339 - val_loss: 0.1865 - val_accuracy: 0.7331\n",
      "Epoch 990/1000\n",
      "563/563 [==============================] - 1s 965us/step - loss: 0.1852 - accuracy: 0.7346 - val_loss: 0.1877 - val_accuracy: 0.7334\n",
      "Epoch 991/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1841 - accuracy: 0.7338 - val_loss: 0.1922 - val_accuracy: 0.7286\n",
      "Epoch 992/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1856 - accuracy: 0.7311 - val_loss: 0.1869 - val_accuracy: 0.7324\n",
      "Epoch 993/1000\n",
      "563/563 [==============================] - 1s 971us/step - loss: 0.1871 - accuracy: 0.7305 - val_loss: 0.1881 - val_accuracy: 0.7357\n",
      "Epoch 994/1000\n",
      "563/563 [==============================] - 1s 965us/step - loss: 0.1829 - accuracy: 0.7383 - val_loss: 0.1870 - val_accuracy: 0.7322\n",
      "Epoch 995/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1862 - accuracy: 0.7335 - val_loss: 0.1885 - val_accuracy: 0.7323\n",
      "Epoch 996/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1861 - accuracy: 0.7298 - val_loss: 0.1866 - val_accuracy: 0.7343\n",
      "Epoch 997/1000\n",
      "563/563 [==============================] - 1s 965us/step - loss: 0.1829 - accuracy: 0.7355 - val_loss: 0.1917 - val_accuracy: 0.7331\n",
      "Epoch 998/1000\n",
      "563/563 [==============================] - 1s 979us/step - loss: 0.1866 - accuracy: 0.7268 - val_loss: 0.1897 - val_accuracy: 0.7324\n",
      "Epoch 999/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1854 - accuracy: 0.7326 - val_loss: 0.1890 - val_accuracy: 0.7275\n",
      "Epoch 1000/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1848 - accuracy: 0.7323 - val_loss: 0.1877 - val_accuracy: 0.7301\n"
     ]
    }
   ],
   "source": [
    "# Fit the model using 50 epochs and the training data\n",
    "fit_model_A27 = nn_A27.fit(X_train_scaled, y_train, validation_split=0.3, epochs=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alternative Model 27 Results\n",
      "Loss: 0.19206713140010834, Accuracy: 0.7251312136650085\n"
     ]
    }
   ],
   "source": [
    "print(\"Alternative Model 27 Results\")\n",
    "# Evaluate the model loss and accuracy metrics using the evaluate method and the test data\n",
    "model_loss, model_accuracy = nn_A27.evaluate(X_test_scaled,y_test,verbose=0)\n",
    "# Display the model loss and accuracy results\n",
    "print(f\"Loss: {model_loss}, Accuracy: {model_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alternative Model 28+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "114"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the the number of inputs (features) to the model\n",
    "number_input_features = len(X_train.iloc[0])\n",
    "# Review the number of features\n",
    "number_input_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the number of neurons in the output layer\n",
    "number_output_neurons_A28 = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the number of hidden nodes for the first hidden layer\n",
    "hidden_nodes_layer1_A28 =  (number_input_features + number_output_neurons_A28) // 2\n",
    "# Review the number of hidden nodes in the first layer\n",
    "hidden_nodes_layer1_A28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the number of hidden nodes for the second hidden layer\n",
    "hidden_nodes_layer2_A28 =  (hidden_nodes_layer1_A28 + number_output_neurons_A28) // 2\n",
    "# Review the number of hidden nodes in the second layer\n",
    "hidden_nodes_layer2_A28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the number of hidden nodes for the third hidden layer\n",
    "hidden_nodes_layer3_A28 =  (hidden_nodes_layer2_A28 + number_output_neurons_A28) // 2\n",
    "# Review the number of hidden nodes in the third layer\n",
    "hidden_nodes_layer3_A28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the number of hidden nodes for the fourth hidden layer\n",
    "hidden_nodes_layer4_A28 =  (hidden_nodes_layer3_A28 + number_output_neurons_A28) // 2\n",
    "# Review the number of hidden nodes in the fourth layer\n",
    "hidden_nodes_layer4_A28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the number of hidden nodes for the fifth hidden layer\n",
    "hidden_nodes_layer5_A28 =  (hidden_nodes_layer4_A28 + number_output_neurons_A28) // 2\n",
    "# Review the number of hidden nodes in the fifth layer\n",
    "hidden_nodes_layer5_A28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the number of hidden nodes for the Sixth hidden layer\n",
    "hidden_nodes_layer6_A28 =  (hidden_nodes_layer5_A28 + number_output_neurons_A28) // 2\n",
    "# Review the number of hidden nodes in the sixth layer\n",
    "hidden_nodes_layer6_A28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_31 (Dense)             (None, 57)                6555      \n",
      "_________________________________________________________________\n",
      "dense_32 (Dense)             (None, 29)                1682      \n",
      "_________________________________________________________________\n",
      "dense_33 (Dense)             (None, 15)                450       \n",
      "_________________________________________________________________\n",
      "dense_34 (Dense)             (None, 8)                 128       \n",
      "_________________________________________________________________\n",
      "dense_35 (Dense)             (None, 4)                 36        \n",
      "_________________________________________________________________\n",
      "dense_36 (Dense)             (None, 2)                 10        \n",
      "_________________________________________________________________\n",
      "dense_37 (Dense)             (None, 1)                 3         \n",
      "=================================================================\n",
      "Total params: 8,864\n",
      "Trainable params: 8,864\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Create and Display the Sequential Model Instance \n",
    "# for Model A28\n",
    "nn_A28 = Sequential() \n",
    "\n",
    "# Add the first hidden layer\n",
    "nn_A28.add(Dense(units=hidden_nodes_layer1_A28, input_dim=number_input_features, activation=\"relu\"))\n",
    "# Add the second hidden layer\n",
    "nn_A28.add(Dense(units=hidden_nodes_layer2_A28, input_dim=number_input_features, activation=\"relu\"))\n",
    "# Add the third hidden layer\n",
    "nn_A28.add(Dense(units=hidden_nodes_layer3_A28, input_dim=number_input_features, activation=\"relu\"))\n",
    "# Add the fourth hidden layer\n",
    "nn_A28.add(Dense(units=hidden_nodes_layer4_A28, input_dim=number_input_features, activation=\"relu\"))\n",
    "# Add the fifth hidden layer\n",
    "nn_A28.add(Dense(units=hidden_nodes_layer5_A28, input_dim=number_input_features, activation=\"relu\"))\n",
    "# Add the sixth hidden layer\n",
    "nn_A28.add(Dense(units=hidden_nodes_layer6_A28, activation=\"relu\"))\n",
    "\n",
    "# Add the output layer to the model specifying the number of output neurons and activation function\n",
    "nn_A28.add(Dense(units=number_output_neurons_A28, activation=\"sigmoid\"))\n",
    "\n",
    "# Display the Sequential model summary\n",
    "nn_A28.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the Sequential model\n",
    "nn_A28.compile(loss=\"mse\", optimizer=\"adagrad\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2505 - accuracy: 0.4605 - val_loss: 0.2501 - val_accuracy: 0.5352\n",
      "Epoch 2/1000\n",
      "563/563 [==============================] - 1s 970us/step - loss: 0.2499 - accuracy: 0.5225 - val_loss: 0.2498 - val_accuracy: 0.5439\n",
      "Epoch 3/1000\n",
      "563/563 [==============================] - 1s 945us/step - loss: 0.2499 - accuracy: 0.5301 - val_loss: 0.2497 - val_accuracy: 0.5443\n",
      "Epoch 4/1000\n",
      "563/563 [==============================] - 1s 918us/step - loss: 0.2498 - accuracy: 0.5235 - val_loss: 0.2496 - val_accuracy: 0.5407\n",
      "Epoch 5/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2497 - accuracy: 0.5289 - val_loss: 0.2495 - val_accuracy: 0.5488\n",
      "Epoch 6/1000\n",
      "563/563 [==============================] - 1s 950us/step - loss: 0.2496 - accuracy: 0.5321 - val_loss: 0.2495 - val_accuracy: 0.5498\n",
      "Epoch 7/1000\n",
      "563/563 [==============================] - 1s 943us/step - loss: 0.2497 - accuracy: 0.5247 - val_loss: 0.2494 - val_accuracy: 0.5499\n",
      "Epoch 8/1000\n",
      "563/563 [==============================] - 1s 956us/step - loss: 0.2496 - accuracy: 0.5284 - val_loss: 0.2494 - val_accuracy: 0.5501\n",
      "Epoch 9/1000\n",
      "563/563 [==============================] - 1s 981us/step - loss: 0.2495 - accuracy: 0.5331 - val_loss: 0.2493 - val_accuracy: 0.5504\n",
      "Epoch 10/1000\n",
      "563/563 [==============================] - 1s 945us/step - loss: 0.2496 - accuracy: 0.5302 - val_loss: 0.2493 - val_accuracy: 0.5496\n",
      "Epoch 11/1000\n",
      "563/563 [==============================] - 1s 958us/step - loss: 0.2495 - accuracy: 0.5343 - val_loss: 0.2492 - val_accuracy: 0.5496\n",
      "Epoch 12/1000\n",
      "563/563 [==============================] - 1s 986us/step - loss: 0.2495 - accuracy: 0.5327 - val_loss: 0.2492 - val_accuracy: 0.5498\n",
      "Epoch 13/1000\n",
      "563/563 [==============================] - 1s 997us/step - loss: 0.2495 - accuracy: 0.5300 - val_loss: 0.2492 - val_accuracy: 0.5505\n",
      "Epoch 14/1000\n",
      "563/563 [==============================] - 1s 953us/step - loss: 0.2496 - accuracy: 0.5245 - val_loss: 0.2491 - val_accuracy: 0.5508\n",
      "Epoch 15/1000\n",
      "563/563 [==============================] - 1s 943us/step - loss: 0.2495 - accuracy: 0.5300 - val_loss: 0.2491 - val_accuracy: 0.5509\n",
      "Epoch 16/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2493 - accuracy: 0.5375 - val_loss: 0.2491 - val_accuracy: 0.5509\n",
      "Epoch 17/1000\n",
      "563/563 [==============================] - 1s 972us/step - loss: 0.2495 - accuracy: 0.5290 - val_loss: 0.2491 - val_accuracy: 0.5509\n",
      "Epoch 18/1000\n",
      "563/563 [==============================] - 1s 938us/step - loss: 0.2495 - accuracy: 0.5268 - val_loss: 0.2490 - val_accuracy: 0.5510\n",
      "Epoch 19/1000\n",
      "563/563 [==============================] - 1s 966us/step - loss: 0.2494 - accuracy: 0.5337 - val_loss: 0.2490 - val_accuracy: 0.5510\n",
      "Epoch 20/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2493 - accuracy: 0.5355 - val_loss: 0.2490 - val_accuracy: 0.5512\n",
      "Epoch 21/1000\n",
      "563/563 [==============================] - 1s 945us/step - loss: 0.2493 - accuracy: 0.5353 - val_loss: 0.2490 - val_accuracy: 0.5512\n",
      "Epoch 22/1000\n",
      "563/563 [==============================] - 1s 936us/step - loss: 0.2494 - accuracy: 0.5320 - val_loss: 0.2490 - val_accuracy: 0.5512\n",
      "Epoch 23/1000\n",
      "563/563 [==============================] - 1s 977us/step - loss: 0.2493 - accuracy: 0.5330 - val_loss: 0.2489 - val_accuracy: 0.5512\n",
      "Epoch 24/1000\n",
      "563/563 [==============================] - 1s 948us/step - loss: 0.2492 - accuracy: 0.5357 - val_loss: 0.2489 - val_accuracy: 0.5513\n",
      "Epoch 25/1000\n",
      "563/563 [==============================] - 1s 940us/step - loss: 0.2492 - accuracy: 0.5355 - val_loss: 0.2489 - val_accuracy: 0.5516\n",
      "Epoch 26/1000\n",
      "563/563 [==============================] - 1s 931us/step - loss: 0.2495 - accuracy: 0.5274 - val_loss: 0.2489 - val_accuracy: 0.5514\n",
      "Epoch 27/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2493 - accuracy: 0.5337 - val_loss: 0.2489 - val_accuracy: 0.5514\n",
      "Epoch 28/1000\n",
      "563/563 [==============================] - 1s 925us/step - loss: 0.2492 - accuracy: 0.5354 - val_loss: 0.2489 - val_accuracy: 0.5514\n",
      "Epoch 29/1000\n",
      "563/563 [==============================] - 1s 924us/step - loss: 0.2493 - accuracy: 0.5321 - val_loss: 0.2488 - val_accuracy: 0.5516\n",
      "Epoch 30/1000\n",
      "563/563 [==============================] - 1s 966us/step - loss: 0.2494 - accuracy: 0.5273 - val_loss: 0.2488 - val_accuracy: 0.5509\n",
      "Epoch 31/1000\n",
      "563/563 [==============================] - 1s 999us/step - loss: 0.2494 - accuracy: 0.5296 - val_loss: 0.2488 - val_accuracy: 0.5516\n",
      "Epoch 32/1000\n",
      "563/563 [==============================] - 1s 934us/step - loss: 0.2493 - accuracy: 0.5310 - val_loss: 0.2488 - val_accuracy: 0.5522\n",
      "Epoch 33/1000\n",
      "563/563 [==============================] - 1s 936us/step - loss: 0.2494 - accuracy: 0.5274 - val_loss: 0.2488 - val_accuracy: 0.5522\n",
      "Epoch 34/1000\n",
      "563/563 [==============================] - 1s 952us/step - loss: 0.2495 - accuracy: 0.5246 - val_loss: 0.2488 - val_accuracy: 0.5522\n",
      "Epoch 35/1000\n",
      "563/563 [==============================] - 1s 975us/step - loss: 0.2495 - accuracy: 0.5261 - val_loss: 0.2488 - val_accuracy: 0.5522\n",
      "Epoch 36/1000\n",
      "563/563 [==============================] - 1s 947us/step - loss: 0.2492 - accuracy: 0.5322 - val_loss: 0.2487 - val_accuracy: 0.5522\n",
      "Epoch 37/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2493 - accuracy: 0.5316 - val_loss: 0.2487 - val_accuracy: 0.5522\n",
      "Epoch 38/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2489 - accuracy: 0.5415 - val_loss: 0.2487 - val_accuracy: 0.5521\n",
      "Epoch 39/1000\n",
      "563/563 [==============================] - 1s 993us/step - loss: 0.2492 - accuracy: 0.5333 - val_loss: 0.2487 - val_accuracy: 0.5521\n",
      "Epoch 40/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2490 - accuracy: 0.5381 - val_loss: 0.2487 - val_accuracy: 0.5521\n",
      "Epoch 41/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2493 - accuracy: 0.5295 - val_loss: 0.2487 - val_accuracy: 0.5520\n",
      "Epoch 42/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2492 - accuracy: 0.5334 - val_loss: 0.2487 - val_accuracy: 0.5521\n",
      "Epoch 43/1000\n",
      "563/563 [==============================] - 1s 930us/step - loss: 0.2491 - accuracy: 0.5358 - val_loss: 0.2487 - val_accuracy: 0.5521\n",
      "Epoch 44/1000\n",
      "563/563 [==============================] - 1s 956us/step - loss: 0.2491 - accuracy: 0.5351 - val_loss: 0.2487 - val_accuracy: 0.5520\n",
      "Epoch 45/1000\n",
      "563/563 [==============================] - 1s 981us/step - loss: 0.2492 - accuracy: 0.5302 - val_loss: 0.2487 - val_accuracy: 0.5520\n",
      "Epoch 46/1000\n",
      "563/563 [==============================] - 1s 949us/step - loss: 0.2492 - accuracy: 0.5331 - val_loss: 0.2487 - val_accuracy: 0.5521\n",
      "Epoch 47/1000\n",
      "563/563 [==============================] - 1s 922us/step - loss: 0.2490 - accuracy: 0.5386 - val_loss: 0.2486 - val_accuracy: 0.5520\n",
      "Epoch 48/1000\n",
      "563/563 [==============================] - 1s 968us/step - loss: 0.2492 - accuracy: 0.5304 - val_loss: 0.2486 - val_accuracy: 0.5520\n",
      "Epoch 49/1000\n",
      "563/563 [==============================] - 1s 959us/step - loss: 0.2489 - accuracy: 0.5422 - val_loss: 0.2486 - val_accuracy: 0.5520\n",
      "Epoch 50/1000\n",
      "563/563 [==============================] - 1s 945us/step - loss: 0.2493 - accuracy: 0.5284 - val_loss: 0.2486 - val_accuracy: 0.5520\n",
      "Epoch 51/1000\n",
      "563/563 [==============================] - 1s 943us/step - loss: 0.2490 - accuracy: 0.5339 - val_loss: 0.2485 - val_accuracy: 0.5561\n",
      "Epoch 52/1000\n",
      "563/563 [==============================] - 1s 988us/step - loss: 0.2491 - accuracy: 0.5347 - val_loss: 0.2485 - val_accuracy: 0.5562\n",
      "Epoch 53/1000\n",
      "563/563 [==============================] - 1s 933us/step - loss: 0.2491 - accuracy: 0.5352 - val_loss: 0.2485 - val_accuracy: 0.5562\n",
      "Epoch 54/1000\n",
      "563/563 [==============================] - 1s 930us/step - loss: 0.2491 - accuracy: 0.5350 - val_loss: 0.2485 - val_accuracy: 0.5562\n",
      "Epoch 55/1000\n",
      "563/563 [==============================] - 1s 924us/step - loss: 0.2489 - accuracy: 0.5403 - val_loss: 0.2485 - val_accuracy: 0.5564\n",
      "Epoch 56/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2488 - accuracy: 0.5408 - val_loss: 0.2484 - val_accuracy: 0.5564\n",
      "Epoch 57/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2490 - accuracy: 0.5351 - val_loss: 0.2484 - val_accuracy: 0.5564\n",
      "Epoch 58/1000\n",
      "563/563 [==============================] - 1s 991us/step - loss: 0.2489 - accuracy: 0.5388 - val_loss: 0.2484 - val_accuracy: 0.5565\n",
      "Epoch 59/1000\n",
      "563/563 [==============================] - 1s 984us/step - loss: 0.2490 - accuracy: 0.5365 - val_loss: 0.2484 - val_accuracy: 0.5565\n",
      "Epoch 60/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2488 - accuracy: 0.5401 - val_loss: 0.2484 - val_accuracy: 0.5566\n",
      "Epoch 61/1000\n",
      "563/563 [==============================] - 1s 958us/step - loss: 0.2489 - accuracy: 0.5385 - val_loss: 0.2484 - val_accuracy: 0.5565\n",
      "Epoch 62/1000\n",
      "563/563 [==============================] - 1s 956us/step - loss: 0.2488 - accuracy: 0.5392 - val_loss: 0.2484 - val_accuracy: 0.5566\n",
      "Epoch 63/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2490 - accuracy: 0.5353 - val_loss: 0.2484 - val_accuracy: 0.5566\n",
      "Epoch 64/1000\n",
      "563/563 [==============================] - 1s 933us/step - loss: 0.2489 - accuracy: 0.5372 - val_loss: 0.2484 - val_accuracy: 0.5565\n",
      "Epoch 65/1000\n",
      "563/563 [==============================] - 1s 933us/step - loss: 0.2490 - accuracy: 0.5346 - val_loss: 0.2484 - val_accuracy: 0.5565\n",
      "Epoch 66/1000\n",
      "563/563 [==============================] - 1s 943us/step - loss: 0.2491 - accuracy: 0.5305 - val_loss: 0.2483 - val_accuracy: 0.5564\n",
      "Epoch 67/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2490 - accuracy: 0.5349 - val_loss: 0.2483 - val_accuracy: 0.5566\n",
      "Epoch 68/1000\n",
      "563/563 [==============================] - 1s 934us/step - loss: 0.2488 - accuracy: 0.5400 - val_loss: 0.2483 - val_accuracy: 0.5566\n",
      "Epoch 69/1000\n",
      "563/563 [==============================] - 1s 929us/step - loss: 0.2488 - accuracy: 0.5391 - val_loss: 0.2483 - val_accuracy: 0.5566\n",
      "Epoch 70/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2490 - accuracy: 0.5317 - val_loss: 0.2483 - val_accuracy: 0.5565\n",
      "Epoch 71/1000\n",
      "563/563 [==============================] - 1s 933us/step - loss: 0.2488 - accuracy: 0.5380 - val_loss: 0.2483 - val_accuracy: 0.5564\n",
      "Epoch 72/1000\n",
      "563/563 [==============================] - 1s 927us/step - loss: 0.2488 - accuracy: 0.5366 - val_loss: 0.2483 - val_accuracy: 0.5564\n",
      "Epoch 73/1000\n",
      "563/563 [==============================] - 1s 949us/step - loss: 0.2489 - accuracy: 0.5326 - val_loss: 0.2483 - val_accuracy: 0.5564\n",
      "Epoch 74/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2489 - accuracy: 0.5342 - val_loss: 0.2483 - val_accuracy: 0.5564\n",
      "Epoch 75/1000\n",
      "563/563 [==============================] - 1s 924us/step - loss: 0.2482 - accuracy: 0.5510 - val_loss: 0.2482 - val_accuracy: 0.5564\n",
      "Epoch 76/1000\n",
      "563/563 [==============================] - 1s 959us/step - loss: 0.2487 - accuracy: 0.5404 - val_loss: 0.2482 - val_accuracy: 0.5565\n",
      "Epoch 77/1000\n",
      "563/563 [==============================] - 1s 958us/step - loss: 0.2489 - accuracy: 0.5342 - val_loss: 0.2482 - val_accuracy: 0.5564\n",
      "Epoch 78/1000\n",
      "563/563 [==============================] - 1s 977us/step - loss: 0.2486 - accuracy: 0.5405 - val_loss: 0.2482 - val_accuracy: 0.5564\n",
      "Epoch 79/1000\n",
      "563/563 [==============================] - 1s 951us/step - loss: 0.2485 - accuracy: 0.5416 - val_loss: 0.2482 - val_accuracy: 0.5564\n",
      "Epoch 80/1000\n",
      "563/563 [==============================] - 1s 931us/step - loss: 0.2486 - accuracy: 0.5385 - val_loss: 0.2482 - val_accuracy: 0.5564\n",
      "Epoch 81/1000\n",
      "563/563 [==============================] - 1s 983us/step - loss: 0.2487 - accuracy: 0.5372 - val_loss: 0.2482 - val_accuracy: 0.5561\n",
      "Epoch 82/1000\n",
      "563/563 [==============================] - 1s 952us/step - loss: 0.2487 - accuracy: 0.5360 - val_loss: 0.2482 - val_accuracy: 0.5561\n",
      "Epoch 83/1000\n",
      "563/563 [==============================] - 1s 941us/step - loss: 0.2488 - accuracy: 0.5340 - val_loss: 0.2481 - val_accuracy: 0.5561\n",
      "Epoch 84/1000\n",
      "563/563 [==============================] - 1s 924us/step - loss: 0.2488 - accuracy: 0.5351 - val_loss: 0.2481 - val_accuracy: 0.5561\n",
      "Epoch 85/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2488 - accuracy: 0.5355 - val_loss: 0.2481 - val_accuracy: 0.5560\n",
      "Epoch 86/1000\n",
      "563/563 [==============================] - 1s 941us/step - loss: 0.2486 - accuracy: 0.5389 - val_loss: 0.2481 - val_accuracy: 0.5558\n",
      "Epoch 87/1000\n",
      "563/563 [==============================] - 1s 936us/step - loss: 0.2487 - accuracy: 0.5330 - val_loss: 0.2481 - val_accuracy: 0.5558\n",
      "Epoch 88/1000\n",
      "563/563 [==============================] - 1s 965us/step - loss: 0.2486 - accuracy: 0.5404 - val_loss: 0.2481 - val_accuracy: 0.5560\n",
      "Epoch 89/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2486 - accuracy: 0.5389 - val_loss: 0.2481 - val_accuracy: 0.5564\n",
      "Epoch 90/1000\n",
      "563/563 [==============================] - 1s 940us/step - loss: 0.2485 - accuracy: 0.5398 - val_loss: 0.2481 - val_accuracy: 0.5565\n",
      "Epoch 91/1000\n",
      "563/563 [==============================] - 1s 927us/step - loss: 0.2484 - accuracy: 0.5421 - val_loss: 0.2480 - val_accuracy: 0.5565\n",
      "Epoch 92/1000\n",
      "563/563 [==============================] - 1s 963us/step - loss: 0.2484 - accuracy: 0.5423 - val_loss: 0.2480 - val_accuracy: 0.5565\n",
      "Epoch 93/1000\n",
      "563/563 [==============================] - 1s 974us/step - loss: 0.2484 - accuracy: 0.5404 - val_loss: 0.2480 - val_accuracy: 0.5562\n",
      "Epoch 94/1000\n",
      "563/563 [==============================] - 1s 933us/step - loss: 0.2482 - accuracy: 0.5448 - val_loss: 0.2479 - val_accuracy: 0.5561\n",
      "Epoch 95/1000\n",
      "563/563 [==============================] - 1s 972us/step - loss: 0.2483 - accuracy: 0.5411 - val_loss: 0.2479 - val_accuracy: 0.5602\n",
      "Epoch 96/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2483 - accuracy: 0.5443 - val_loss: 0.2479 - val_accuracy: 0.5602\n",
      "Epoch 97/1000\n",
      "563/563 [==============================] - 1s 938us/step - loss: 0.2485 - accuracy: 0.5404 - val_loss: 0.2479 - val_accuracy: 0.5604\n",
      "Epoch 98/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2482 - accuracy: 0.5462 - val_loss: 0.2478 - val_accuracy: 0.5605\n",
      "Epoch 99/1000\n",
      "563/563 [==============================] - 1s 966us/step - loss: 0.2482 - accuracy: 0.5428 - val_loss: 0.2478 - val_accuracy: 0.5608\n",
      "Epoch 100/1000\n",
      "563/563 [==============================] - 1s 993us/step - loss: 0.2482 - accuracy: 0.5452 - val_loss: 0.2478 - val_accuracy: 0.5609\n",
      "Epoch 101/1000\n",
      "563/563 [==============================] - 1s 949us/step - loss: 0.2484 - accuracy: 0.5385 - val_loss: 0.2477 - val_accuracy: 0.5609\n",
      "Epoch 102/1000\n",
      "563/563 [==============================] - 1s 954us/step - loss: 0.2480 - accuracy: 0.5469 - val_loss: 0.2476 - val_accuracy: 0.5608\n",
      "Epoch 103/1000\n",
      "563/563 [==============================] - 1s 993us/step - loss: 0.2483 - accuracy: 0.5375 - val_loss: 0.2476 - val_accuracy: 0.5606\n",
      "Epoch 104/1000\n",
      "563/563 [==============================] - 1s 995us/step - loss: 0.2481 - accuracy: 0.5415 - val_loss: 0.2475 - val_accuracy: 0.5605\n",
      "Epoch 105/1000\n",
      "563/563 [==============================] - 1s 950us/step - loss: 0.2479 - accuracy: 0.5426 - val_loss: 0.2475 - val_accuracy: 0.5596\n",
      "Epoch 106/1000\n",
      "563/563 [==============================] - 1s 918us/step - loss: 0.2477 - accuracy: 0.5487 - val_loss: 0.2474 - val_accuracy: 0.5604\n",
      "Epoch 107/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2477 - accuracy: 0.5473 - val_loss: 0.2473 - val_accuracy: 0.5600\n",
      "Epoch 108/1000\n",
      "563/563 [==============================] - 1s 920us/step - loss: 0.2477 - accuracy: 0.5415 - val_loss: 0.2472 - val_accuracy: 0.5599\n",
      "Epoch 109/1000\n",
      "563/563 [==============================] - 1s 941us/step - loss: 0.2473 - accuracy: 0.5486 - val_loss: 0.2471 - val_accuracy: 0.5593\n",
      "Epoch 110/1000\n",
      "563/563 [==============================] - 1s 950us/step - loss: 0.2475 - accuracy: 0.5437 - val_loss: 0.2469 - val_accuracy: 0.5639\n",
      "Epoch 111/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2470 - accuracy: 0.5541 - val_loss: 0.2466 - val_accuracy: 0.5864\n",
      "Epoch 112/1000\n",
      "563/563 [==============================] - 1s 963us/step - loss: 0.2464 - accuracy: 0.5843 - val_loss: 0.2464 - val_accuracy: 0.5866\n",
      "Epoch 113/1000\n",
      "563/563 [==============================] - 1s 929us/step - loss: 0.2464 - accuracy: 0.5792 - val_loss: 0.2462 - val_accuracy: 0.5868\n",
      "Epoch 114/1000\n",
      "563/563 [==============================] - 1s 984us/step - loss: 0.2460 - accuracy: 0.5854 - val_loss: 0.2461 - val_accuracy: 0.5867\n",
      "Epoch 115/1000\n",
      "563/563 [==============================] - 1s 972us/step - loss: 0.2465 - accuracy: 0.5731 - val_loss: 0.2458 - val_accuracy: 0.5867\n",
      "Epoch 116/1000\n",
      "563/563 [==============================] - 1s 917us/step - loss: 0.2459 - accuracy: 0.5824 - val_loss: 0.2454 - val_accuracy: 0.5866\n",
      "Epoch 117/1000\n",
      "563/563 [==============================] - 1s 947us/step - loss: 0.2456 - accuracy: 0.5892 - val_loss: 0.2447 - val_accuracy: 0.6025\n",
      "Epoch 118/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2448 - accuracy: 0.5942 - val_loss: 0.2442 - val_accuracy: 0.6069\n",
      "Epoch 119/1000\n",
      "563/563 [==============================] - 1s 924us/step - loss: 0.2445 - accuracy: 0.5907 - val_loss: 0.2437 - val_accuracy: 0.6070\n",
      "Epoch 120/1000\n",
      "563/563 [==============================] - 1s 924us/step - loss: 0.2436 - accuracy: 0.5917 - val_loss: 0.2431 - val_accuracy: 0.6100\n",
      "Epoch 121/1000\n",
      "563/563 [==============================] - 1s 954us/step - loss: 0.2433 - accuracy: 0.5983 - val_loss: 0.2425 - val_accuracy: 0.6079\n",
      "Epoch 122/1000\n",
      "563/563 [==============================] - 1s 997us/step - loss: 0.2430 - accuracy: 0.5961 - val_loss: 0.2419 - val_accuracy: 0.6060\n",
      "Epoch 123/1000\n",
      "563/563 [==============================] - 1s 979us/step - loss: 0.2424 - accuracy: 0.5966 - val_loss: 0.2414 - val_accuracy: 0.6068\n",
      "Epoch 124/1000\n",
      "563/563 [==============================] - 1s 933us/step - loss: 0.2410 - accuracy: 0.6061 - val_loss: 0.2410 - val_accuracy: 0.6095\n",
      "Epoch 125/1000\n",
      "563/563 [==============================] - 1s 979us/step - loss: 0.2407 - accuracy: 0.5995 - val_loss: 0.2405 - val_accuracy: 0.6088\n",
      "Epoch 126/1000\n",
      "563/563 [==============================] - 1s 972us/step - loss: 0.2406 - accuracy: 0.6016 - val_loss: 0.2401 - val_accuracy: 0.6104\n",
      "Epoch 127/1000\n",
      "563/563 [==============================] - 1s 934us/step - loss: 0.2397 - accuracy: 0.6060 - val_loss: 0.2396 - val_accuracy: 0.6104\n",
      "Epoch 128/1000\n",
      "563/563 [==============================] - 1s 952us/step - loss: 0.2401 - accuracy: 0.6118 - val_loss: 0.2392 - val_accuracy: 0.6250\n",
      "Epoch 129/1000\n",
      "563/563 [==============================] - 1s 982us/step - loss: 0.2385 - accuracy: 0.6244 - val_loss: 0.2388 - val_accuracy: 0.6250\n",
      "Epoch 130/1000\n",
      "563/563 [==============================] - 1s 938us/step - loss: 0.2378 - accuracy: 0.6210 - val_loss: 0.2380 - val_accuracy: 0.6303\n",
      "Epoch 131/1000\n",
      "563/563 [==============================] - 1s 943us/step - loss: 0.2374 - accuracy: 0.6311 - val_loss: 0.2366 - val_accuracy: 0.6636\n",
      "Epoch 132/1000\n",
      "563/563 [==============================] - 1s 936us/step - loss: 0.2351 - accuracy: 0.6689 - val_loss: 0.2349 - val_accuracy: 0.6853\n",
      "Epoch 133/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2346 - accuracy: 0.6779 - val_loss: 0.2337 - val_accuracy: 0.6849\n",
      "Epoch 134/1000\n",
      "563/563 [==============================] - 1s 966us/step - loss: 0.2323 - accuracy: 0.6846 - val_loss: 0.2327 - val_accuracy: 0.6848\n",
      "Epoch 135/1000\n",
      "563/563 [==============================] - 1s 963us/step - loss: 0.2308 - accuracy: 0.6815 - val_loss: 0.2316 - val_accuracy: 0.6933\n",
      "Epoch 136/1000\n",
      "563/563 [==============================] - 1s 970us/step - loss: 0.2301 - accuracy: 0.6918 - val_loss: 0.2307 - val_accuracy: 0.6947\n",
      "Epoch 137/1000\n",
      "563/563 [==============================] - 1s 975us/step - loss: 0.2294 - accuracy: 0.6852 - val_loss: 0.2298 - val_accuracy: 0.6955\n",
      "Epoch 138/1000\n",
      "563/563 [==============================] - 1s 940us/step - loss: 0.2288 - accuracy: 0.6975 - val_loss: 0.2289 - val_accuracy: 0.6954\n",
      "Epoch 139/1000\n",
      "563/563 [==============================] - 1s 938us/step - loss: 0.2264 - accuracy: 0.6935 - val_loss: 0.2281 - val_accuracy: 0.6941\n",
      "Epoch 140/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2260 - accuracy: 0.6941 - val_loss: 0.2273 - val_accuracy: 0.6945\n",
      "Epoch 141/1000\n",
      "563/563 [==============================] - 1s 959us/step - loss: 0.2259 - accuracy: 0.6977 - val_loss: 0.2266 - val_accuracy: 0.6976\n",
      "Epoch 142/1000\n",
      "563/563 [==============================] - 1s 945us/step - loss: 0.2250 - accuracy: 0.6907 - val_loss: 0.2260 - val_accuracy: 0.6988\n",
      "Epoch 143/1000\n",
      "563/563 [==============================] - 1s 933us/step - loss: 0.2236 - accuracy: 0.7013 - val_loss: 0.2240 - val_accuracy: 0.7212\n",
      "Epoch 144/1000\n",
      "563/563 [==============================] - 1s 994us/step - loss: 0.2209 - accuracy: 0.7223 - val_loss: 0.2226 - val_accuracy: 0.7222\n",
      "Epoch 145/1000\n",
      "563/563 [==============================] - 1s 917us/step - loss: 0.2212 - accuracy: 0.7210 - val_loss: 0.2217 - val_accuracy: 0.7226\n",
      "Epoch 146/1000\n",
      "563/563 [==============================] - 1s 945us/step - loss: 0.2206 - accuracy: 0.7134 - val_loss: 0.2211 - val_accuracy: 0.7227\n",
      "Epoch 147/1000\n",
      "563/563 [==============================] - 1s 975us/step - loss: 0.2212 - accuracy: 0.7083 - val_loss: 0.2205 - val_accuracy: 0.7230\n",
      "Epoch 148/1000\n",
      "563/563 [==============================] - 1s 966us/step - loss: 0.2186 - accuracy: 0.7183 - val_loss: 0.2200 - val_accuracy: 0.7231\n",
      "Epoch 149/1000\n",
      "563/563 [==============================] - 1s 945us/step - loss: 0.2184 - accuracy: 0.7146 - val_loss: 0.2195 - val_accuracy: 0.7245\n",
      "Epoch 150/1000\n",
      "563/563 [==============================] - 1s 936us/step - loss: 0.2180 - accuracy: 0.7191 - val_loss: 0.2190 - val_accuracy: 0.7249\n",
      "Epoch 151/1000\n",
      "563/563 [==============================] - 1s 979us/step - loss: 0.2175 - accuracy: 0.7186 - val_loss: 0.2186 - val_accuracy: 0.7251\n",
      "Epoch 152/1000\n",
      "563/563 [==============================] - 1s 981us/step - loss: 0.2162 - accuracy: 0.7203 - val_loss: 0.2182 - val_accuracy: 0.7245\n",
      "Epoch 153/1000\n",
      "563/563 [==============================] - 1s 959us/step - loss: 0.2149 - accuracy: 0.7208 - val_loss: 0.2178 - val_accuracy: 0.7267\n",
      "Epoch 154/1000\n",
      "563/563 [==============================] - 1s 933us/step - loss: 0.2171 - accuracy: 0.7198 - val_loss: 0.2174 - val_accuracy: 0.7273\n",
      "Epoch 155/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2158 - accuracy: 0.7207 - val_loss: 0.2170 - val_accuracy: 0.7262\n",
      "Epoch 156/1000\n",
      "563/563 [==============================] - 1s 947us/step - loss: 0.2165 - accuracy: 0.7120 - val_loss: 0.2166 - val_accuracy: 0.7264\n",
      "Epoch 157/1000\n",
      "563/563 [==============================] - 1s 945us/step - loss: 0.2168 - accuracy: 0.7153 - val_loss: 0.2163 - val_accuracy: 0.7264\n",
      "Epoch 158/1000\n",
      "563/563 [==============================] - 1s 945us/step - loss: 0.2169 - accuracy: 0.7156 - val_loss: 0.2160 - val_accuracy: 0.7273\n",
      "Epoch 159/1000\n",
      "563/563 [==============================] - 1s 990us/step - loss: 0.2148 - accuracy: 0.7206 - val_loss: 0.2157 - val_accuracy: 0.7271\n",
      "Epoch 160/1000\n",
      "563/563 [==============================] - 1s 940us/step - loss: 0.2142 - accuracy: 0.7221 - val_loss: 0.2154 - val_accuracy: 0.7273\n",
      "Epoch 161/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2137 - accuracy: 0.7281 - val_loss: 0.2151 - val_accuracy: 0.7274\n",
      "Epoch 162/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2160 - accuracy: 0.7163 - val_loss: 0.2149 - val_accuracy: 0.7273\n",
      "Epoch 163/1000\n",
      "563/563 [==============================] - 1s 961us/step - loss: 0.2127 - accuracy: 0.7230 - val_loss: 0.2147 - val_accuracy: 0.7292\n",
      "Epoch 164/1000\n",
      "563/563 [==============================] - 1s 940us/step - loss: 0.2113 - accuracy: 0.7280 - val_loss: 0.2145 - val_accuracy: 0.7289\n",
      "Epoch 165/1000\n",
      "563/563 [==============================] - 1s 940us/step - loss: 0.2132 - accuracy: 0.7211 - val_loss: 0.2142 - val_accuracy: 0.7289\n",
      "Epoch 166/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2115 - accuracy: 0.7244 - val_loss: 0.2140 - val_accuracy: 0.7288\n",
      "Epoch 167/1000\n",
      "563/563 [==============================] - 1s 963us/step - loss: 0.2112 - accuracy: 0.7206 - val_loss: 0.2138 - val_accuracy: 0.7287\n",
      "Epoch 168/1000\n",
      "563/563 [==============================] - 1s 940us/step - loss: 0.2135 - accuracy: 0.7189 - val_loss: 0.2136 - val_accuracy: 0.7286\n",
      "Epoch 169/1000\n",
      "563/563 [==============================] - 1s 959us/step - loss: 0.2128 - accuracy: 0.7209 - val_loss: 0.2135 - val_accuracy: 0.7283\n",
      "Epoch 170/1000\n",
      "563/563 [==============================] - 1s 979us/step - loss: 0.2110 - accuracy: 0.7281 - val_loss: 0.2133 - val_accuracy: 0.7283\n",
      "Epoch 171/1000\n",
      "563/563 [==============================] - 1s 965us/step - loss: 0.2106 - accuracy: 0.7275 - val_loss: 0.2131 - val_accuracy: 0.7286\n",
      "Epoch 172/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2094 - accuracy: 0.7283 - val_loss: 0.2129 - val_accuracy: 0.7284\n",
      "Epoch 173/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2109 - accuracy: 0.7269 - val_loss: 0.2128 - val_accuracy: 0.7282\n",
      "Epoch 174/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2109 - accuracy: 0.7261 - val_loss: 0.2126 - val_accuracy: 0.7280\n",
      "Epoch 175/1000\n",
      "563/563 [==============================] - 1s 956us/step - loss: 0.2112 - accuracy: 0.7211 - val_loss: 0.2125 - val_accuracy: 0.7280\n",
      "Epoch 176/1000\n",
      "563/563 [==============================] - 1s 962us/step - loss: 0.2095 - accuracy: 0.7281 - val_loss: 0.2123 - val_accuracy: 0.7280\n",
      "Epoch 177/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2099 - accuracy: 0.7268 - val_loss: 0.2122 - val_accuracy: 0.7286\n",
      "Epoch 178/1000\n",
      "563/563 [==============================] - 1s 972us/step - loss: 0.2112 - accuracy: 0.7216 - val_loss: 0.2121 - val_accuracy: 0.7284\n",
      "Epoch 179/1000\n",
      "563/563 [==============================] - 1s 927us/step - loss: 0.2084 - accuracy: 0.7310 - val_loss: 0.2119 - val_accuracy: 0.7286\n",
      "Epoch 180/1000\n",
      "563/563 [==============================] - 1s 977us/step - loss: 0.2099 - accuracy: 0.7254 - val_loss: 0.2118 - val_accuracy: 0.7284\n",
      "Epoch 181/1000\n",
      "563/563 [==============================] - 1s 970us/step - loss: 0.2096 - accuracy: 0.7203 - val_loss: 0.2116 - val_accuracy: 0.7283\n",
      "Epoch 182/1000\n",
      "563/563 [==============================] - 1s 938us/step - loss: 0.2100 - accuracy: 0.7261 - val_loss: 0.2115 - val_accuracy: 0.7283\n",
      "Epoch 183/1000\n",
      "563/563 [==============================] - 1s 933us/step - loss: 0.2106 - accuracy: 0.7232 - val_loss: 0.2114 - val_accuracy: 0.7282\n",
      "Epoch 184/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2102 - accuracy: 0.7281 - val_loss: 0.2112 - val_accuracy: 0.7282\n",
      "Epoch 185/1000\n",
      "563/563 [==============================] - 1s 938us/step - loss: 0.2107 - accuracy: 0.7230 - val_loss: 0.2111 - val_accuracy: 0.7282\n",
      "Epoch 186/1000\n",
      "563/563 [==============================] - 1s 937us/step - loss: 0.2101 - accuracy: 0.7214 - val_loss: 0.2110 - val_accuracy: 0.7289\n",
      "Epoch 187/1000\n",
      "563/563 [==============================] - 1s 961us/step - loss: 0.2099 - accuracy: 0.7248 - val_loss: 0.2108 - val_accuracy: 0.7288\n",
      "Epoch 188/1000\n",
      "563/563 [==============================] - 1s 998us/step - loss: 0.2100 - accuracy: 0.7246 - val_loss: 0.2107 - val_accuracy: 0.7288\n",
      "Epoch 189/1000\n",
      "563/563 [==============================] - 1s 952us/step - loss: 0.2088 - accuracy: 0.7255 - val_loss: 0.2105 - val_accuracy: 0.7310\n",
      "Epoch 190/1000\n",
      "563/563 [==============================] - 1s 988us/step - loss: 0.2074 - accuracy: 0.7312 - val_loss: 0.2103 - val_accuracy: 0.7311\n",
      "Epoch 191/1000\n",
      "563/563 [==============================] - 1s 979us/step - loss: 0.2079 - accuracy: 0.7287 - val_loss: 0.2102 - val_accuracy: 0.7309\n",
      "Epoch 192/1000\n",
      "563/563 [==============================] - 1s 979us/step - loss: 0.2072 - accuracy: 0.7338 - val_loss: 0.2100 - val_accuracy: 0.7309\n",
      "Epoch 193/1000\n",
      "563/563 [==============================] - 1s 941us/step - loss: 0.2077 - accuracy: 0.7259 - val_loss: 0.2099 - val_accuracy: 0.7309\n",
      "Epoch 194/1000\n",
      "563/563 [==============================] - 1s 934us/step - loss: 0.2077 - accuracy: 0.7287 - val_loss: 0.2098 - val_accuracy: 0.7310\n",
      "Epoch 195/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2074 - accuracy: 0.7287 - val_loss: 0.2097 - val_accuracy: 0.7310\n",
      "Epoch 196/1000\n",
      "563/563 [==============================] - 1s 933us/step - loss: 0.2081 - accuracy: 0.7235 - val_loss: 0.2096 - val_accuracy: 0.7311\n",
      "Epoch 197/1000\n",
      "563/563 [==============================] - 1s 947us/step - loss: 0.2085 - accuracy: 0.7307 - val_loss: 0.2094 - val_accuracy: 0.7313\n",
      "Epoch 198/1000\n",
      "563/563 [==============================] - 1s 956us/step - loss: 0.2067 - accuracy: 0.7296 - val_loss: 0.2093 - val_accuracy: 0.7313\n",
      "Epoch 199/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2069 - accuracy: 0.7319 - val_loss: 0.2092 - val_accuracy: 0.7311\n",
      "Epoch 200/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2078 - accuracy: 0.7357 - val_loss: 0.2091 - val_accuracy: 0.7311\n",
      "Epoch 201/1000\n",
      "563/563 [==============================] - 1s 922us/step - loss: 0.2064 - accuracy: 0.7343 - val_loss: 0.2090 - val_accuracy: 0.7313\n",
      "Epoch 202/1000\n",
      "563/563 [==============================] - 1s 991us/step - loss: 0.2063 - accuracy: 0.7321 - val_loss: 0.2089 - val_accuracy: 0.7315\n",
      "Epoch 203/1000\n",
      "563/563 [==============================] - 1s 940us/step - loss: 0.2057 - accuracy: 0.7361 - val_loss: 0.2087 - val_accuracy: 0.7313\n",
      "Epoch 204/1000\n",
      "563/563 [==============================] - 1s 938us/step - loss: 0.2071 - accuracy: 0.7329 - val_loss: 0.2087 - val_accuracy: 0.7314\n",
      "Epoch 205/1000\n",
      "563/563 [==============================] - 1s 981us/step - loss: 0.2063 - accuracy: 0.7330 - val_loss: 0.2085 - val_accuracy: 0.7304\n",
      "Epoch 206/1000\n",
      "563/563 [==============================] - 1s 984us/step - loss: 0.2050 - accuracy: 0.7327 - val_loss: 0.2084 - val_accuracy: 0.7302\n",
      "Epoch 207/1000\n",
      "563/563 [==============================] - 1s 963us/step - loss: 0.2056 - accuracy: 0.7357 - val_loss: 0.2084 - val_accuracy: 0.7302\n",
      "Epoch 208/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2063 - accuracy: 0.7317 - val_loss: 0.2082 - val_accuracy: 0.7302\n",
      "Epoch 209/1000\n",
      "563/563 [==============================] - 1s 998us/step - loss: 0.2067 - accuracy: 0.7283 - val_loss: 0.2082 - val_accuracy: 0.7302\n",
      "Epoch 210/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2043 - accuracy: 0.7343 - val_loss: 0.2081 - val_accuracy: 0.7302\n",
      "Epoch 211/1000\n",
      "563/563 [==============================] - 1s 961us/step - loss: 0.2055 - accuracy: 0.7294 - val_loss: 0.2080 - val_accuracy: 0.7308\n",
      "Epoch 212/1000\n",
      "563/563 [==============================] - 1s 941us/step - loss: 0.2062 - accuracy: 0.7325 - val_loss: 0.2079 - val_accuracy: 0.7308\n",
      "Epoch 213/1000\n",
      "563/563 [==============================] - 1s 975us/step - loss: 0.2061 - accuracy: 0.7325 - val_loss: 0.2078 - val_accuracy: 0.7308\n",
      "Epoch 214/1000\n",
      "563/563 [==============================] - 1s 943us/step - loss: 0.2056 - accuracy: 0.7319 - val_loss: 0.2077 - val_accuracy: 0.7308\n",
      "Epoch 215/1000\n",
      "563/563 [==============================] - 1s 943us/step - loss: 0.2061 - accuracy: 0.7254 - val_loss: 0.2076 - val_accuracy: 0.7308\n",
      "Epoch 216/1000\n",
      "563/563 [==============================] - 1s 952us/step - loss: 0.2057 - accuracy: 0.7322 - val_loss: 0.2075 - val_accuracy: 0.7308\n",
      "Epoch 217/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2056 - accuracy: 0.7345 - val_loss: 0.2074 - val_accuracy: 0.7321\n",
      "Epoch 218/1000\n",
      "563/563 [==============================] - 1s 952us/step - loss: 0.2051 - accuracy: 0.7324 - val_loss: 0.2073 - val_accuracy: 0.7321\n",
      "Epoch 219/1000\n",
      "563/563 [==============================] - 1s 947us/step - loss: 0.2039 - accuracy: 0.7339 - val_loss: 0.2072 - val_accuracy: 0.7319\n",
      "Epoch 220/1000\n",
      "563/563 [==============================] - 1s 965us/step - loss: 0.2057 - accuracy: 0.7315 - val_loss: 0.2072 - val_accuracy: 0.7321\n",
      "Epoch 221/1000\n",
      "563/563 [==============================] - 1s 981us/step - loss: 0.2047 - accuracy: 0.7316 - val_loss: 0.2071 - val_accuracy: 0.7319\n",
      "Epoch 222/1000\n",
      "563/563 [==============================] - 1s 949us/step - loss: 0.2060 - accuracy: 0.7314 - val_loss: 0.2070 - val_accuracy: 0.7319\n",
      "Epoch 223/1000\n",
      "563/563 [==============================] - 1s 961us/step - loss: 0.2064 - accuracy: 0.7297 - val_loss: 0.2069 - val_accuracy: 0.7319\n",
      "Epoch 224/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2039 - accuracy: 0.7332 - val_loss: 0.2068 - val_accuracy: 0.7319\n",
      "Epoch 225/1000\n",
      "563/563 [==============================] - 1s 947us/step - loss: 0.2044 - accuracy: 0.7337 - val_loss: 0.2068 - val_accuracy: 0.7319\n",
      "Epoch 226/1000\n",
      "563/563 [==============================] - 1s 938us/step - loss: 0.2054 - accuracy: 0.7292 - val_loss: 0.2067 - val_accuracy: 0.7319\n",
      "Epoch 227/1000\n",
      "563/563 [==============================] - 1s 950us/step - loss: 0.2050 - accuracy: 0.7322 - val_loss: 0.2066 - val_accuracy: 0.7319\n",
      "Epoch 228/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2052 - accuracy: 0.7295 - val_loss: 0.2065 - val_accuracy: 0.7324\n",
      "Epoch 229/1000\n",
      "563/563 [==============================] - 1s 977us/step - loss: 0.2051 - accuracy: 0.7292 - val_loss: 0.2065 - val_accuracy: 0.7324\n",
      "Epoch 230/1000\n",
      "563/563 [==============================] - 1s 965us/step - loss: 0.2057 - accuracy: 0.7279 - val_loss: 0.2064 - val_accuracy: 0.7319\n",
      "Epoch 231/1000\n",
      "563/563 [==============================] - 1s 961us/step - loss: 0.2032 - accuracy: 0.7380 - val_loss: 0.2063 - val_accuracy: 0.7323\n",
      "Epoch 232/1000\n",
      "563/563 [==============================] - 1s 988us/step - loss: 0.2043 - accuracy: 0.7318 - val_loss: 0.2062 - val_accuracy: 0.7326\n",
      "Epoch 233/1000\n",
      "563/563 [==============================] - 1s 922us/step - loss: 0.2045 - accuracy: 0.7343 - val_loss: 0.2062 - val_accuracy: 0.7326\n",
      "Epoch 234/1000\n",
      "563/563 [==============================] - 1s 949us/step - loss: 0.2030 - accuracy: 0.7345 - val_loss: 0.2061 - val_accuracy: 0.7326\n",
      "Epoch 235/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2060 - accuracy: 0.7272 - val_loss: 0.2060 - val_accuracy: 0.7326\n",
      "Epoch 236/1000\n",
      "563/563 [==============================] - 1s 929us/step - loss: 0.2056 - accuracy: 0.7249 - val_loss: 0.2059 - val_accuracy: 0.7326\n",
      "Epoch 237/1000\n",
      "563/563 [==============================] - 1s 951us/step - loss: 0.2018 - accuracy: 0.7361 - val_loss: 0.2059 - val_accuracy: 0.7326\n",
      "Epoch 238/1000\n",
      "563/563 [==============================] - 1s 956us/step - loss: 0.2037 - accuracy: 0.7378 - val_loss: 0.2058 - val_accuracy: 0.7326\n",
      "Epoch 239/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2054 - accuracy: 0.7249 - val_loss: 0.2057 - val_accuracy: 0.7326\n",
      "Epoch 240/1000\n",
      "563/563 [==============================] - 1s 929us/step - loss: 0.2044 - accuracy: 0.7311 - val_loss: 0.2056 - val_accuracy: 0.7326\n",
      "Epoch 241/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2050 - accuracy: 0.7327 - val_loss: 0.2056 - val_accuracy: 0.7326\n",
      "Epoch 242/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2037 - accuracy: 0.7301 - val_loss: 0.2055 - val_accuracy: 0.7341\n",
      "Epoch 243/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2022 - accuracy: 0.7383 - val_loss: 0.2054 - val_accuracy: 0.7341\n",
      "Epoch 244/1000\n",
      "563/563 [==============================] - 1s 977us/step - loss: 0.2038 - accuracy: 0.7283 - val_loss: 0.2054 - val_accuracy: 0.7341\n",
      "Epoch 245/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2036 - accuracy: 0.7330 - val_loss: 0.2053 - val_accuracy: 0.7341\n",
      "Epoch 246/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2028 - accuracy: 0.7325 - val_loss: 0.2052 - val_accuracy: 0.7341\n",
      "Epoch 247/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2020 - accuracy: 0.7359 - val_loss: 0.2052 - val_accuracy: 0.7341\n",
      "Epoch 248/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2054 - accuracy: 0.7254 - val_loss: 0.2051 - val_accuracy: 0.7341\n",
      "Epoch 249/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2043 - accuracy: 0.7262 - val_loss: 0.2050 - val_accuracy: 0.7341\n",
      "Epoch 250/1000\n",
      "563/563 [==============================] - 1s 977us/step - loss: 0.2016 - accuracy: 0.7336 - val_loss: 0.2050 - val_accuracy: 0.7341\n",
      "Epoch 251/1000\n",
      "563/563 [==============================] - 1s 954us/step - loss: 0.2041 - accuracy: 0.7282 - val_loss: 0.2049 - val_accuracy: 0.7341\n",
      "Epoch 252/1000\n",
      "563/563 [==============================] - 1s 954us/step - loss: 0.2031 - accuracy: 0.7307 - val_loss: 0.2048 - val_accuracy: 0.7341\n",
      "Epoch 253/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2026 - accuracy: 0.7300 - val_loss: 0.2048 - val_accuracy: 0.7341\n",
      "Epoch 254/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2032 - accuracy: 0.7307 - val_loss: 0.2047 - val_accuracy: 0.7341\n",
      "Epoch 255/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2028 - accuracy: 0.7306 - val_loss: 0.2046 - val_accuracy: 0.7340\n",
      "Epoch 256/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2027 - accuracy: 0.7337 - val_loss: 0.2046 - val_accuracy: 0.7340\n",
      "Epoch 257/1000\n",
      "563/563 [==============================] - 1s 941us/step - loss: 0.2007 - accuracy: 0.7358 - val_loss: 0.2045 - val_accuracy: 0.7340\n",
      "Epoch 258/1000\n",
      "563/563 [==============================] - 1s 990us/step - loss: 0.2012 - accuracy: 0.7320 - val_loss: 0.2044 - val_accuracy: 0.7340\n",
      "Epoch 259/1000\n",
      "563/563 [==============================] - 1s 966us/step - loss: 0.2029 - accuracy: 0.7297 - val_loss: 0.2044 - val_accuracy: 0.7341\n",
      "Epoch 260/1000\n",
      "563/563 [==============================] - 1s 986us/step - loss: 0.2024 - accuracy: 0.7331 - val_loss: 0.2043 - val_accuracy: 0.7340\n",
      "Epoch 261/1000\n",
      "563/563 [==============================] - 1s 927us/step - loss: 0.2023 - accuracy: 0.7315 - val_loss: 0.2042 - val_accuracy: 0.7340\n",
      "Epoch 262/1000\n",
      "563/563 [==============================] - 1s 945us/step - loss: 0.2017 - accuracy: 0.7334 - val_loss: 0.2042 - val_accuracy: 0.7340\n",
      "Epoch 263/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2026 - accuracy: 0.7298 - val_loss: 0.2041 - val_accuracy: 0.7341\n",
      "Epoch 264/1000\n",
      "563/563 [==============================] - 1s 972us/step - loss: 0.2038 - accuracy: 0.7244 - val_loss: 0.2040 - val_accuracy: 0.7341\n",
      "Epoch 265/1000\n",
      "563/563 [==============================] - 1s 934us/step - loss: 0.1990 - accuracy: 0.7394 - val_loss: 0.2040 - val_accuracy: 0.7341\n",
      "Epoch 266/1000\n",
      "563/563 [==============================] - 1s 949us/step - loss: 0.2010 - accuracy: 0.7326 - val_loss: 0.2039 - val_accuracy: 0.7341\n",
      "Epoch 267/1000\n",
      "563/563 [==============================] - 1s 997us/step - loss: 0.2040 - accuracy: 0.7250 - val_loss: 0.2039 - val_accuracy: 0.7340\n",
      "Epoch 268/1000\n",
      "563/563 [==============================] - 1s 927us/step - loss: 0.2013 - accuracy: 0.7323 - val_loss: 0.2038 - val_accuracy: 0.7340\n",
      "Epoch 269/1000\n",
      "563/563 [==============================] - 1s 934us/step - loss: 0.2030 - accuracy: 0.7253 - val_loss: 0.2037 - val_accuracy: 0.7340\n",
      "Epoch 270/1000\n",
      "563/563 [==============================] - 1s 977us/step - loss: 0.2025 - accuracy: 0.7317 - val_loss: 0.2037 - val_accuracy: 0.7340\n",
      "Epoch 271/1000\n",
      "563/563 [==============================] - 1s 966us/step - loss: 0.2015 - accuracy: 0.7321 - val_loss: 0.2036 - val_accuracy: 0.7340\n",
      "Epoch 272/1000\n",
      "563/563 [==============================] - 1s 925us/step - loss: 0.2024 - accuracy: 0.7329 - val_loss: 0.2035 - val_accuracy: 0.7340\n",
      "Epoch 273/1000\n",
      "563/563 [==============================] - 1s 925us/step - loss: 0.2012 - accuracy: 0.7327 - val_loss: 0.2035 - val_accuracy: 0.7340\n",
      "Epoch 274/1000\n",
      "563/563 [==============================] - 1s 961us/step - loss: 0.2030 - accuracy: 0.7268 - val_loss: 0.2034 - val_accuracy: 0.7340\n",
      "Epoch 275/1000\n",
      "563/563 [==============================] - 1s 960us/step - loss: 0.2019 - accuracy: 0.7336 - val_loss: 0.2034 - val_accuracy: 0.7340\n",
      "Epoch 276/1000\n",
      "563/563 [==============================] - 1s 929us/step - loss: 0.2013 - accuracy: 0.7330 - val_loss: 0.2033 - val_accuracy: 0.7341\n",
      "Epoch 277/1000\n",
      "563/563 [==============================] - 1s 924us/step - loss: 0.2015 - accuracy: 0.7298 - val_loss: 0.2032 - val_accuracy: 0.7341\n",
      "Epoch 278/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2012 - accuracy: 0.7314 - val_loss: 0.2032 - val_accuracy: 0.7341\n",
      "Epoch 279/1000\n",
      "563/563 [==============================] - 1s 929us/step - loss: 0.2012 - accuracy: 0.7355 - val_loss: 0.2031 - val_accuracy: 0.7341\n",
      "Epoch 280/1000\n",
      "563/563 [==============================] - 1s 974us/step - loss: 0.2016 - accuracy: 0.7281 - val_loss: 0.2031 - val_accuracy: 0.7344\n",
      "Epoch 281/1000\n",
      "563/563 [==============================] - 1s 958us/step - loss: 0.2000 - accuracy: 0.7330 - val_loss: 0.2030 - val_accuracy: 0.7344\n",
      "Epoch 282/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2000 - accuracy: 0.7374 - val_loss: 0.2030 - val_accuracy: 0.7344\n",
      "Epoch 283/1000\n",
      "563/563 [==============================] - 1s 954us/step - loss: 0.2020 - accuracy: 0.7326 - val_loss: 0.2029 - val_accuracy: 0.7344\n",
      "Epoch 284/1000\n",
      "563/563 [==============================] - 1s 966us/step - loss: 0.2003 - accuracy: 0.7363 - val_loss: 0.2029 - val_accuracy: 0.7344\n",
      "Epoch 285/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2017 - accuracy: 0.7310 - val_loss: 0.2028 - val_accuracy: 0.7344\n",
      "Epoch 286/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2021 - accuracy: 0.7285 - val_loss: 0.2028 - val_accuracy: 0.7344\n",
      "Epoch 287/1000\n",
      "563/563 [==============================] - 1s 936us/step - loss: 0.1997 - accuracy: 0.7328 - val_loss: 0.2027 - val_accuracy: 0.7344\n",
      "Epoch 288/1000\n",
      "563/563 [==============================] - 1s 936us/step - loss: 0.2007 - accuracy: 0.7345 - val_loss: 0.2027 - val_accuracy: 0.7344\n",
      "Epoch 289/1000\n",
      "563/563 [==============================] - 1s 990us/step - loss: 0.2003 - accuracy: 0.7339 - val_loss: 0.2026 - val_accuracy: 0.7344\n",
      "Epoch 290/1000\n",
      "563/563 [==============================] - 1s 933us/step - loss: 0.1982 - accuracy: 0.7397 - val_loss: 0.2025 - val_accuracy: 0.7344\n",
      "Epoch 291/1000\n",
      "563/563 [==============================] - 1s 933us/step - loss: 0.2014 - accuracy: 0.7328 - val_loss: 0.2025 - val_accuracy: 0.7344\n",
      "Epoch 292/1000\n",
      "563/563 [==============================] - 1s 965us/step - loss: 0.2006 - accuracy: 0.7327 - val_loss: 0.2024 - val_accuracy: 0.7344\n",
      "Epoch 293/1000\n",
      "563/563 [==============================] - 1s 988us/step - loss: 0.1988 - accuracy: 0.7381 - val_loss: 0.2024 - val_accuracy: 0.7344\n",
      "Epoch 294/1000\n",
      "563/563 [==============================] - 1s 909us/step - loss: 0.2009 - accuracy: 0.7284 - val_loss: 0.2023 - val_accuracy: 0.7344\n",
      "Epoch 295/1000\n",
      "563/563 [==============================] - 1s 927us/step - loss: 0.2003 - accuracy: 0.7328 - val_loss: 0.2023 - val_accuracy: 0.7344\n",
      "Epoch 296/1000\n",
      "563/563 [==============================] - 1s 993us/step - loss: 0.2018 - accuracy: 0.7262 - val_loss: 0.2022 - val_accuracy: 0.7343\n",
      "Epoch 297/1000\n",
      "563/563 [==============================] - 1s 979us/step - loss: 0.2020 - accuracy: 0.7293 - val_loss: 0.2022 - val_accuracy: 0.7343\n",
      "Epoch 298/1000\n",
      "563/563 [==============================] - 1s 927us/step - loss: 0.2004 - accuracy: 0.7318 - val_loss: 0.2021 - val_accuracy: 0.7343\n",
      "Epoch 299/1000\n",
      "563/563 [==============================] - 1s 936us/step - loss: 0.2008 - accuracy: 0.7302 - val_loss: 0.2021 - val_accuracy: 0.7343\n",
      "Epoch 300/1000\n",
      "563/563 [==============================] - 1s 975us/step - loss: 0.2015 - accuracy: 0.7266 - val_loss: 0.2020 - val_accuracy: 0.7343\n",
      "Epoch 301/1000\n",
      "563/563 [==============================] - 1s 927us/step - loss: 0.2010 - accuracy: 0.7318 - val_loss: 0.2020 - val_accuracy: 0.7343\n",
      "Epoch 302/1000\n",
      "563/563 [==============================] - 1s 918us/step - loss: 0.2000 - accuracy: 0.7345 - val_loss: 0.2019 - val_accuracy: 0.7344\n",
      "Epoch 303/1000\n",
      "563/563 [==============================] - 1s 929us/step - loss: 0.2010 - accuracy: 0.7297 - val_loss: 0.2019 - val_accuracy: 0.7344\n",
      "Epoch 304/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2008 - accuracy: 0.7312 - val_loss: 0.2018 - val_accuracy: 0.7344\n",
      "Epoch 305/1000\n",
      "563/563 [==============================] - 1s 943us/step - loss: 0.1989 - accuracy: 0.7356 - val_loss: 0.2018 - val_accuracy: 0.7344\n",
      "Epoch 306/1000\n",
      "563/563 [==============================] - 1s 929us/step - loss: 0.1978 - accuracy: 0.7391 - val_loss: 0.2017 - val_accuracy: 0.7344\n",
      "Epoch 307/1000\n",
      "563/563 [==============================] - 1s 959us/step - loss: 0.1979 - accuracy: 0.7374 - val_loss: 0.2017 - val_accuracy: 0.7345\n",
      "Epoch 308/1000\n",
      "563/563 [==============================] - 1s 999us/step - loss: 0.2012 - accuracy: 0.7282 - val_loss: 0.2016 - val_accuracy: 0.7344\n",
      "Epoch 309/1000\n",
      "563/563 [==============================] - 1s 924us/step - loss: 0.1995 - accuracy: 0.7338 - val_loss: 0.2016 - val_accuracy: 0.7344\n",
      "Epoch 310/1000\n",
      "563/563 [==============================] - 1s 958us/step - loss: 0.2000 - accuracy: 0.7352 - val_loss: 0.2015 - val_accuracy: 0.7345\n",
      "Epoch 311/1000\n",
      "563/563 [==============================] - 1s 979us/step - loss: 0.1990 - accuracy: 0.7346 - val_loss: 0.2015 - val_accuracy: 0.7345\n",
      "Epoch 312/1000\n",
      "563/563 [==============================] - 1s 938us/step - loss: 0.1999 - accuracy: 0.7322 - val_loss: 0.2014 - val_accuracy: 0.7345\n",
      "Epoch 313/1000\n",
      "563/563 [==============================] - 1s 958us/step - loss: 0.2001 - accuracy: 0.7339 - val_loss: 0.2014 - val_accuracy: 0.7344\n",
      "Epoch 314/1000\n",
      "563/563 [==============================] - 1s 972us/step - loss: 0.2004 - accuracy: 0.7293 - val_loss: 0.2014 - val_accuracy: 0.7344\n",
      "Epoch 315/1000\n",
      "563/563 [==============================] - 1s 997us/step - loss: 0.1998 - accuracy: 0.7306 - val_loss: 0.2013 - val_accuracy: 0.7345\n",
      "Epoch 316/1000\n",
      "563/563 [==============================] - 1s 941us/step - loss: 0.1984 - accuracy: 0.7345 - val_loss: 0.2013 - val_accuracy: 0.7344\n",
      "Epoch 317/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1992 - accuracy: 0.7328 - val_loss: 0.2012 - val_accuracy: 0.7345\n",
      "Epoch 318/1000\n",
      "563/563 [==============================] - 1s 965us/step - loss: 0.2007 - accuracy: 0.7316 - val_loss: 0.2012 - val_accuracy: 0.7345\n",
      "Epoch 319/1000\n",
      "563/563 [==============================] - 1s 958us/step - loss: 0.2000 - accuracy: 0.7310 - val_loss: 0.2011 - val_accuracy: 0.7345\n",
      "Epoch 320/1000\n",
      "563/563 [==============================] - 1s 958us/step - loss: 0.1994 - accuracy: 0.7312 - val_loss: 0.2011 - val_accuracy: 0.7345\n",
      "Epoch 321/1000\n",
      "563/563 [==============================] - 1s 913us/step - loss: 0.1985 - accuracy: 0.7353 - val_loss: 0.2010 - val_accuracy: 0.7345\n",
      "Epoch 322/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1967 - accuracy: 0.7422 - val_loss: 0.2010 - val_accuracy: 0.7345\n",
      "Epoch 323/1000\n",
      "563/563 [==============================] - 1s 990us/step - loss: 0.1988 - accuracy: 0.7340 - val_loss: 0.2010 - val_accuracy: 0.7344\n",
      "Epoch 324/1000\n",
      "563/563 [==============================] - 1s 952us/step - loss: 0.1998 - accuracy: 0.7319 - val_loss: 0.2009 - val_accuracy: 0.7345\n",
      "Epoch 325/1000\n",
      "563/563 [==============================] - 1s 924us/step - loss: 0.1980 - accuracy: 0.7314 - val_loss: 0.2009 - val_accuracy: 0.7345\n",
      "Epoch 326/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2006 - accuracy: 0.7295 - val_loss: 0.2008 - val_accuracy: 0.7345\n",
      "Epoch 327/1000\n",
      "563/563 [==============================] - 1s 922us/step - loss: 0.1975 - accuracy: 0.7364 - val_loss: 0.2008 - val_accuracy: 0.7345\n",
      "Epoch 328/1000\n",
      "563/563 [==============================] - 1s 931us/step - loss: 0.1976 - accuracy: 0.7369 - val_loss: 0.2007 - val_accuracy: 0.7344\n",
      "Epoch 329/1000\n",
      "563/563 [==============================] - 1s 941us/step - loss: 0.1984 - accuracy: 0.7343 - val_loss: 0.2007 - val_accuracy: 0.7345\n",
      "Epoch 330/1000\n",
      "563/563 [==============================] - 1s 974us/step - loss: 0.1972 - accuracy: 0.7356 - val_loss: 0.2006 - val_accuracy: 0.7344\n",
      "Epoch 331/1000\n",
      "563/563 [==============================] - 1s 968us/step - loss: 0.1984 - accuracy: 0.7332 - val_loss: 0.2006 - val_accuracy: 0.7344\n",
      "Epoch 332/1000\n",
      "563/563 [==============================] - 1s 931us/step - loss: 0.2002 - accuracy: 0.7293 - val_loss: 0.2006 - val_accuracy: 0.7345\n",
      "Epoch 333/1000\n",
      "563/563 [==============================] - 1s 961us/step - loss: 0.1984 - accuracy: 0.7354 - val_loss: 0.2005 - val_accuracy: 0.7345\n",
      "Epoch 334/1000\n",
      "563/563 [==============================] - 1s 941us/step - loss: 0.1989 - accuracy: 0.7316 - val_loss: 0.2005 - val_accuracy: 0.7344\n",
      "Epoch 335/1000\n",
      "563/563 [==============================] - 1s 931us/step - loss: 0.1979 - accuracy: 0.7350 - val_loss: 0.2004 - val_accuracy: 0.7345\n",
      "Epoch 336/1000\n",
      "563/563 [==============================] - 1s 917us/step - loss: 0.1980 - accuracy: 0.7352 - val_loss: 0.2004 - val_accuracy: 0.7348\n",
      "Epoch 337/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1986 - accuracy: 0.7327 - val_loss: 0.2003 - val_accuracy: 0.7348\n",
      "Epoch 338/1000\n",
      "563/563 [==============================] - 1s 949us/step - loss: 0.1983 - accuracy: 0.7385 - val_loss: 0.2003 - val_accuracy: 0.7346\n",
      "Epoch 339/1000\n",
      "563/563 [==============================] - 1s 929us/step - loss: 0.1980 - accuracy: 0.7341 - val_loss: 0.2003 - val_accuracy: 0.7346\n",
      "Epoch 340/1000\n",
      "563/563 [==============================] - 1s 933us/step - loss: 0.1996 - accuracy: 0.7314 - val_loss: 0.2002 - val_accuracy: 0.7348\n",
      "Epoch 341/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1971 - accuracy: 0.7359 - val_loss: 0.2002 - val_accuracy: 0.7348\n",
      "Epoch 342/1000\n",
      "563/563 [==============================] - 1s 952us/step - loss: 0.1987 - accuracy: 0.7314 - val_loss: 0.2001 - val_accuracy: 0.7348\n",
      "Epoch 343/1000\n",
      "563/563 [==============================] - 1s 949us/step - loss: 0.1981 - accuracy: 0.7314 - val_loss: 0.2001 - val_accuracy: 0.7346\n",
      "Epoch 344/1000\n",
      "563/563 [==============================] - 1s 968us/step - loss: 0.1988 - accuracy: 0.7342 - val_loss: 0.2001 - val_accuracy: 0.7346\n",
      "Epoch 345/1000\n",
      "563/563 [==============================] - 1s 990us/step - loss: 0.1975 - accuracy: 0.7340 - val_loss: 0.2000 - val_accuracy: 0.7346\n",
      "Epoch 346/1000\n",
      "563/563 [==============================] - 1s 929us/step - loss: 0.1980 - accuracy: 0.7384 - val_loss: 0.2000 - val_accuracy: 0.7346\n",
      "Epoch 347/1000\n",
      "563/563 [==============================] - 1s 917us/step - loss: 0.1986 - accuracy: 0.7342 - val_loss: 0.2000 - val_accuracy: 0.7346\n",
      "Epoch 348/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1974 - accuracy: 0.7337 - val_loss: 0.1999 - val_accuracy: 0.7346\n",
      "Epoch 349/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1974 - accuracy: 0.7352 - val_loss: 0.1999 - val_accuracy: 0.7346\n",
      "Epoch 350/1000\n",
      "563/563 [==============================] - 1s 966us/step - loss: 0.1966 - accuracy: 0.7342 - val_loss: 0.1998 - val_accuracy: 0.7346\n",
      "Epoch 351/1000\n",
      "563/563 [==============================] - 1s 949us/step - loss: 0.1996 - accuracy: 0.7278 - val_loss: 0.1998 - val_accuracy: 0.7346\n",
      "Epoch 352/1000\n",
      "563/563 [==============================] - 1s 981us/step - loss: 0.1966 - accuracy: 0.7337 - val_loss: 0.1998 - val_accuracy: 0.7346\n",
      "Epoch 353/1000\n",
      "563/563 [==============================] - 1s 936us/step - loss: 0.1981 - accuracy: 0.7343 - val_loss: 0.1997 - val_accuracy: 0.7346\n",
      "Epoch 354/1000\n",
      "563/563 [==============================] - 1s 930us/step - loss: 0.1997 - accuracy: 0.7288 - val_loss: 0.1997 - val_accuracy: 0.7346\n",
      "Epoch 355/1000\n",
      "563/563 [==============================] - 1s 974us/step - loss: 0.1960 - accuracy: 0.7397 - val_loss: 0.1996 - val_accuracy: 0.7346\n",
      "Epoch 356/1000\n",
      "563/563 [==============================] - 1s 979us/step - loss: 0.1961 - accuracy: 0.7343 - val_loss: 0.1996 - val_accuracy: 0.7346\n",
      "Epoch 357/1000\n",
      "563/563 [==============================] - 1s 952us/step - loss: 0.1981 - accuracy: 0.7327 - val_loss: 0.1996 - val_accuracy: 0.7344\n",
      "Epoch 358/1000\n",
      "563/563 [==============================] - 1s 918us/step - loss: 0.1972 - accuracy: 0.7303 - val_loss: 0.1995 - val_accuracy: 0.7344\n",
      "Epoch 359/1000\n",
      "563/563 [==============================] - 1s 972us/step - loss: 0.1974 - accuracy: 0.7326 - val_loss: 0.1995 - val_accuracy: 0.7344\n",
      "Epoch 360/1000\n",
      "563/563 [==============================] - 1s 978us/step - loss: 0.1973 - accuracy: 0.7341 - val_loss: 0.1995 - val_accuracy: 0.7344\n",
      "Epoch 361/1000\n",
      "563/563 [==============================] - 1s 936us/step - loss: 0.1977 - accuracy: 0.7321 - val_loss: 0.1994 - val_accuracy: 0.7344\n",
      "Epoch 362/1000\n",
      "563/563 [==============================] - 1s 949us/step - loss: 0.1971 - accuracy: 0.7362 - val_loss: 0.1994 - val_accuracy: 0.7344\n",
      "Epoch 363/1000\n",
      "563/563 [==============================] - 1s 991us/step - loss: 0.1976 - accuracy: 0.7330 - val_loss: 0.1993 - val_accuracy: 0.7337\n",
      "Epoch 364/1000\n",
      "563/563 [==============================] - 1s 925us/step - loss: 0.1967 - accuracy: 0.7341 - val_loss: 0.1993 - val_accuracy: 0.7344\n",
      "Epoch 365/1000\n",
      "563/563 [==============================] - 1s 938us/step - loss: 0.1963 - accuracy: 0.7361 - val_loss: 0.1993 - val_accuracy: 0.7337\n",
      "Epoch 366/1000\n",
      "563/563 [==============================] - 1s 936us/step - loss: 0.1970 - accuracy: 0.7345 - val_loss: 0.1992 - val_accuracy: 0.7337\n",
      "Epoch 367/1000\n",
      "563/563 [==============================] - 1s 961us/step - loss: 0.1974 - accuracy: 0.7337 - val_loss: 0.1992 - val_accuracy: 0.7337\n",
      "Epoch 368/1000\n",
      "563/563 [==============================] - 1s 965us/step - loss: 0.1957 - accuracy: 0.7376 - val_loss: 0.1992 - val_accuracy: 0.7337\n",
      "Epoch 369/1000\n",
      "563/563 [==============================] - 1s 956us/step - loss: 0.1979 - accuracy: 0.7288 - val_loss: 0.1991 - val_accuracy: 0.7337\n",
      "Epoch 370/1000\n",
      "563/563 [==============================] - 1s 977us/step - loss: 0.1994 - accuracy: 0.7265 - val_loss: 0.1991 - val_accuracy: 0.7337\n",
      "Epoch 371/1000\n",
      "563/563 [==============================] - 1s 958us/step - loss: 0.1961 - accuracy: 0.7334 - val_loss: 0.1991 - val_accuracy: 0.7337\n",
      "Epoch 372/1000\n",
      "563/563 [==============================] - 1s 952us/step - loss: 0.1974 - accuracy: 0.7331 - val_loss: 0.1990 - val_accuracy: 0.7337\n",
      "Epoch 373/1000\n",
      "563/563 [==============================] - 1s 926us/step - loss: 0.1972 - accuracy: 0.7361 - val_loss: 0.1990 - val_accuracy: 0.7337\n",
      "Epoch 374/1000\n",
      "563/563 [==============================] - 1s 995us/step - loss: 0.1974 - accuracy: 0.7343 - val_loss: 0.1990 - val_accuracy: 0.7337\n",
      "Epoch 375/1000\n",
      "563/563 [==============================] - 1s 934us/step - loss: 0.1979 - accuracy: 0.7339 - val_loss: 0.1989 - val_accuracy: 0.7337\n",
      "Epoch 376/1000\n",
      "563/563 [==============================] - 1s 918us/step - loss: 0.1972 - accuracy: 0.7340 - val_loss: 0.1989 - val_accuracy: 0.7337\n",
      "Epoch 377/1000\n",
      "563/563 [==============================] - 1s 929us/step - loss: 0.1963 - accuracy: 0.7345 - val_loss: 0.1989 - val_accuracy: 0.7337\n",
      "Epoch 378/1000\n",
      "563/563 [==============================] - 1s 988us/step - loss: 0.1982 - accuracy: 0.7304 - val_loss: 0.1988 - val_accuracy: 0.7337\n",
      "Epoch 379/1000\n",
      "563/563 [==============================] - 1s 920us/step - loss: 0.1951 - accuracy: 0.7360 - val_loss: 0.1988 - val_accuracy: 0.7337\n",
      "Epoch 380/1000\n",
      "563/563 [==============================] - 1s 950us/step - loss: 0.1971 - accuracy: 0.7301 - val_loss: 0.1987 - val_accuracy: 0.7337\n",
      "Epoch 381/1000\n",
      "563/563 [==============================] - 1s 958us/step - loss: 0.1970 - accuracy: 0.7336 - val_loss: 0.1987 - val_accuracy: 0.7337\n",
      "Epoch 382/1000\n",
      "563/563 [==============================] - 1s 998us/step - loss: 0.1959 - accuracy: 0.7346 - val_loss: 0.1987 - val_accuracy: 0.7340\n",
      "Epoch 383/1000\n",
      "563/563 [==============================] - 1s 949us/step - loss: 0.1963 - accuracy: 0.7322 - val_loss: 0.1986 - val_accuracy: 0.7340\n",
      "Epoch 384/1000\n",
      "563/563 [==============================] - 1s 931us/step - loss: 0.1940 - accuracy: 0.7377 - val_loss: 0.1986 - val_accuracy: 0.7340\n",
      "Epoch 385/1000\n",
      "563/563 [==============================] - 1s 965us/step - loss: 0.1959 - accuracy: 0.7357 - val_loss: 0.1986 - val_accuracy: 0.7340\n",
      "Epoch 386/1000\n",
      "563/563 [==============================] - 1s 952us/step - loss: 0.1954 - accuracy: 0.7388 - val_loss: 0.1986 - val_accuracy: 0.7337\n",
      "Epoch 387/1000\n",
      "563/563 [==============================] - 1s 931us/step - loss: 0.1972 - accuracy: 0.7296 - val_loss: 0.1985 - val_accuracy: 0.7337\n",
      "Epoch 388/1000\n",
      "563/563 [==============================] - 1s 918us/step - loss: 0.1968 - accuracy: 0.7335 - val_loss: 0.1985 - val_accuracy: 0.7340\n",
      "Epoch 389/1000\n",
      "563/563 [==============================] - 1s 990us/step - loss: 0.1957 - accuracy: 0.7373 - val_loss: 0.1985 - val_accuracy: 0.7339\n",
      "Epoch 390/1000\n",
      "563/563 [==============================] - 1s 934us/step - loss: 0.1976 - accuracy: 0.7311 - val_loss: 0.1984 - val_accuracy: 0.7337\n",
      "Epoch 391/1000\n",
      "563/563 [==============================] - 1s 929us/step - loss: 0.1967 - accuracy: 0.7298 - val_loss: 0.1984 - val_accuracy: 0.7340\n",
      "Epoch 392/1000\n",
      "563/563 [==============================] - 1s 965us/step - loss: 0.1957 - accuracy: 0.7367 - val_loss: 0.1984 - val_accuracy: 0.7339\n",
      "Epoch 393/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1957 - accuracy: 0.7355 - val_loss: 0.1983 - val_accuracy: 0.7339\n",
      "Epoch 394/1000\n",
      "563/563 [==============================] - 1s 963us/step - loss: 0.1973 - accuracy: 0.7314 - val_loss: 0.1983 - val_accuracy: 0.7340\n",
      "Epoch 395/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1976 - accuracy: 0.7319 - val_loss: 0.1983 - val_accuracy: 0.7340\n",
      "Epoch 396/1000\n",
      "563/563 [==============================] - 1s 993us/step - loss: 0.1950 - accuracy: 0.7367 - val_loss: 0.1982 - val_accuracy: 0.7339\n",
      "Epoch 397/1000\n",
      "563/563 [==============================] - 1s 977us/step - loss: 0.1955 - accuracy: 0.7346 - val_loss: 0.1982 - val_accuracy: 0.7340\n",
      "Epoch 398/1000\n",
      "563/563 [==============================] - 1s 934us/step - loss: 0.1960 - accuracy: 0.7335 - val_loss: 0.1982 - val_accuracy: 0.7340\n",
      "Epoch 399/1000\n",
      "563/563 [==============================] - 1s 936us/step - loss: 0.1959 - accuracy: 0.7375 - val_loss: 0.1981 - val_accuracy: 0.7340\n",
      "Epoch 400/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1960 - accuracy: 0.7352 - val_loss: 0.1981 - val_accuracy: 0.7340\n",
      "Epoch 401/1000\n",
      "563/563 [==============================] - 1s 952us/step - loss: 0.1949 - accuracy: 0.7363 - val_loss: 0.1981 - val_accuracy: 0.7344\n",
      "Epoch 402/1000\n",
      "563/563 [==============================] - 1s 956us/step - loss: 0.1965 - accuracy: 0.7329 - val_loss: 0.1980 - val_accuracy: 0.7340\n",
      "Epoch 403/1000\n",
      "563/563 [==============================] - 1s 958us/step - loss: 0.1960 - accuracy: 0.7353 - val_loss: 0.1980 - val_accuracy: 0.7344\n",
      "Epoch 404/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1971 - accuracy: 0.7341 - val_loss: 0.1980 - val_accuracy: 0.7344\n",
      "Epoch 405/1000\n",
      "563/563 [==============================] - 1s 954us/step - loss: 0.1963 - accuracy: 0.7329 - val_loss: 0.1979 - val_accuracy: 0.7344\n",
      "Epoch 406/1000\n",
      "563/563 [==============================] - 1s 941us/step - loss: 0.1985 - accuracy: 0.7265 - val_loss: 0.1979 - val_accuracy: 0.7344\n",
      "Epoch 407/1000\n",
      "563/563 [==============================] - 1s 954us/step - loss: 0.1987 - accuracy: 0.7272 - val_loss: 0.1979 - val_accuracy: 0.7344\n",
      "Epoch 408/1000\n",
      "563/563 [==============================] - 1s 954us/step - loss: 0.1957 - accuracy: 0.7337 - val_loss: 0.1979 - val_accuracy: 0.7344\n",
      "Epoch 409/1000\n",
      "563/563 [==============================] - 1s 918us/step - loss: 0.1958 - accuracy: 0.7355 - val_loss: 0.1978 - val_accuracy: 0.7344\n",
      "Epoch 410/1000\n",
      "563/563 [==============================] - 1s 949us/step - loss: 0.1969 - accuracy: 0.7327 - val_loss: 0.1978 - val_accuracy: 0.7344\n",
      "Epoch 411/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1961 - accuracy: 0.7321 - val_loss: 0.1978 - val_accuracy: 0.7344\n",
      "Epoch 412/1000\n",
      "563/563 [==============================] - 1s 920us/step - loss: 0.1973 - accuracy: 0.7288 - val_loss: 0.1978 - val_accuracy: 0.7344\n",
      "Epoch 413/1000\n",
      "563/563 [==============================] - 1s 938us/step - loss: 0.1966 - accuracy: 0.7317 - val_loss: 0.1977 - val_accuracy: 0.7344\n",
      "Epoch 414/1000\n",
      "563/563 [==============================] - 1s 945us/step - loss: 0.1955 - accuracy: 0.7349 - val_loss: 0.1977 - val_accuracy: 0.7344\n",
      "Epoch 415/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1953 - accuracy: 0.7360 - val_loss: 0.1977 - val_accuracy: 0.7344\n",
      "Epoch 416/1000\n",
      "563/563 [==============================] - 1s 950us/step - loss: 0.1949 - accuracy: 0.7356 - val_loss: 0.1976 - val_accuracy: 0.7344\n",
      "Epoch 417/1000\n",
      "563/563 [==============================] - 1s 938us/step - loss: 0.1943 - accuracy: 0.7399 - val_loss: 0.1976 - val_accuracy: 0.7344\n",
      "Epoch 418/1000\n",
      "563/563 [==============================] - 1s 993us/step - loss: 0.1953 - accuracy: 0.7360 - val_loss: 0.1976 - val_accuracy: 0.7344\n",
      "Epoch 419/1000\n",
      "563/563 [==============================] - 1s 949us/step - loss: 0.1973 - accuracy: 0.7305 - val_loss: 0.1975 - val_accuracy: 0.7343\n",
      "Epoch 420/1000\n",
      "563/563 [==============================] - 1s 943us/step - loss: 0.1962 - accuracy: 0.7306 - val_loss: 0.1975 - val_accuracy: 0.7343\n",
      "Epoch 421/1000\n",
      "563/563 [==============================] - 1s 909us/step - loss: 0.1950 - accuracy: 0.7352 - val_loss: 0.1975 - val_accuracy: 0.7343\n",
      "Epoch 422/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1946 - accuracy: 0.7367 - val_loss: 0.1975 - val_accuracy: 0.7344\n",
      "Epoch 423/1000\n",
      "563/563 [==============================] - 1s 936us/step - loss: 0.1938 - accuracy: 0.7381 - val_loss: 0.1974 - val_accuracy: 0.7344\n",
      "Epoch 424/1000\n",
      "563/563 [==============================] - 1s 918us/step - loss: 0.1964 - accuracy: 0.7321 - val_loss: 0.1974 - val_accuracy: 0.7343\n",
      "Epoch 425/1000\n",
      "563/563 [==============================] - 1s 981us/step - loss: 0.1937 - accuracy: 0.7379 - val_loss: 0.1974 - val_accuracy: 0.7343\n",
      "Epoch 426/1000\n",
      "563/563 [==============================] - 1s 981us/step - loss: 0.1965 - accuracy: 0.7302 - val_loss: 0.1973 - val_accuracy: 0.7344\n",
      "Epoch 427/1000\n",
      "563/563 [==============================] - 1s 952us/step - loss: 0.1946 - accuracy: 0.7332 - val_loss: 0.1973 - val_accuracy: 0.7344\n",
      "Epoch 428/1000\n",
      "563/563 [==============================] - 1s 993us/step - loss: 0.1963 - accuracy: 0.7322 - val_loss: 0.1973 - val_accuracy: 0.7344\n",
      "Epoch 429/1000\n",
      "563/563 [==============================] - 1s 979us/step - loss: 0.1965 - accuracy: 0.7346 - val_loss: 0.1973 - val_accuracy: 0.7341\n",
      "Epoch 430/1000\n",
      "563/563 [==============================] - 1s 958us/step - loss: 0.1940 - accuracy: 0.7371 - val_loss: 0.1972 - val_accuracy: 0.7343\n",
      "Epoch 431/1000\n",
      "563/563 [==============================] - 1s 913us/step - loss: 0.1946 - accuracy: 0.7353 - val_loss: 0.1972 - val_accuracy: 0.7341\n",
      "Epoch 432/1000\n",
      "563/563 [==============================] - 1s 922us/step - loss: 0.1952 - accuracy: 0.7356 - val_loss: 0.1972 - val_accuracy: 0.7343\n",
      "Epoch 433/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1928 - accuracy: 0.7399 - val_loss: 0.1972 - val_accuracy: 0.7343\n",
      "Epoch 434/1000\n",
      "563/563 [==============================] - 1s 956us/step - loss: 0.1928 - accuracy: 0.7412 - val_loss: 0.1971 - val_accuracy: 0.7343\n",
      "Epoch 435/1000\n",
      "563/563 [==============================] - 1s 954us/step - loss: 0.1955 - accuracy: 0.7338 - val_loss: 0.1971 - val_accuracy: 0.7341\n",
      "Epoch 436/1000\n",
      "563/563 [==============================] - 1s 925us/step - loss: 0.1955 - accuracy: 0.7335 - val_loss: 0.1971 - val_accuracy: 0.7341\n",
      "Epoch 437/1000\n",
      "563/563 [==============================] - 1s 974us/step - loss: 0.1952 - accuracy: 0.7343 - val_loss: 0.1971 - val_accuracy: 0.7343\n",
      "Epoch 438/1000\n",
      "563/563 [==============================] - 1s 933us/step - loss: 0.1954 - accuracy: 0.7324 - val_loss: 0.1970 - val_accuracy: 0.7341\n",
      "Epoch 439/1000\n",
      "563/563 [==============================] - 1s 917us/step - loss: 0.1951 - accuracy: 0.7325 - val_loss: 0.1970 - val_accuracy: 0.7341\n",
      "Epoch 440/1000\n",
      "563/563 [==============================] - 1s 965us/step - loss: 0.1940 - accuracy: 0.7355 - val_loss: 0.1970 - val_accuracy: 0.7341\n",
      "Epoch 441/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1962 - accuracy: 0.7315 - val_loss: 0.1969 - val_accuracy: 0.7340\n",
      "Epoch 442/1000\n",
      "563/563 [==============================] - 1s 972us/step - loss: 0.1938 - accuracy: 0.7375 - val_loss: 0.1969 - val_accuracy: 0.7343\n",
      "Epoch 443/1000\n",
      "563/563 [==============================] - 1s 977us/step - loss: 0.1943 - accuracy: 0.7362 - val_loss: 0.1969 - val_accuracy: 0.7343\n",
      "Epoch 444/1000\n",
      "563/563 [==============================] - 1s 990us/step - loss: 0.1940 - accuracy: 0.7341 - val_loss: 0.1969 - val_accuracy: 0.7339\n",
      "Epoch 445/1000\n",
      "563/563 [==============================] - 1s 934us/step - loss: 0.1953 - accuracy: 0.7356 - val_loss: 0.1968 - val_accuracy: 0.7343\n",
      "Epoch 446/1000\n",
      "563/563 [==============================] - 1s 934us/step - loss: 0.1961 - accuracy: 0.7333 - val_loss: 0.1968 - val_accuracy: 0.7343\n",
      "Epoch 447/1000\n",
      "563/563 [==============================] - 1s 931us/step - loss: 0.1933 - accuracy: 0.7383 - val_loss: 0.1968 - val_accuracy: 0.7343\n",
      "Epoch 448/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1952 - accuracy: 0.7342 - val_loss: 0.1968 - val_accuracy: 0.7341\n",
      "Epoch 449/1000\n",
      "563/563 [==============================] - 1s 915us/step - loss: 0.1936 - accuracy: 0.7393 - val_loss: 0.1967 - val_accuracy: 0.7343\n",
      "Epoch 450/1000\n",
      "563/563 [==============================] - 1s 922us/step - loss: 0.1937 - accuracy: 0.7376 - val_loss: 0.1967 - val_accuracy: 0.7341\n",
      "Epoch 451/1000\n",
      "563/563 [==============================] - 1s 934us/step - loss: 0.1952 - accuracy: 0.7344 - val_loss: 0.1967 - val_accuracy: 0.7341\n",
      "Epoch 452/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1938 - accuracy: 0.7396 - val_loss: 0.1967 - val_accuracy: 0.7341\n",
      "Epoch 453/1000\n",
      "563/563 [==============================] - 1s 963us/step - loss: 0.1926 - accuracy: 0.7400 - val_loss: 0.1966 - val_accuracy: 0.7341\n",
      "Epoch 454/1000\n",
      "563/563 [==============================] - 1s 974us/step - loss: 0.1943 - accuracy: 0.7372 - val_loss: 0.1966 - val_accuracy: 0.7352\n",
      "Epoch 455/1000\n",
      "563/563 [==============================] - 1s 965us/step - loss: 0.1951 - accuracy: 0.7361 - val_loss: 0.1966 - val_accuracy: 0.7350\n",
      "Epoch 456/1000\n",
      "563/563 [==============================] - 1s 943us/step - loss: 0.1939 - accuracy: 0.7377 - val_loss: 0.1966 - val_accuracy: 0.7350\n",
      "Epoch 457/1000\n",
      "563/563 [==============================] - 1s 954us/step - loss: 0.1945 - accuracy: 0.7353 - val_loss: 0.1965 - val_accuracy: 0.7352\n",
      "Epoch 458/1000\n",
      "563/563 [==============================] - 1s 933us/step - loss: 0.1962 - accuracy: 0.7325 - val_loss: 0.1965 - val_accuracy: 0.7353\n",
      "Epoch 459/1000\n",
      "563/563 [==============================] - 1s 977us/step - loss: 0.1942 - accuracy: 0.7362 - val_loss: 0.1965 - val_accuracy: 0.7353\n",
      "Epoch 460/1000\n",
      "563/563 [==============================] - 1s 934us/step - loss: 0.1940 - accuracy: 0.7356 - val_loss: 0.1965 - val_accuracy: 0.7353\n",
      "Epoch 461/1000\n",
      "563/563 [==============================] - 1s 991us/step - loss: 0.1939 - accuracy: 0.7349 - val_loss: 0.1964 - val_accuracy: 0.7352\n",
      "Epoch 462/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1964 - accuracy: 0.7294 - val_loss: 0.1964 - val_accuracy: 0.7353\n",
      "Epoch 463/1000\n",
      "563/563 [==============================] - 1s 979us/step - loss: 0.1952 - accuracy: 0.7353 - val_loss: 0.1964 - val_accuracy: 0.7352\n",
      "Epoch 464/1000\n",
      "563/563 [==============================] - 1s 934us/step - loss: 0.1921 - accuracy: 0.7411 - val_loss: 0.1964 - val_accuracy: 0.7352\n",
      "Epoch 465/1000\n",
      "563/563 [==============================] - 1s 931us/step - loss: 0.1938 - accuracy: 0.7354 - val_loss: 0.1963 - val_accuracy: 0.7353\n",
      "Epoch 466/1000\n",
      "563/563 [==============================] - 1s 975us/step - loss: 0.1959 - accuracy: 0.7290 - val_loss: 0.1963 - val_accuracy: 0.7352\n",
      "Epoch 467/1000\n",
      "563/563 [==============================] - 1s 938us/step - loss: 0.1951 - accuracy: 0.7327 - val_loss: 0.1963 - val_accuracy: 0.7352\n",
      "Epoch 468/1000\n",
      "563/563 [==============================] - 1s 927us/step - loss: 0.1959 - accuracy: 0.7307 - val_loss: 0.1963 - val_accuracy: 0.7352\n",
      "Epoch 469/1000\n",
      "563/563 [==============================] - 1s 929us/step - loss: 0.1940 - accuracy: 0.7347 - val_loss: 0.1963 - val_accuracy: 0.7352\n",
      "Epoch 470/1000\n",
      "563/563 [==============================] - 1s 988us/step - loss: 0.1941 - accuracy: 0.7370 - val_loss: 0.1962 - val_accuracy: 0.7352\n",
      "Epoch 471/1000\n",
      "563/563 [==============================] - 1s 945us/step - loss: 0.1962 - accuracy: 0.7306 - val_loss: 0.1962 - val_accuracy: 0.7352\n",
      "Epoch 472/1000\n",
      "563/563 [==============================] - 1s 947us/step - loss: 0.1939 - accuracy: 0.7344 - val_loss: 0.1962 - val_accuracy: 0.7352\n",
      "Epoch 473/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1954 - accuracy: 0.7284 - val_loss: 0.1962 - val_accuracy: 0.7352\n",
      "Epoch 474/1000\n",
      "563/563 [==============================] - 1s 966us/step - loss: 0.1923 - accuracy: 0.7383 - val_loss: 0.1961 - val_accuracy: 0.7352\n",
      "Epoch 475/1000\n",
      "563/563 [==============================] - 1s 925us/step - loss: 0.1935 - accuracy: 0.7349 - val_loss: 0.1961 - val_accuracy: 0.7352\n",
      "Epoch 476/1000\n",
      "563/563 [==============================] - 1s 949us/step - loss: 0.1940 - accuracy: 0.7333 - val_loss: 0.1961 - val_accuracy: 0.7352\n",
      "Epoch 477/1000\n",
      "563/563 [==============================] - 1s 982us/step - loss: 0.1937 - accuracy: 0.7370 - val_loss: 0.1961 - val_accuracy: 0.7352\n",
      "Epoch 478/1000\n",
      "563/563 [==============================] - 1s 956us/step - loss: 0.1939 - accuracy: 0.7362 - val_loss: 0.1961 - val_accuracy: 0.7352\n",
      "Epoch 479/1000\n",
      "563/563 [==============================] - 1s 913us/step - loss: 0.1941 - accuracy: 0.7354 - val_loss: 0.1960 - val_accuracy: 0.7352\n",
      "Epoch 480/1000\n",
      "563/563 [==============================] - 1s 958us/step - loss: 0.1952 - accuracy: 0.7303 - val_loss: 0.1960 - val_accuracy: 0.7352\n",
      "Epoch 481/1000\n",
      "563/563 [==============================] - 1s 988us/step - loss: 0.1938 - accuracy: 0.7329 - val_loss: 0.1960 - val_accuracy: 0.7352\n",
      "Epoch 482/1000\n",
      "563/563 [==============================] - 1s 934us/step - loss: 0.1935 - accuracy: 0.7362 - val_loss: 0.1960 - val_accuracy: 0.7352\n",
      "Epoch 483/1000\n",
      "563/563 [==============================] - 1s 960us/step - loss: 0.1921 - accuracy: 0.7373 - val_loss: 0.1959 - val_accuracy: 0.7352\n",
      "Epoch 484/1000\n",
      "563/563 [==============================] - 1s 949us/step - loss: 0.1932 - accuracy: 0.7406 - val_loss: 0.1959 - val_accuracy: 0.7352\n",
      "Epoch 485/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1929 - accuracy: 0.7383 - val_loss: 0.1959 - val_accuracy: 0.7352\n",
      "Epoch 486/1000\n",
      "563/563 [==============================] - 1s 931us/step - loss: 0.1926 - accuracy: 0.7375 - val_loss: 0.1959 - val_accuracy: 0.7353\n",
      "Epoch 487/1000\n",
      "563/563 [==============================] - 1s 936us/step - loss: 0.1932 - accuracy: 0.7376 - val_loss: 0.1958 - val_accuracy: 0.7353\n",
      "Epoch 488/1000\n",
      "563/563 [==============================] - 1s 945us/step - loss: 0.1951 - accuracy: 0.7325 - val_loss: 0.1958 - val_accuracy: 0.7353\n",
      "Epoch 489/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1924 - accuracy: 0.7419 - val_loss: 0.1958 - val_accuracy: 0.7352\n",
      "Epoch 490/1000\n",
      "563/563 [==============================] - 1s 915us/step - loss: 0.1951 - accuracy: 0.7290 - val_loss: 0.1958 - val_accuracy: 0.7352\n",
      "Epoch 491/1000\n",
      "563/563 [==============================] - 1s 922us/step - loss: 0.1940 - accuracy: 0.7348 - val_loss: 0.1958 - val_accuracy: 0.7350\n",
      "Epoch 492/1000\n",
      "563/563 [==============================] - 1s 958us/step - loss: 0.1956 - accuracy: 0.7281 - val_loss: 0.1957 - val_accuracy: 0.7350\n",
      "Epoch 493/1000\n",
      "563/563 [==============================] - 1s 947us/step - loss: 0.1932 - accuracy: 0.7371 - val_loss: 0.1957 - val_accuracy: 0.7350\n",
      "Epoch 494/1000\n",
      "563/563 [==============================] - 1s 924us/step - loss: 0.1947 - accuracy: 0.7324 - val_loss: 0.1957 - val_accuracy: 0.7352\n",
      "Epoch 495/1000\n",
      "563/563 [==============================] - 1s 972us/step - loss: 0.1956 - accuracy: 0.7294 - val_loss: 0.1957 - val_accuracy: 0.7350\n",
      "Epoch 496/1000\n",
      "563/563 [==============================] - 1s 995us/step - loss: 0.1934 - accuracy: 0.7347 - val_loss: 0.1957 - val_accuracy: 0.7350\n",
      "Epoch 497/1000\n",
      "563/563 [==============================] - 1s 998us/step - loss: 0.1941 - accuracy: 0.7327 - val_loss: 0.1956 - val_accuracy: 0.7350\n",
      "Epoch 498/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1941 - accuracy: 0.7314 - val_loss: 0.1956 - val_accuracy: 0.7352\n",
      "Epoch 499/1000\n",
      "563/563 [==============================] - 1s 958us/step - loss: 0.1927 - accuracy: 0.7380 - val_loss: 0.1956 - val_accuracy: 0.7350\n",
      "Epoch 500/1000\n",
      "563/563 [==============================] - 1s 986us/step - loss: 0.1923 - accuracy: 0.7384 - val_loss: 0.1956 - val_accuracy: 0.7350\n",
      "Epoch 501/1000\n",
      "563/563 [==============================] - 1s 940us/step - loss: 0.1948 - accuracy: 0.7315 - val_loss: 0.1956 - val_accuracy: 0.7352\n",
      "Epoch 502/1000\n",
      "563/563 [==============================] - 1s 920us/step - loss: 0.1915 - accuracy: 0.7408 - val_loss: 0.1955 - val_accuracy: 0.7352\n",
      "Epoch 503/1000\n",
      "563/563 [==============================] - 1s 972us/step - loss: 0.1933 - accuracy: 0.7342 - val_loss: 0.1955 - val_accuracy: 0.7352\n",
      "Epoch 504/1000\n",
      "563/563 [==============================] - 1s 956us/step - loss: 0.1942 - accuracy: 0.7346 - val_loss: 0.1955 - val_accuracy: 0.7352\n",
      "Epoch 505/1000\n",
      "563/563 [==============================] - 1s 945us/step - loss: 0.1954 - accuracy: 0.7304 - val_loss: 0.1955 - val_accuracy: 0.7352\n",
      "Epoch 506/1000\n",
      "563/563 [==============================] - 1s 934us/step - loss: 0.1930 - accuracy: 0.7369 - val_loss: 0.1955 - val_accuracy: 0.7352\n",
      "Epoch 507/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1933 - accuracy: 0.7365 - val_loss: 0.1954 - val_accuracy: 0.7352\n",
      "Epoch 508/1000\n",
      "563/563 [==============================] - 1s 968us/step - loss: 0.1934 - accuracy: 0.7378 - val_loss: 0.1954 - val_accuracy: 0.7352\n",
      "Epoch 509/1000\n",
      "563/563 [==============================] - 1s 917us/step - loss: 0.1938 - accuracy: 0.7336 - val_loss: 0.1954 - val_accuracy: 0.7352\n",
      "Epoch 510/1000\n",
      "563/563 [==============================] - 1s 924us/step - loss: 0.1949 - accuracy: 0.7351 - val_loss: 0.1954 - val_accuracy: 0.7352\n",
      "Epoch 511/1000\n",
      "563/563 [==============================] - 1s 981us/step - loss: 0.1942 - accuracy: 0.7294 - val_loss: 0.1954 - val_accuracy: 0.7352\n",
      "Epoch 512/1000\n",
      "563/563 [==============================] - 1s 965us/step - loss: 0.1927 - accuracy: 0.7372 - val_loss: 0.1953 - val_accuracy: 0.7352\n",
      "Epoch 513/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1942 - accuracy: 0.7333 - val_loss: 0.1953 - val_accuracy: 0.7352\n",
      "Epoch 514/1000\n",
      "563/563 [==============================] - 1s 959us/step - loss: 0.1929 - accuracy: 0.7343 - val_loss: 0.1953 - val_accuracy: 0.7352\n",
      "Epoch 515/1000\n",
      "563/563 [==============================] - 1s 974us/step - loss: 0.1918 - accuracy: 0.7376 - val_loss: 0.1953 - val_accuracy: 0.7352\n",
      "Epoch 516/1000\n",
      "563/563 [==============================] - 1s 924us/step - loss: 0.1944 - accuracy: 0.7292 - val_loss: 0.1953 - val_accuracy: 0.7352\n",
      "Epoch 517/1000\n",
      "563/563 [==============================] - 1s 913us/step - loss: 0.1926 - accuracy: 0.7347 - val_loss: 0.1952 - val_accuracy: 0.7352\n",
      "Epoch 518/1000\n",
      "563/563 [==============================] - 1s 993us/step - loss: 0.1935 - accuracy: 0.7345 - val_loss: 0.1952 - val_accuracy: 0.7353\n",
      "Epoch 519/1000\n",
      "563/563 [==============================] - 1s 941us/step - loss: 0.1942 - accuracy: 0.7299 - val_loss: 0.1952 - val_accuracy: 0.7353\n",
      "Epoch 520/1000\n",
      "563/563 [==============================] - 1s 934us/step - loss: 0.1931 - accuracy: 0.7324 - val_loss: 0.1952 - val_accuracy: 0.7353\n",
      "Epoch 521/1000\n",
      "563/563 [==============================] - 1s 939us/step - loss: 0.1931 - accuracy: 0.7358 - val_loss: 0.1952 - val_accuracy: 0.7353\n",
      "Epoch 522/1000\n",
      "563/563 [==============================] - 1s 984us/step - loss: 0.1921 - accuracy: 0.7384 - val_loss: 0.1951 - val_accuracy: 0.7353\n",
      "Epoch 523/1000\n",
      "563/563 [==============================] - 1s 920us/step - loss: 0.1941 - accuracy: 0.7319 - val_loss: 0.1951 - val_accuracy: 0.7353\n",
      "Epoch 524/1000\n",
      "563/563 [==============================] - 1s 936us/step - loss: 0.1950 - accuracy: 0.7292 - val_loss: 0.1951 - val_accuracy: 0.7353\n",
      "Epoch 525/1000\n",
      "563/563 [==============================] - 1s 956us/step - loss: 0.1919 - accuracy: 0.7367 - val_loss: 0.1951 - val_accuracy: 0.7353\n",
      "Epoch 526/1000\n",
      "563/563 [==============================] - 1s 995us/step - loss: 0.1932 - accuracy: 0.7353 - val_loss: 0.1951 - val_accuracy: 0.7353\n",
      "Epoch 527/1000\n",
      "563/563 [==============================] - 1s 934us/step - loss: 0.1933 - accuracy: 0.7323 - val_loss: 0.1950 - val_accuracy: 0.7353\n",
      "Epoch 528/1000\n",
      "563/563 [==============================] - 1s 945us/step - loss: 0.1932 - accuracy: 0.7339 - val_loss: 0.1950 - val_accuracy: 0.7353\n",
      "Epoch 529/1000\n",
      "563/563 [==============================] - 1s 991us/step - loss: 0.1924 - accuracy: 0.7377 - val_loss: 0.1950 - val_accuracy: 0.7353\n",
      "Epoch 530/1000\n",
      "563/563 [==============================] - 1s 981us/step - loss: 0.1949 - accuracy: 0.7308 - val_loss: 0.1950 - val_accuracy: 0.7353\n",
      "Epoch 531/1000\n",
      "563/563 [==============================] - 1s 934us/step - loss: 0.1939 - accuracy: 0.7302 - val_loss: 0.1950 - val_accuracy: 0.7353\n",
      "Epoch 532/1000\n",
      "563/563 [==============================] - 1s 925us/step - loss: 0.1926 - accuracy: 0.7370 - val_loss: 0.1949 - val_accuracy: 0.7353\n",
      "Epoch 533/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1921 - accuracy: 0.7375 - val_loss: 0.1949 - val_accuracy: 0.7353\n",
      "Epoch 534/1000\n",
      "563/563 [==============================] - 1s 931us/step - loss: 0.1926 - accuracy: 0.7367 - val_loss: 0.1949 - val_accuracy: 0.7353\n",
      "Epoch 535/1000\n",
      "563/563 [==============================] - 1s 979us/step - loss: 0.1935 - accuracy: 0.7319 - val_loss: 0.1949 - val_accuracy: 0.7353\n",
      "Epoch 536/1000\n",
      "563/563 [==============================] - 1s 963us/step - loss: 0.1925 - accuracy: 0.7339 - val_loss: 0.1949 - val_accuracy: 0.7353\n",
      "Epoch 537/1000\n",
      "563/563 [==============================] - 1s 982us/step - loss: 0.1931 - accuracy: 0.7326 - val_loss: 0.1949 - val_accuracy: 0.7353\n",
      "Epoch 538/1000\n",
      "563/563 [==============================] - 1s 992us/step - loss: 0.1943 - accuracy: 0.7303 - val_loss: 0.1948 - val_accuracy: 0.7353\n",
      "Epoch 539/1000\n",
      "563/563 [==============================] - 1s 954us/step - loss: 0.1915 - accuracy: 0.7386 - val_loss: 0.1948 - val_accuracy: 0.7353\n",
      "Epoch 540/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1929 - accuracy: 0.7343 - val_loss: 0.1948 - val_accuracy: 0.7353\n",
      "Epoch 541/1000\n",
      "563/563 [==============================] - 1s 938us/step - loss: 0.1921 - accuracy: 0.7347 - val_loss: 0.1948 - val_accuracy: 0.7353\n",
      "Epoch 542/1000\n",
      "563/563 [==============================] - 1s 925us/step - loss: 0.1914 - accuracy: 0.7371 - val_loss: 0.1948 - val_accuracy: 0.7353\n",
      "Epoch 543/1000\n",
      "563/563 [==============================] - 1s 931us/step - loss: 0.1939 - accuracy: 0.7335 - val_loss: 0.1947 - val_accuracy: 0.7353\n",
      "Epoch 544/1000\n",
      "563/563 [==============================] - 1s 995us/step - loss: 0.1916 - accuracy: 0.7360 - val_loss: 0.1947 - val_accuracy: 0.7353\n",
      "Epoch 545/1000\n",
      "563/563 [==============================] - 1s 927us/step - loss: 0.1912 - accuracy: 0.7352 - val_loss: 0.1947 - val_accuracy: 0.7352\n",
      "Epoch 546/1000\n",
      "563/563 [==============================] - 1s 915us/step - loss: 0.1914 - accuracy: 0.7385 - val_loss: 0.1947 - val_accuracy: 0.7352\n",
      "Epoch 547/1000\n",
      "563/563 [==============================] - 1s 940us/step - loss: 0.1932 - accuracy: 0.7313 - val_loss: 0.1947 - val_accuracy: 0.7352\n",
      "Epoch 548/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1921 - accuracy: 0.7367 - val_loss: 0.1947 - val_accuracy: 0.7352\n",
      "Epoch 549/1000\n",
      "563/563 [==============================] - 1s 950us/step - loss: 0.1922 - accuracy: 0.7370 - val_loss: 0.1946 - val_accuracy: 0.7352\n",
      "Epoch 550/1000\n",
      "563/563 [==============================] - 1s 925us/step - loss: 0.1904 - accuracy: 0.7420 - val_loss: 0.1946 - val_accuracy: 0.7352\n",
      "Epoch 551/1000\n",
      "563/563 [==============================] - 1s 991us/step - loss: 0.1920 - accuracy: 0.7329 - val_loss: 0.1946 - val_accuracy: 0.7352\n",
      "Epoch 552/1000\n",
      "563/563 [==============================] - 1s 950us/step - loss: 0.1929 - accuracy: 0.7338 - val_loss: 0.1946 - val_accuracy: 0.7352\n",
      "Epoch 553/1000\n",
      "563/563 [==============================] - 1s 963us/step - loss: 0.1916 - accuracy: 0.7379 - val_loss: 0.1946 - val_accuracy: 0.7352\n",
      "Epoch 554/1000\n",
      "563/563 [==============================] - 1s 940us/step - loss: 0.1927 - accuracy: 0.7359 - val_loss: 0.1945 - val_accuracy: 0.7352\n",
      "Epoch 555/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1930 - accuracy: 0.7329 - val_loss: 0.1945 - val_accuracy: 0.7353\n",
      "Epoch 556/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1911 - accuracy: 0.7365 - val_loss: 0.1945 - val_accuracy: 0.7353\n",
      "Epoch 557/1000\n",
      "563/563 [==============================] - 1s 958us/step - loss: 0.1924 - accuracy: 0.7382 - val_loss: 0.1945 - val_accuracy: 0.7352\n",
      "Epoch 558/1000\n",
      "563/563 [==============================] - 1s 945us/step - loss: 0.1926 - accuracy: 0.7319 - val_loss: 0.1945 - val_accuracy: 0.7352\n",
      "Epoch 559/1000\n",
      "563/563 [==============================] - 1s 974us/step - loss: 0.1909 - accuracy: 0.7393 - val_loss: 0.1945 - val_accuracy: 0.7352\n",
      "Epoch 560/1000\n",
      "563/563 [==============================] - 1s 917us/step - loss: 0.1914 - accuracy: 0.7383 - val_loss: 0.1944 - val_accuracy: 0.7352\n",
      "Epoch 561/1000\n",
      "563/563 [==============================] - 1s 936us/step - loss: 0.1925 - accuracy: 0.7331 - val_loss: 0.1944 - val_accuracy: 0.7353\n",
      "Epoch 562/1000\n",
      "563/563 [==============================] - 1s 966us/step - loss: 0.1917 - accuracy: 0.7337 - val_loss: 0.1944 - val_accuracy: 0.7353\n",
      "Epoch 563/1000\n",
      "563/563 [==============================] - 1s 995us/step - loss: 0.1909 - accuracy: 0.7390 - val_loss: 0.1944 - val_accuracy: 0.7353\n",
      "Epoch 564/1000\n",
      "563/563 [==============================] - 1s 941us/step - loss: 0.1908 - accuracy: 0.7384 - val_loss: 0.1944 - val_accuracy: 0.7356\n",
      "Epoch 565/1000\n",
      "563/563 [==============================] - 1s 924us/step - loss: 0.1938 - accuracy: 0.7325 - val_loss: 0.1944 - val_accuracy: 0.7353\n",
      "Epoch 566/1000\n",
      "563/563 [==============================] - 1s 984us/step - loss: 0.1927 - accuracy: 0.7335 - val_loss: 0.1943 - val_accuracy: 0.7357\n",
      "Epoch 567/1000\n",
      "563/563 [==============================] - 1s 936us/step - loss: 0.1933 - accuracy: 0.7330 - val_loss: 0.1943 - val_accuracy: 0.7353\n",
      "Epoch 568/1000\n",
      "563/563 [==============================] - 1s 933us/step - loss: 0.1921 - accuracy: 0.7365 - val_loss: 0.1943 - val_accuracy: 0.7357\n",
      "Epoch 569/1000\n",
      "563/563 [==============================] - 1s 943us/step - loss: 0.1885 - accuracy: 0.7438 - val_loss: 0.1943 - val_accuracy: 0.7356\n",
      "Epoch 570/1000\n",
      "563/563 [==============================] - 1s 972us/step - loss: 0.1916 - accuracy: 0.7360 - val_loss: 0.1943 - val_accuracy: 0.7356\n",
      "Epoch 571/1000\n",
      "563/563 [==============================] - 1s 931us/step - loss: 0.1912 - accuracy: 0.7385 - val_loss: 0.1943 - val_accuracy: 0.7357\n",
      "Epoch 572/1000\n",
      "563/563 [==============================] - 1s 943us/step - loss: 0.1920 - accuracy: 0.7350 - val_loss: 0.1942 - val_accuracy: 0.7357\n",
      "Epoch 573/1000\n",
      "563/563 [==============================] - 1s 947us/step - loss: 0.1947 - accuracy: 0.7268 - val_loss: 0.1942 - val_accuracy: 0.7357\n",
      "Epoch 574/1000\n",
      "563/563 [==============================] - 1s 981us/step - loss: 0.1911 - accuracy: 0.7378 - val_loss: 0.1942 - val_accuracy: 0.7356\n",
      "Epoch 575/1000\n",
      "563/563 [==============================] - 1s 908us/step - loss: 0.1925 - accuracy: 0.7370 - val_loss: 0.1942 - val_accuracy: 0.7357\n",
      "Epoch 576/1000\n",
      "563/563 [==============================] - 1s 929us/step - loss: 0.1936 - accuracy: 0.7313 - val_loss: 0.1942 - val_accuracy: 0.7356\n",
      "Epoch 577/1000\n",
      "563/563 [==============================] - 1s 966us/step - loss: 0.1925 - accuracy: 0.7354 - val_loss: 0.1942 - val_accuracy: 0.7357\n",
      "Epoch 578/1000\n",
      "563/563 [==============================] - 1s 954us/step - loss: 0.1937 - accuracy: 0.7296 - val_loss: 0.1941 - val_accuracy: 0.7356\n",
      "Epoch 579/1000\n",
      "563/563 [==============================] - 1s 924us/step - loss: 0.1909 - accuracy: 0.7378 - val_loss: 0.1941 - val_accuracy: 0.7357\n",
      "Epoch 580/1000\n",
      "563/563 [==============================] - 1s 940us/step - loss: 0.1923 - accuracy: 0.7346 - val_loss: 0.1941 - val_accuracy: 0.7358\n",
      "Epoch 581/1000\n",
      "563/563 [==============================] - 1s 990us/step - loss: 0.1926 - accuracy: 0.7330 - val_loss: 0.1941 - val_accuracy: 0.7358\n",
      "Epoch 582/1000\n",
      "563/563 [==============================] - 1s 934us/step - loss: 0.1912 - accuracy: 0.7410 - val_loss: 0.1941 - val_accuracy: 0.7358\n",
      "Epoch 583/1000\n",
      "563/563 [==============================] - 1s 929us/step - loss: 0.1928 - accuracy: 0.7318 - val_loss: 0.1941 - val_accuracy: 0.7357\n",
      "Epoch 584/1000\n",
      "563/563 [==============================] - 1s 945us/step - loss: 0.1918 - accuracy: 0.7377 - val_loss: 0.1940 - val_accuracy: 0.7357\n",
      "Epoch 585/1000\n",
      "563/563 [==============================] - 1s 981us/step - loss: 0.1918 - accuracy: 0.7352 - val_loss: 0.1940 - val_accuracy: 0.7358\n",
      "Epoch 586/1000\n",
      "563/563 [==============================] - 1s 924us/step - loss: 0.1921 - accuracy: 0.7351 - val_loss: 0.1940 - val_accuracy: 0.7358\n",
      "Epoch 587/1000\n",
      "563/563 [==============================] - 1s 924us/step - loss: 0.1939 - accuracy: 0.7317 - val_loss: 0.1940 - val_accuracy: 0.7357\n",
      "Epoch 588/1000\n",
      "563/563 [==============================] - 1s 958us/step - loss: 0.1911 - accuracy: 0.7387 - val_loss: 0.1940 - val_accuracy: 0.7358\n",
      "Epoch 589/1000\n",
      "563/563 [==============================] - 1s 965us/step - loss: 0.1921 - accuracy: 0.7361 - val_loss: 0.1940 - val_accuracy: 0.7358\n",
      "Epoch 590/1000\n",
      "563/563 [==============================] - 1s 913us/step - loss: 0.1928 - accuracy: 0.7328 - val_loss: 0.1940 - val_accuracy: 0.7358\n",
      "Epoch 591/1000\n",
      "563/563 [==============================] - 1s 931us/step - loss: 0.1905 - accuracy: 0.7373 - val_loss: 0.1939 - val_accuracy: 0.7358\n",
      "Epoch 592/1000\n",
      "563/563 [==============================] - 1s 963us/step - loss: 0.1915 - accuracy: 0.7374 - val_loss: 0.1939 - val_accuracy: 0.7358\n",
      "Epoch 593/1000\n",
      "563/563 [==============================] - 1s 970us/step - loss: 0.1910 - accuracy: 0.7385 - val_loss: 0.1939 - val_accuracy: 0.7358\n",
      "Epoch 594/1000\n",
      "563/563 [==============================] - 1s 929us/step - loss: 0.1913 - accuracy: 0.7371 - val_loss: 0.1939 - val_accuracy: 0.7358\n",
      "Epoch 595/1000\n",
      "563/563 [==============================] - 1s 931us/step - loss: 0.1914 - accuracy: 0.7355 - val_loss: 0.1939 - val_accuracy: 0.7358\n",
      "Epoch 596/1000\n",
      "563/563 [==============================] - 1s 984us/step - loss: 0.1931 - accuracy: 0.7292 - val_loss: 0.1939 - val_accuracy: 0.7358\n",
      "Epoch 597/1000\n",
      "563/563 [==============================] - 1s 918us/step - loss: 0.1906 - accuracy: 0.7369 - val_loss: 0.1938 - val_accuracy: 0.7358\n",
      "Epoch 598/1000\n",
      "563/563 [==============================] - 1s 936us/step - loss: 0.1919 - accuracy: 0.7352 - val_loss: 0.1938 - val_accuracy: 0.7358\n",
      "Epoch 599/1000\n",
      "563/563 [==============================] - 1s 940us/step - loss: 0.1921 - accuracy: 0.7349 - val_loss: 0.1938 - val_accuracy: 0.7358\n",
      "Epoch 600/1000\n",
      "563/563 [==============================] - 1s 982us/step - loss: 0.1926 - accuracy: 0.7323 - val_loss: 0.1938 - val_accuracy: 0.7358\n",
      "Epoch 601/1000\n",
      "563/563 [==============================] - 1s 945us/step - loss: 0.1926 - accuracy: 0.7321 - val_loss: 0.1938 - val_accuracy: 0.7358\n",
      "Epoch 602/1000\n",
      "563/563 [==============================] - 1s 929us/step - loss: 0.1910 - accuracy: 0.7381 - val_loss: 0.1938 - val_accuracy: 0.7358\n",
      "Epoch 603/1000\n",
      "563/563 [==============================] - 1s 958us/step - loss: 0.1927 - accuracy: 0.7342 - val_loss: 0.1938 - val_accuracy: 0.7358\n",
      "Epoch 604/1000\n",
      "563/563 [==============================] - 1s 982us/step - loss: 0.1924 - accuracy: 0.7339 - val_loss: 0.1937 - val_accuracy: 0.7358\n",
      "Epoch 605/1000\n",
      "563/563 [==============================] - 1s 927us/step - loss: 0.1913 - accuracy: 0.7365 - val_loss: 0.1937 - val_accuracy: 0.7358\n",
      "Epoch 606/1000\n",
      "563/563 [==============================] - 1s 939us/step - loss: 0.1910 - accuracy: 0.7351 - val_loss: 0.1937 - val_accuracy: 0.7358\n",
      "Epoch 607/1000\n",
      "563/563 [==============================] - 1s 974us/step - loss: 0.1910 - accuracy: 0.7379 - val_loss: 0.1937 - val_accuracy: 0.7358\n",
      "Epoch 608/1000\n",
      "563/563 [==============================] - 1s 934us/step - loss: 0.1910 - accuracy: 0.7369 - val_loss: 0.1937 - val_accuracy: 0.7358\n",
      "Epoch 609/1000\n",
      "563/563 [==============================] - 1s 920us/step - loss: 0.1914 - accuracy: 0.7331 - val_loss: 0.1937 - val_accuracy: 0.7358\n",
      "Epoch 610/1000\n",
      "563/563 [==============================] - 1s 929us/step - loss: 0.1908 - accuracy: 0.7375 - val_loss: 0.1936 - val_accuracy: 0.7358\n",
      "Epoch 611/1000\n",
      "563/563 [==============================] - 1s 991us/step - loss: 0.1903 - accuracy: 0.7367 - val_loss: 0.1936 - val_accuracy: 0.7359\n",
      "Epoch 612/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1914 - accuracy: 0.7350 - val_loss: 0.1936 - val_accuracy: 0.7358\n",
      "Epoch 613/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1894 - accuracy: 0.7450 - val_loss: 0.1936 - val_accuracy: 0.7359\n",
      "Epoch 614/1000\n",
      "563/563 [==============================] - 1s 949us/step - loss: 0.1927 - accuracy: 0.7318 - val_loss: 0.1936 - val_accuracy: 0.7358\n",
      "Epoch 615/1000\n",
      "563/563 [==============================] - 1s 966us/step - loss: 0.1914 - accuracy: 0.7363 - val_loss: 0.1936 - val_accuracy: 0.7359\n",
      "Epoch 616/1000\n",
      "563/563 [==============================] - 1s 933us/step - loss: 0.1891 - accuracy: 0.7434 - val_loss: 0.1936 - val_accuracy: 0.7359\n",
      "Epoch 617/1000\n",
      "563/563 [==============================] - 1s 959us/step - loss: 0.1917 - accuracy: 0.7355 - val_loss: 0.1935 - val_accuracy: 0.7359\n",
      "Epoch 618/1000\n",
      "563/563 [==============================] - 1s 991us/step - loss: 0.1916 - accuracy: 0.7349 - val_loss: 0.1935 - val_accuracy: 0.7359\n",
      "Epoch 619/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1892 - accuracy: 0.7426 - val_loss: 0.1935 - val_accuracy: 0.7359\n",
      "Epoch 620/1000\n",
      "563/563 [==============================] - 1s 937us/step - loss: 0.1895 - accuracy: 0.7398 - val_loss: 0.1935 - val_accuracy: 0.7359\n",
      "Epoch 621/1000\n",
      "563/563 [==============================] - 1s 941us/step - loss: 0.1926 - accuracy: 0.7327 - val_loss: 0.1935 - val_accuracy: 0.7359\n",
      "Epoch 622/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1931 - accuracy: 0.7318 - val_loss: 0.1935 - val_accuracy: 0.7357\n",
      "Epoch 623/1000\n",
      "563/563 [==============================] - 1s 934us/step - loss: 0.1922 - accuracy: 0.7346 - val_loss: 0.1935 - val_accuracy: 0.7358\n",
      "Epoch 624/1000\n",
      "563/563 [==============================] - 1s 925us/step - loss: 0.1915 - accuracy: 0.7348 - val_loss: 0.1934 - val_accuracy: 0.7358\n",
      "Epoch 625/1000\n",
      "563/563 [==============================] - 1s 956us/step - loss: 0.1883 - accuracy: 0.7413 - val_loss: 0.1934 - val_accuracy: 0.7358\n",
      "Epoch 626/1000\n",
      "563/563 [==============================] - 1s 961us/step - loss: 0.1896 - accuracy: 0.7386 - val_loss: 0.1934 - val_accuracy: 0.7358\n",
      "Epoch 627/1000\n",
      "563/563 [==============================] - 1s 936us/step - loss: 0.1925 - accuracy: 0.7321 - val_loss: 0.1934 - val_accuracy: 0.7359\n",
      "Epoch 628/1000\n",
      "563/563 [==============================] - 1s 924us/step - loss: 0.1909 - accuracy: 0.7389 - val_loss: 0.1934 - val_accuracy: 0.7359\n",
      "Epoch 629/1000\n",
      "563/563 [==============================] - 1s 959us/step - loss: 0.1908 - accuracy: 0.7361 - val_loss: 0.1934 - val_accuracy: 0.7358\n",
      "Epoch 630/1000\n",
      "563/563 [==============================] - 1s 943us/step - loss: 0.1903 - accuracy: 0.7359 - val_loss: 0.1934 - val_accuracy: 0.7358\n",
      "Epoch 631/1000\n",
      "563/563 [==============================] - 1s 922us/step - loss: 0.1909 - accuracy: 0.7343 - val_loss: 0.1933 - val_accuracy: 0.7358\n",
      "Epoch 632/1000\n",
      "563/563 [==============================] - 1s 929us/step - loss: 0.1911 - accuracy: 0.7358 - val_loss: 0.1933 - val_accuracy: 0.7358\n",
      "Epoch 633/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1912 - accuracy: 0.7360 - val_loss: 0.1933 - val_accuracy: 0.7357\n",
      "Epoch 634/1000\n",
      "563/563 [==============================] - 1s 933us/step - loss: 0.1917 - accuracy: 0.7367 - val_loss: 0.1933 - val_accuracy: 0.7358\n",
      "Epoch 635/1000\n",
      "563/563 [==============================] - 1s 920us/step - loss: 0.1918 - accuracy: 0.7364 - val_loss: 0.1933 - val_accuracy: 0.7358\n",
      "Epoch 636/1000\n",
      "563/563 [==============================] - 1s 918us/step - loss: 0.1896 - accuracy: 0.7410 - val_loss: 0.1933 - val_accuracy: 0.7358\n",
      "Epoch 637/1000\n",
      "563/563 [==============================] - 1s 986us/step - loss: 0.1919 - accuracy: 0.7344 - val_loss: 0.1933 - val_accuracy: 0.7358\n",
      "Epoch 638/1000\n",
      "563/563 [==============================] - 1s 947us/step - loss: 0.1891 - accuracy: 0.7430 - val_loss: 0.1932 - val_accuracy: 0.7358\n",
      "Epoch 639/1000\n",
      "563/563 [==============================] - 1s 941us/step - loss: 0.1904 - accuracy: 0.7391 - val_loss: 0.1932 - val_accuracy: 0.7358\n",
      "Epoch 640/1000\n",
      "563/563 [==============================] - 1s 945us/step - loss: 0.1907 - accuracy: 0.7373 - val_loss: 0.1932 - val_accuracy: 0.7358\n",
      "Epoch 641/1000\n",
      "563/563 [==============================] - 1s 968us/step - loss: 0.1914 - accuracy: 0.7350 - val_loss: 0.1932 - val_accuracy: 0.7358\n",
      "Epoch 642/1000\n",
      "563/563 [==============================] - 1s 947us/step - loss: 0.1922 - accuracy: 0.7361 - val_loss: 0.1932 - val_accuracy: 0.7358\n",
      "Epoch 643/1000\n",
      "563/563 [==============================] - 1s 925us/step - loss: 0.1906 - accuracy: 0.7382 - val_loss: 0.1932 - val_accuracy: 0.7358\n",
      "Epoch 644/1000\n",
      "563/563 [==============================] - 1s 965us/step - loss: 0.1890 - accuracy: 0.7429 - val_loss: 0.1932 - val_accuracy: 0.7358\n",
      "Epoch 645/1000\n",
      "563/563 [==============================] - 1s 944us/step - loss: 0.1901 - accuracy: 0.7399 - val_loss: 0.1932 - val_accuracy: 0.7358\n",
      "Epoch 646/1000\n",
      "563/563 [==============================] - 1s 924us/step - loss: 0.1916 - accuracy: 0.7329 - val_loss: 0.1931 - val_accuracy: 0.7358\n",
      "Epoch 647/1000\n",
      "563/563 [==============================] - 1s 949us/step - loss: 0.1925 - accuracy: 0.7336 - val_loss: 0.1931 - val_accuracy: 0.7358\n",
      "Epoch 648/1000\n",
      "563/563 [==============================] - 1s 966us/step - loss: 0.1925 - accuracy: 0.7346 - val_loss: 0.1931 - val_accuracy: 0.7358\n",
      "Epoch 649/1000\n",
      "563/563 [==============================] - 1s 925us/step - loss: 0.1907 - accuracy: 0.7367 - val_loss: 0.1931 - val_accuracy: 0.7358\n",
      "Epoch 650/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1885 - accuracy: 0.7449 - val_loss: 0.1931 - val_accuracy: 0.7358\n",
      "Epoch 651/1000\n",
      "563/563 [==============================] - 1s 997us/step - loss: 0.1888 - accuracy: 0.7412 - val_loss: 0.1931 - val_accuracy: 0.7358\n",
      "Epoch 652/1000\n",
      "563/563 [==============================] - 1s 986us/step - loss: 0.1899 - accuracy: 0.7397 - val_loss: 0.1930 - val_accuracy: 0.7358\n",
      "Epoch 653/1000\n",
      "563/563 [==============================] - 1s 918us/step - loss: 0.1922 - accuracy: 0.7321 - val_loss: 0.1930 - val_accuracy: 0.7358\n",
      "Epoch 654/1000\n",
      "563/563 [==============================] - 1s 922us/step - loss: 0.1905 - accuracy: 0.7371 - val_loss: 0.1930 - val_accuracy: 0.7358\n",
      "Epoch 655/1000\n",
      "563/563 [==============================] - 1s 984us/step - loss: 0.1902 - accuracy: 0.7376 - val_loss: 0.1930 - val_accuracy: 0.7358\n",
      "Epoch 656/1000\n",
      "563/563 [==============================] - 1s 968us/step - loss: 0.1897 - accuracy: 0.7408 - val_loss: 0.1930 - val_accuracy: 0.7358\n",
      "Epoch 657/1000\n",
      "563/563 [==============================] - 1s 940us/step - loss: 0.1911 - accuracy: 0.7355 - val_loss: 0.1930 - val_accuracy: 0.7358\n",
      "Epoch 658/1000\n",
      "563/563 [==============================] - 1s 947us/step - loss: 0.1932 - accuracy: 0.7301 - val_loss: 0.1930 - val_accuracy: 0.7358\n",
      "Epoch 659/1000\n",
      "563/563 [==============================] - 1s 991us/step - loss: 0.1904 - accuracy: 0.7357 - val_loss: 0.1930 - val_accuracy: 0.7358\n",
      "Epoch 660/1000\n",
      "563/563 [==============================] - 1s 963us/step - loss: 0.1919 - accuracy: 0.7325 - val_loss: 0.1929 - val_accuracy: 0.7358\n",
      "Epoch 661/1000\n",
      "563/563 [==============================] - 1s 931us/step - loss: 0.1904 - accuracy: 0.7374 - val_loss: 0.1929 - val_accuracy: 0.7358\n",
      "Epoch 662/1000\n",
      "563/563 [==============================] - 1s 949us/step - loss: 0.1910 - accuracy: 0.7314 - val_loss: 0.1929 - val_accuracy: 0.7358\n",
      "Epoch 663/1000\n",
      "563/563 [==============================] - 1s 997us/step - loss: 0.1905 - accuracy: 0.7371 - val_loss: 0.1929 - val_accuracy: 0.7358\n",
      "Epoch 664/1000\n",
      "563/563 [==============================] - 1s 949us/step - loss: 0.1903 - accuracy: 0.7366 - val_loss: 0.1929 - val_accuracy: 0.7358\n",
      "Epoch 665/1000\n",
      "563/563 [==============================] - 1s 935us/step - loss: 0.1915 - accuracy: 0.7348 - val_loss: 0.1929 - val_accuracy: 0.7358\n",
      "Epoch 666/1000\n",
      "563/563 [==============================] - 1s 965us/step - loss: 0.1910 - accuracy: 0.7341 - val_loss: 0.1929 - val_accuracy: 0.7358\n",
      "Epoch 667/1000\n",
      "563/563 [==============================] - 1s 947us/step - loss: 0.1916 - accuracy: 0.7324 - val_loss: 0.1929 - val_accuracy: 0.7358\n",
      "Epoch 668/1000\n",
      "563/563 [==============================] - 1s 940us/step - loss: 0.1907 - accuracy: 0.7347 - val_loss: 0.1928 - val_accuracy: 0.7358\n",
      "Epoch 669/1000\n",
      "563/563 [==============================] - 1s 920us/step - loss: 0.1900 - accuracy: 0.7363 - val_loss: 0.1928 - val_accuracy: 0.7358\n",
      "Epoch 670/1000\n",
      "563/563 [==============================] - 1s 997us/step - loss: 0.1917 - accuracy: 0.7323 - val_loss: 0.1928 - val_accuracy: 0.7358\n",
      "Epoch 671/1000\n",
      "563/563 [==============================] - 1s 929us/step - loss: 0.1875 - accuracy: 0.7445 - val_loss: 0.1928 - val_accuracy: 0.7358\n",
      "Epoch 672/1000\n",
      "563/563 [==============================] - 1s 925us/step - loss: 0.1900 - accuracy: 0.7404 - val_loss: 0.1928 - val_accuracy: 0.7358\n",
      "Epoch 673/1000\n",
      "563/563 [==============================] - 1s 922us/step - loss: 0.1924 - accuracy: 0.7339 - val_loss: 0.1928 - val_accuracy: 0.7358\n",
      "Epoch 674/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1920 - accuracy: 0.7322 - val_loss: 0.1928 - val_accuracy: 0.7358\n",
      "Epoch 675/1000\n",
      "563/563 [==============================] - 1s 924us/step - loss: 0.1920 - accuracy: 0.7314 - val_loss: 0.1928 - val_accuracy: 0.7358\n",
      "Epoch 676/1000\n",
      "563/563 [==============================] - 1s 913us/step - loss: 0.1901 - accuracy: 0.7365 - val_loss: 0.1927 - val_accuracy: 0.7358\n",
      "Epoch 677/1000\n",
      "563/563 [==============================] - 1s 950us/step - loss: 0.1905 - accuracy: 0.7351 - val_loss: 0.1927 - val_accuracy: 0.7366\n",
      "Epoch 678/1000\n",
      "563/563 [==============================] - 1s 991us/step - loss: 0.1907 - accuracy: 0.7358 - val_loss: 0.1927 - val_accuracy: 0.7359\n",
      "Epoch 679/1000\n",
      "563/563 [==============================] - 1s 943us/step - loss: 0.1900 - accuracy: 0.7355 - val_loss: 0.1927 - val_accuracy: 0.7359\n",
      "Epoch 680/1000\n",
      "563/563 [==============================] - 1s 945us/step - loss: 0.1907 - accuracy: 0.7378 - val_loss: 0.1927 - val_accuracy: 0.7359\n",
      "Epoch 681/1000\n",
      "563/563 [==============================] - 1s 943us/step - loss: 0.1897 - accuracy: 0.7367 - val_loss: 0.1927 - val_accuracy: 0.7366\n",
      "Epoch 682/1000\n",
      "563/563 [==============================] - 1s 949us/step - loss: 0.1913 - accuracy: 0.7352 - val_loss: 0.1927 - val_accuracy: 0.7359\n",
      "Epoch 683/1000\n",
      "563/563 [==============================] - 1s 922us/step - loss: 0.1913 - accuracy: 0.7334 - val_loss: 0.1927 - val_accuracy: 0.7359\n",
      "Epoch 684/1000\n",
      "563/563 [==============================] - 1s 918us/step - loss: 0.1893 - accuracy: 0.7408 - val_loss: 0.1926 - val_accuracy: 0.7366\n",
      "Epoch 685/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1908 - accuracy: 0.7348 - val_loss: 0.1926 - val_accuracy: 0.7366\n",
      "Epoch 686/1000\n",
      "563/563 [==============================] - 1s 940us/step - loss: 0.1895 - accuracy: 0.7419 - val_loss: 0.1926 - val_accuracy: 0.7359\n",
      "Epoch 687/1000\n",
      "563/563 [==============================] - 1s 952us/step - loss: 0.1909 - accuracy: 0.7348 - val_loss: 0.1926 - val_accuracy: 0.7359\n",
      "Epoch 688/1000\n",
      "563/563 [==============================] - 1s 963us/step - loss: 0.1880 - accuracy: 0.7393 - val_loss: 0.1926 - val_accuracy: 0.7359\n",
      "Epoch 689/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1909 - accuracy: 0.7335 - val_loss: 0.1926 - val_accuracy: 0.7366\n",
      "Epoch 690/1000\n",
      "563/563 [==============================] - 1s 929us/step - loss: 0.1908 - accuracy: 0.7368 - val_loss: 0.1926 - val_accuracy: 0.7366\n",
      "Epoch 691/1000\n",
      "563/563 [==============================] - 1s 935us/step - loss: 0.1905 - accuracy: 0.7378 - val_loss: 0.1926 - val_accuracy: 0.7366\n",
      "Epoch 692/1000\n",
      "563/563 [==============================] - 1s 990us/step - loss: 0.1931 - accuracy: 0.7277 - val_loss: 0.1926 - val_accuracy: 0.7366\n",
      "Epoch 693/1000\n",
      "563/563 [==============================] - 1s 979us/step - loss: 0.1902 - accuracy: 0.7377 - val_loss: 0.1925 - val_accuracy: 0.7366\n",
      "Epoch 694/1000\n",
      "563/563 [==============================] - 1s 943us/step - loss: 0.1913 - accuracy: 0.7354 - val_loss: 0.1925 - val_accuracy: 0.7359\n",
      "Epoch 695/1000\n",
      "563/563 [==============================] - 1s 922us/step - loss: 0.1922 - accuracy: 0.7322 - val_loss: 0.1925 - val_accuracy: 0.7359\n",
      "Epoch 696/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1909 - accuracy: 0.7335 - val_loss: 0.1925 - val_accuracy: 0.7359\n",
      "Epoch 697/1000\n",
      "563/563 [==============================] - 1s 954us/step - loss: 0.1910 - accuracy: 0.7362 - val_loss: 0.1925 - val_accuracy: 0.7359\n",
      "Epoch 698/1000\n",
      "563/563 [==============================] - 1s 956us/step - loss: 0.1894 - accuracy: 0.7395 - val_loss: 0.1925 - val_accuracy: 0.7359\n",
      "Epoch 699/1000\n",
      "563/563 [==============================] - 1s 970us/step - loss: 0.1924 - accuracy: 0.7312 - val_loss: 0.1925 - val_accuracy: 0.7359\n",
      "Epoch 700/1000\n",
      "563/563 [==============================] - 1s 966us/step - loss: 0.1907 - accuracy: 0.7347 - val_loss: 0.1925 - val_accuracy: 0.7366\n",
      "Epoch 701/1000\n",
      "563/563 [==============================] - 1s 941us/step - loss: 0.1907 - accuracy: 0.7364 - val_loss: 0.1924 - val_accuracy: 0.7366\n",
      "Epoch 702/1000\n",
      "563/563 [==============================] - 1s 933us/step - loss: 0.1910 - accuracy: 0.7354 - val_loss: 0.1924 - val_accuracy: 0.7359\n",
      "Epoch 703/1000\n",
      "563/563 [==============================] - 1s 952us/step - loss: 0.1922 - accuracy: 0.7303 - val_loss: 0.1924 - val_accuracy: 0.7366\n",
      "Epoch 704/1000\n",
      "563/563 [==============================] - 1s 963us/step - loss: 0.1897 - accuracy: 0.7381 - val_loss: 0.1924 - val_accuracy: 0.7366\n",
      "Epoch 705/1000\n",
      "563/563 [==============================] - 1s 919us/step - loss: 0.1907 - accuracy: 0.7381 - val_loss: 0.1924 - val_accuracy: 0.7366\n",
      "Epoch 706/1000\n",
      "563/563 [==============================] - 1s 940us/step - loss: 0.1902 - accuracy: 0.7381 - val_loss: 0.1924 - val_accuracy: 0.7366\n",
      "Epoch 707/1000\n",
      "563/563 [==============================] - 1s 966us/step - loss: 0.1915 - accuracy: 0.7324 - val_loss: 0.1924 - val_accuracy: 0.7366\n",
      "Epoch 708/1000\n",
      "563/563 [==============================] - 1s 954us/step - loss: 0.1912 - accuracy: 0.7346 - val_loss: 0.1924 - val_accuracy: 0.7366\n",
      "Epoch 709/1000\n",
      "563/563 [==============================] - 1s 941us/step - loss: 0.1900 - accuracy: 0.7401 - val_loss: 0.1923 - val_accuracy: 0.7366\n",
      "Epoch 710/1000\n",
      "563/563 [==============================] - 1s 911us/step - loss: 0.1924 - accuracy: 0.7331 - val_loss: 0.1923 - val_accuracy: 0.7366\n",
      "Epoch 711/1000\n",
      "563/563 [==============================] - 1s 991us/step - loss: 0.1904 - accuracy: 0.7364 - val_loss: 0.1923 - val_accuracy: 0.7366\n",
      "Epoch 712/1000\n",
      "563/563 [==============================] - 1s 926us/step - loss: 0.1928 - accuracy: 0.7294 - val_loss: 0.1923 - val_accuracy: 0.7359\n",
      "Epoch 713/1000\n",
      "563/563 [==============================] - 1s 925us/step - loss: 0.1910 - accuracy: 0.7330 - val_loss: 0.1923 - val_accuracy: 0.7366\n",
      "Epoch 714/1000\n",
      "563/563 [==============================] - 1s 927us/step - loss: 0.1889 - accuracy: 0.7385 - val_loss: 0.1923 - val_accuracy: 0.7366\n",
      "Epoch 715/1000\n",
      "563/563 [==============================] - 1s 988us/step - loss: 0.1890 - accuracy: 0.7382 - val_loss: 0.1923 - val_accuracy: 0.7366\n",
      "Epoch 716/1000\n",
      "563/563 [==============================] - 1s 934us/step - loss: 0.1907 - accuracy: 0.7344 - val_loss: 0.1923 - val_accuracy: 0.7366\n",
      "Epoch 717/1000\n",
      "563/563 [==============================] - 1s 920us/step - loss: 0.1891 - accuracy: 0.7392 - val_loss: 0.1923 - val_accuracy: 0.7366\n",
      "Epoch 718/1000\n",
      "563/563 [==============================] - 1s 949us/step - loss: 0.1900 - accuracy: 0.7382 - val_loss: 0.1922 - val_accuracy: 0.7366\n",
      "Epoch 719/1000\n",
      "563/563 [==============================] - 1s 972us/step - loss: 0.1905 - accuracy: 0.7343 - val_loss: 0.1922 - val_accuracy: 0.7366\n",
      "Epoch 720/1000\n",
      "563/563 [==============================] - 1s 943us/step - loss: 0.1906 - accuracy: 0.7349 - val_loss: 0.1922 - val_accuracy: 0.7368\n",
      "Epoch 721/1000\n",
      "563/563 [==============================] - 1s 918us/step - loss: 0.1892 - accuracy: 0.7397 - val_loss: 0.1922 - val_accuracy: 0.7368\n",
      "Epoch 722/1000\n",
      "563/563 [==============================] - 1s 997us/step - loss: 0.1885 - accuracy: 0.7405 - val_loss: 0.1922 - val_accuracy: 0.7368\n",
      "Epoch 723/1000\n",
      "563/563 [==============================] - 1s 925us/step - loss: 0.1910 - accuracy: 0.7335 - val_loss: 0.1922 - val_accuracy: 0.7368\n",
      "Epoch 724/1000\n",
      "563/563 [==============================] - 1s 936us/step - loss: 0.1895 - accuracy: 0.7391 - val_loss: 0.1922 - val_accuracy: 0.7367\n",
      "Epoch 725/1000\n",
      "563/563 [==============================] - 1s 941us/step - loss: 0.1877 - accuracy: 0.7448 - val_loss: 0.1922 - val_accuracy: 0.7368\n",
      "Epoch 726/1000\n",
      "563/563 [==============================] - 1s 997us/step - loss: 0.1901 - accuracy: 0.7339 - val_loss: 0.1922 - val_accuracy: 0.7367\n",
      "Epoch 727/1000\n",
      "563/563 [==============================] - 1s 929us/step - loss: 0.1886 - accuracy: 0.7398 - val_loss: 0.1921 - val_accuracy: 0.7367\n",
      "Epoch 728/1000\n",
      "563/563 [==============================] - 1s 911us/step - loss: 0.1904 - accuracy: 0.7335 - val_loss: 0.1921 - val_accuracy: 0.7361\n",
      "Epoch 729/1000\n",
      "563/563 [==============================] - 1s 959us/step - loss: 0.1877 - accuracy: 0.7425 - val_loss: 0.1921 - val_accuracy: 0.7368\n",
      "Epoch 730/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1909 - accuracy: 0.7325 - val_loss: 0.1921 - val_accuracy: 0.7368\n",
      "Epoch 731/1000\n",
      "563/563 [==============================] - 1s 938us/step - loss: 0.1882 - accuracy: 0.7417 - val_loss: 0.1921 - val_accuracy: 0.7368\n",
      "Epoch 732/1000\n",
      "563/563 [==============================] - 1s 945us/step - loss: 0.1894 - accuracy: 0.7381 - val_loss: 0.1921 - val_accuracy: 0.7368\n",
      "Epoch 733/1000\n",
      "563/563 [==============================] - 1s 961us/step - loss: 0.1895 - accuracy: 0.7412 - val_loss: 0.1921 - val_accuracy: 0.7372\n",
      "Epoch 734/1000\n",
      "563/563 [==============================] - 1s 965us/step - loss: 0.1920 - accuracy: 0.7309 - val_loss: 0.1921 - val_accuracy: 0.7368\n",
      "Epoch 735/1000\n",
      "563/563 [==============================] - 1s 925us/step - loss: 0.1908 - accuracy: 0.7366 - val_loss: 0.1920 - val_accuracy: 0.7368\n",
      "Epoch 736/1000\n",
      "563/563 [==============================] - 1s 949us/step - loss: 0.1903 - accuracy: 0.7356 - val_loss: 0.1920 - val_accuracy: 0.7367\n",
      "Epoch 737/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1900 - accuracy: 0.7350 - val_loss: 0.1920 - val_accuracy: 0.7368\n",
      "Epoch 738/1000\n",
      "563/563 [==============================] - 1s 936us/step - loss: 0.1907 - accuracy: 0.7335 - val_loss: 0.1920 - val_accuracy: 0.7367\n",
      "Epoch 739/1000\n",
      "563/563 [==============================] - 1s 950us/step - loss: 0.1894 - accuracy: 0.7369 - val_loss: 0.1920 - val_accuracy: 0.7367\n",
      "Epoch 740/1000\n",
      "563/563 [==============================] - 1s 952us/step - loss: 0.1914 - accuracy: 0.7331 - val_loss: 0.1920 - val_accuracy: 0.7367\n",
      "Epoch 741/1000\n",
      "563/563 [==============================] - 1s 991us/step - loss: 0.1903 - accuracy: 0.7351 - val_loss: 0.1920 - val_accuracy: 0.7372\n",
      "Epoch 742/1000\n",
      "563/563 [==============================] - 1s 945us/step - loss: 0.1911 - accuracy: 0.7343 - val_loss: 0.1920 - val_accuracy: 0.7367\n",
      "Epoch 743/1000\n",
      "563/563 [==============================] - 1s 913us/step - loss: 0.1887 - accuracy: 0.7389 - val_loss: 0.1920 - val_accuracy: 0.7367\n",
      "Epoch 744/1000\n",
      "563/563 [==============================] - 1s 954us/step - loss: 0.1901 - accuracy: 0.7365 - val_loss: 0.1919 - val_accuracy: 0.7367\n",
      "Epoch 745/1000\n",
      "563/563 [==============================] - 1s 959us/step - loss: 0.1906 - accuracy: 0.7332 - val_loss: 0.1919 - val_accuracy: 0.7371\n",
      "Epoch 746/1000\n",
      "563/563 [==============================] - 1s 931us/step - loss: 0.1878 - accuracy: 0.7405 - val_loss: 0.1919 - val_accuracy: 0.7371\n",
      "Epoch 747/1000\n",
      "563/563 [==============================] - 1s 933us/step - loss: 0.1898 - accuracy: 0.7368 - val_loss: 0.1919 - val_accuracy: 0.7367\n",
      "Epoch 748/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1902 - accuracy: 0.7363 - val_loss: 0.1919 - val_accuracy: 0.7371\n",
      "Epoch 749/1000\n",
      "563/563 [==============================] - 1s 950us/step - loss: 0.1904 - accuracy: 0.7331 - val_loss: 0.1919 - val_accuracy: 0.7371\n",
      "Epoch 750/1000\n",
      "563/563 [==============================] - 1s 920us/step - loss: 0.1909 - accuracy: 0.7342 - val_loss: 0.1919 - val_accuracy: 0.7371\n",
      "Epoch 751/1000\n",
      "563/563 [==============================] - 1s 945us/step - loss: 0.1893 - accuracy: 0.7366 - val_loss: 0.1919 - val_accuracy: 0.7371\n",
      "Epoch 752/1000\n",
      "563/563 [==============================] - 1s 986us/step - loss: 0.1892 - accuracy: 0.7360 - val_loss: 0.1918 - val_accuracy: 0.7371\n",
      "Epoch 753/1000\n",
      "563/563 [==============================] - 1s 931us/step - loss: 0.1894 - accuracy: 0.7373 - val_loss: 0.1918 - val_accuracy: 0.7367\n",
      "Epoch 754/1000\n",
      "563/563 [==============================] - 1s 925us/step - loss: 0.1921 - accuracy: 0.7305 - val_loss: 0.1918 - val_accuracy: 0.7367\n",
      "Epoch 755/1000\n",
      "563/563 [==============================] - 1s 970us/step - loss: 0.1911 - accuracy: 0.7333 - val_loss: 0.1918 - val_accuracy: 0.7371\n",
      "Epoch 756/1000\n",
      "563/563 [==============================] - 1s 979us/step - loss: 0.1909 - accuracy: 0.7338 - val_loss: 0.1918 - val_accuracy: 0.7371\n",
      "Epoch 757/1000\n",
      "563/563 [==============================] - 1s 922us/step - loss: 0.1923 - accuracy: 0.7320 - val_loss: 0.1918 - val_accuracy: 0.7367\n",
      "Epoch 758/1000\n",
      "563/563 [==============================] - 1s 986us/step - loss: 0.1898 - accuracy: 0.7371 - val_loss: 0.1918 - val_accuracy: 0.7367\n",
      "Epoch 759/1000\n",
      "563/563 [==============================] - 1s 997us/step - loss: 0.1880 - accuracy: 0.7387 - val_loss: 0.1918 - val_accuracy: 0.7367\n",
      "Epoch 760/1000\n",
      "563/563 [==============================] - 1s 949us/step - loss: 0.1897 - accuracy: 0.7361 - val_loss: 0.1918 - val_accuracy: 0.7367\n",
      "Epoch 761/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1909 - accuracy: 0.7345 - val_loss: 0.1918 - val_accuracy: 0.7371\n",
      "Epoch 762/1000\n",
      "563/563 [==============================] - 1s 933us/step - loss: 0.1901 - accuracy: 0.7357 - val_loss: 0.1918 - val_accuracy: 0.7371\n",
      "Epoch 763/1000\n",
      "563/563 [==============================] - 1s 986us/step - loss: 0.1897 - accuracy: 0.7364 - val_loss: 0.1917 - val_accuracy: 0.7371\n",
      "Epoch 764/1000\n",
      "563/563 [==============================] - 1s 925us/step - loss: 0.1899 - accuracy: 0.7361 - val_loss: 0.1917 - val_accuracy: 0.7367\n",
      "Epoch 765/1000\n",
      "563/563 [==============================] - 1s 938us/step - loss: 0.1885 - accuracy: 0.7396 - val_loss: 0.1917 - val_accuracy: 0.7371\n",
      "Epoch 766/1000\n",
      "563/563 [==============================] - 1s 947us/step - loss: 0.1909 - accuracy: 0.7324 - val_loss: 0.1917 - val_accuracy: 0.7367\n",
      "Epoch 767/1000\n",
      "563/563 [==============================] - 1s 975us/step - loss: 0.1898 - accuracy: 0.7373 - val_loss: 0.1917 - val_accuracy: 0.7367\n",
      "Epoch 768/1000\n",
      "563/563 [==============================] - 1s 934us/step - loss: 0.1909 - accuracy: 0.7337 - val_loss: 0.1917 - val_accuracy: 0.7366\n",
      "Epoch 769/1000\n",
      "563/563 [==============================] - 1s 929us/step - loss: 0.1902 - accuracy: 0.7355 - val_loss: 0.1917 - val_accuracy: 0.7366\n",
      "Epoch 770/1000\n",
      "563/563 [==============================] - 1s 949us/step - loss: 0.1889 - accuracy: 0.7364 - val_loss: 0.1917 - val_accuracy: 0.7366\n",
      "Epoch 771/1000\n",
      "563/563 [==============================] - 1s 972us/step - loss: 0.1910 - accuracy: 0.7340 - val_loss: 0.1917 - val_accuracy: 0.7366\n",
      "Epoch 772/1000\n",
      "563/563 [==============================] - 1s 943us/step - loss: 0.1878 - accuracy: 0.7438 - val_loss: 0.1917 - val_accuracy: 0.7370\n",
      "Epoch 773/1000\n",
      "563/563 [==============================] - 1s 920us/step - loss: 0.1884 - accuracy: 0.7393 - val_loss: 0.1916 - val_accuracy: 0.7370\n",
      "Epoch 774/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1886 - accuracy: 0.7382 - val_loss: 0.1916 - val_accuracy: 0.7370\n",
      "Epoch 775/1000\n",
      "563/563 [==============================] - 1s 929us/step - loss: 0.1910 - accuracy: 0.7328 - val_loss: 0.1916 - val_accuracy: 0.7370\n",
      "Epoch 776/1000\n",
      "563/563 [==============================] - 1s 931us/step - loss: 0.1890 - accuracy: 0.7366 - val_loss: 0.1916 - val_accuracy: 0.7370\n",
      "Epoch 777/1000\n",
      "563/563 [==============================] - 1s 949us/step - loss: 0.1882 - accuracy: 0.7409 - val_loss: 0.1916 - val_accuracy: 0.7370\n",
      "Epoch 778/1000\n",
      "563/563 [==============================] - 1s 981us/step - loss: 0.1896 - accuracy: 0.7378 - val_loss: 0.1916 - val_accuracy: 0.7370\n",
      "Epoch 779/1000\n",
      "563/563 [==============================] - 1s 929us/step - loss: 0.1898 - accuracy: 0.7356 - val_loss: 0.1916 - val_accuracy: 0.7366\n",
      "Epoch 780/1000\n",
      "563/563 [==============================] - 1s 918us/step - loss: 0.1910 - accuracy: 0.7328 - val_loss: 0.1916 - val_accuracy: 0.7370\n",
      "Epoch 781/1000\n",
      "563/563 [==============================] - 1s 943us/step - loss: 0.1887 - accuracy: 0.7397 - val_loss: 0.1916 - val_accuracy: 0.7370\n",
      "Epoch 782/1000\n",
      "563/563 [==============================] - 1s 950us/step - loss: 0.1913 - accuracy: 0.7329 - val_loss: 0.1916 - val_accuracy: 0.7370\n",
      "Epoch 783/1000\n",
      "563/563 [==============================] - 1s 941us/step - loss: 0.1877 - accuracy: 0.7424 - val_loss: 0.1915 - val_accuracy: 0.7370\n",
      "Epoch 784/1000\n",
      "563/563 [==============================] - 1s 941us/step - loss: 0.1883 - accuracy: 0.7393 - val_loss: 0.1915 - val_accuracy: 0.7370\n",
      "Epoch 785/1000\n",
      "563/563 [==============================] - 1s 974us/step - loss: 0.1912 - accuracy: 0.7293 - val_loss: 0.1915 - val_accuracy: 0.7370\n",
      "Epoch 786/1000\n",
      "563/563 [==============================] - 1s 959us/step - loss: 0.1905 - accuracy: 0.7338 - val_loss: 0.1915 - val_accuracy: 0.7370\n",
      "Epoch 787/1000\n",
      "563/563 [==============================] - 1s 943us/step - loss: 0.1886 - accuracy: 0.7399 - val_loss: 0.1915 - val_accuracy: 0.7370\n",
      "Epoch 788/1000\n",
      "563/563 [==============================] - 1s 954us/step - loss: 0.1894 - accuracy: 0.7367 - val_loss: 0.1915 - val_accuracy: 0.7370\n",
      "Epoch 789/1000\n",
      "563/563 [==============================] - 1s 988us/step - loss: 0.1896 - accuracy: 0.7356 - val_loss: 0.1915 - val_accuracy: 0.7370\n",
      "Epoch 790/1000\n",
      "563/563 [==============================] - 1s 931us/step - loss: 0.1907 - accuracy: 0.7307 - val_loss: 0.1915 - val_accuracy: 0.7370\n",
      "Epoch 791/1000\n",
      "563/563 [==============================] - 1s 934us/step - loss: 0.1894 - accuracy: 0.7365 - val_loss: 0.1915 - val_accuracy: 0.7368\n",
      "Epoch 792/1000\n",
      "563/563 [==============================] - 1s 963us/step - loss: 0.1922 - accuracy: 0.7281 - val_loss: 0.1915 - val_accuracy: 0.7368\n",
      "Epoch 793/1000\n",
      "563/563 [==============================] - 1s 982us/step - loss: 0.1905 - accuracy: 0.7345 - val_loss: 0.1914 - val_accuracy: 0.7368\n",
      "Epoch 794/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1905 - accuracy: 0.7346 - val_loss: 0.1914 - val_accuracy: 0.7368\n",
      "Epoch 795/1000\n",
      "563/563 [==============================] - 1s 952us/step - loss: 0.1907 - accuracy: 0.7331 - val_loss: 0.1914 - val_accuracy: 0.7368\n",
      "Epoch 796/1000\n",
      "563/563 [==============================] - 1s 998us/step - loss: 0.1901 - accuracy: 0.7365 - val_loss: 0.1914 - val_accuracy: 0.7368\n",
      "Epoch 797/1000\n",
      "563/563 [==============================] - 1s 986us/step - loss: 0.1893 - accuracy: 0.7395 - val_loss: 0.1914 - val_accuracy: 0.7370\n",
      "Epoch 798/1000\n",
      "563/563 [==============================] - 1s 922us/step - loss: 0.1922 - accuracy: 0.7301 - val_loss: 0.1914 - val_accuracy: 0.7371\n",
      "Epoch 799/1000\n",
      "563/563 [==============================] - 1s 918us/step - loss: 0.1874 - accuracy: 0.7431 - val_loss: 0.1914 - val_accuracy: 0.7370\n",
      "Epoch 800/1000\n",
      "563/563 [==============================] - 1s 988us/step - loss: 0.1897 - accuracy: 0.7370 - val_loss: 0.1914 - val_accuracy: 0.7371\n",
      "Epoch 801/1000\n",
      "563/563 [==============================] - 1s 924us/step - loss: 0.1905 - accuracy: 0.7336 - val_loss: 0.1914 - val_accuracy: 0.7371\n",
      "Epoch 802/1000\n",
      "563/563 [==============================] - 1s 934us/step - loss: 0.1898 - accuracy: 0.7379 - val_loss: 0.1914 - val_accuracy: 0.7371\n",
      "Epoch 803/1000\n",
      "563/563 [==============================] - 1s 945us/step - loss: 0.1875 - accuracy: 0.7390 - val_loss: 0.1914 - val_accuracy: 0.7371\n",
      "Epoch 804/1000\n",
      "563/563 [==============================] - 1s 991us/step - loss: 0.1872 - accuracy: 0.7414 - val_loss: 0.1913 - val_accuracy: 0.7371\n",
      "Epoch 805/1000\n",
      "563/563 [==============================] - 1s 938us/step - loss: 0.1887 - accuracy: 0.7371 - val_loss: 0.1913 - val_accuracy: 0.7371\n",
      "Epoch 806/1000\n",
      "563/563 [==============================] - 1s 933us/step - loss: 0.1869 - accuracy: 0.7406 - val_loss: 0.1913 - val_accuracy: 0.7371\n",
      "Epoch 807/1000\n",
      "563/563 [==============================] - 1s 950us/step - loss: 0.1900 - accuracy: 0.7349 - val_loss: 0.1913 - val_accuracy: 0.7371\n",
      "Epoch 808/1000\n",
      "563/563 [==============================] - 1s 972us/step - loss: 0.1903 - accuracy: 0.7311 - val_loss: 0.1913 - val_accuracy: 0.7371\n",
      "Epoch 809/1000\n",
      "563/563 [==============================] - 1s 929us/step - loss: 0.1908 - accuracy: 0.7327 - val_loss: 0.1913 - val_accuracy: 0.7371\n",
      "Epoch 810/1000\n",
      "563/563 [==============================] - 1s 940us/step - loss: 0.1892 - accuracy: 0.7370 - val_loss: 0.1913 - val_accuracy: 0.7371\n",
      "Epoch 811/1000\n",
      "563/563 [==============================] - 1s 963us/step - loss: 0.1881 - accuracy: 0.7410 - val_loss: 0.1913 - val_accuracy: 0.7371\n",
      "Epoch 812/1000\n",
      "563/563 [==============================] - 1s 934us/step - loss: 0.1897 - accuracy: 0.7343 - val_loss: 0.1913 - val_accuracy: 0.7371\n",
      "Epoch 813/1000\n",
      "563/563 [==============================] - 1s 943us/step - loss: 0.1889 - accuracy: 0.7369 - val_loss: 0.1913 - val_accuracy: 0.7371\n",
      "Epoch 814/1000\n",
      "563/563 [==============================] - 1s 958us/step - loss: 0.1886 - accuracy: 0.7368 - val_loss: 0.1913 - val_accuracy: 0.7371\n",
      "Epoch 815/1000\n",
      "563/563 [==============================] - 1s 986us/step - loss: 0.1899 - accuracy: 0.7324 - val_loss: 0.1912 - val_accuracy: 0.7371\n",
      "Epoch 816/1000\n",
      "563/563 [==============================] - 1s 922us/step - loss: 0.1896 - accuracy: 0.7322 - val_loss: 0.1912 - val_accuracy: 0.7371\n",
      "Epoch 817/1000\n",
      "563/563 [==============================] - 1s 941us/step - loss: 0.1891 - accuracy: 0.7376 - val_loss: 0.1912 - val_accuracy: 0.7371\n",
      "Epoch 818/1000\n",
      "563/563 [==============================] - 1s 977us/step - loss: 0.1886 - accuracy: 0.7392 - val_loss: 0.1912 - val_accuracy: 0.7371\n",
      "Epoch 819/1000\n",
      "563/563 [==============================] - 1s 972us/step - loss: 0.1882 - accuracy: 0.7390 - val_loss: 0.1912 - val_accuracy: 0.7371\n",
      "Epoch 820/1000\n",
      "563/563 [==============================] - 1s 952us/step - loss: 0.1902 - accuracy: 0.7342 - val_loss: 0.1912 - val_accuracy: 0.7371\n",
      "Epoch 821/1000\n",
      "563/563 [==============================] - 1s 917us/step - loss: 0.1903 - accuracy: 0.7336 - val_loss: 0.1912 - val_accuracy: 0.7371\n",
      "Epoch 822/1000\n",
      "563/563 [==============================] - 1s 975us/step - loss: 0.1900 - accuracy: 0.7332 - val_loss: 0.1912 - val_accuracy: 0.7371\n",
      "Epoch 823/1000\n",
      "563/563 [==============================] - 1s 975us/step - loss: 0.1868 - accuracy: 0.7437 - val_loss: 0.1912 - val_accuracy: 0.7371\n",
      "Epoch 824/1000\n",
      "563/563 [==============================] - 1s 927us/step - loss: 0.1872 - accuracy: 0.7381 - val_loss: 0.1912 - val_accuracy: 0.7371\n",
      "Epoch 825/1000\n",
      "563/563 [==============================] - 1s 940us/step - loss: 0.1864 - accuracy: 0.7412 - val_loss: 0.1911 - val_accuracy: 0.7371\n",
      "Epoch 826/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1884 - accuracy: 0.7377 - val_loss: 0.1911 - val_accuracy: 0.7371\n",
      "Epoch 827/1000\n",
      "563/563 [==============================] - 1s 929us/step - loss: 0.1886 - accuracy: 0.7363 - val_loss: 0.1911 - val_accuracy: 0.7371\n",
      "Epoch 828/1000\n",
      "563/563 [==============================] - 1s 934us/step - loss: 0.1889 - accuracy: 0.7368 - val_loss: 0.1911 - val_accuracy: 0.7371\n",
      "Epoch 829/1000\n",
      "563/563 [==============================] - 1s 909us/step - loss: 0.1905 - accuracy: 0.7338 - val_loss: 0.1911 - val_accuracy: 0.7371\n",
      "Epoch 830/1000\n",
      "563/563 [==============================] - 1s 988us/step - loss: 0.1879 - accuracy: 0.7407 - val_loss: 0.1911 - val_accuracy: 0.7371\n",
      "Epoch 831/1000\n",
      "563/563 [==============================] - 1s 938us/step - loss: 0.1900 - accuracy: 0.7317 - val_loss: 0.1911 - val_accuracy: 0.7371\n",
      "Epoch 832/1000\n",
      "563/563 [==============================] - 1s 943us/step - loss: 0.1907 - accuracy: 0.7322 - val_loss: 0.1911 - val_accuracy: 0.7371\n",
      "Epoch 833/1000\n",
      "563/563 [==============================] - 1s 966us/step - loss: 0.1890 - accuracy: 0.7385 - val_loss: 0.1911 - val_accuracy: 0.7371\n",
      "Epoch 834/1000\n",
      "563/563 [==============================] - 1s 970us/step - loss: 0.1899 - accuracy: 0.7363 - val_loss: 0.1911 - val_accuracy: 0.7371\n",
      "Epoch 835/1000\n",
      "563/563 [==============================] - 1s 943us/step - loss: 0.1885 - accuracy: 0.7392 - val_loss: 0.1911 - val_accuracy: 0.7371\n",
      "Epoch 836/1000\n",
      "563/563 [==============================] - 1s 920us/step - loss: 0.1891 - accuracy: 0.7373 - val_loss: 0.1910 - val_accuracy: 0.7371\n",
      "Epoch 837/1000\n",
      "563/563 [==============================] - 1s 974us/step - loss: 0.1894 - accuracy: 0.7344 - val_loss: 0.1910 - val_accuracy: 0.7371\n",
      "Epoch 838/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1884 - accuracy: 0.7392 - val_loss: 0.1910 - val_accuracy: 0.7371\n",
      "Epoch 839/1000\n",
      "563/563 [==============================] - 1s 949us/step - loss: 0.1886 - accuracy: 0.7383 - val_loss: 0.1910 - val_accuracy: 0.7371\n",
      "Epoch 840/1000\n",
      "563/563 [==============================] - 1s 909us/step - loss: 0.1879 - accuracy: 0.7394 - val_loss: 0.1910 - val_accuracy: 0.7371\n",
      "Epoch 841/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1902 - accuracy: 0.7314 - val_loss: 0.1910 - val_accuracy: 0.7371\n",
      "Epoch 842/1000\n",
      "563/563 [==============================] - 1s 909us/step - loss: 0.1885 - accuracy: 0.7368 - val_loss: 0.1910 - val_accuracy: 0.7371\n",
      "Epoch 843/1000\n",
      "563/563 [==============================] - 1s 927us/step - loss: 0.1886 - accuracy: 0.7389 - val_loss: 0.1910 - val_accuracy: 0.7371\n",
      "Epoch 844/1000\n",
      "563/563 [==============================] - 1s 940us/step - loss: 0.1883 - accuracy: 0.7371 - val_loss: 0.1910 - val_accuracy: 0.7371\n",
      "Epoch 845/1000\n",
      "563/563 [==============================] - 1s 972us/step - loss: 0.1896 - accuracy: 0.7359 - val_loss: 0.1910 - val_accuracy: 0.7371\n",
      "Epoch 846/1000\n",
      "563/563 [==============================] - 1s 950us/step - loss: 0.1881 - accuracy: 0.7405 - val_loss: 0.1910 - val_accuracy: 0.7371\n",
      "Epoch 847/1000\n",
      "563/563 [==============================] - 1s 922us/step - loss: 0.1880 - accuracy: 0.7401 - val_loss: 0.1909 - val_accuracy: 0.7371\n",
      "Epoch 848/1000\n",
      "563/563 [==============================] - 1s 986us/step - loss: 0.1880 - accuracy: 0.7396 - val_loss: 0.1909 - val_accuracy: 0.7371\n",
      "Epoch 849/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1888 - accuracy: 0.7397 - val_loss: 0.1909 - val_accuracy: 0.7371\n",
      "Epoch 850/1000\n",
      "563/563 [==============================] - 1s 949us/step - loss: 0.1894 - accuracy: 0.7375 - val_loss: 0.1909 - val_accuracy: 0.7371\n",
      "Epoch 851/1000\n",
      "563/563 [==============================] - 1s 997us/step - loss: 0.1892 - accuracy: 0.7344 - val_loss: 0.1909 - val_accuracy: 0.7371\n",
      "Epoch 852/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1879 - accuracy: 0.7380 - val_loss: 0.1909 - val_accuracy: 0.7374\n",
      "Epoch 853/1000\n",
      "563/563 [==============================] - 1s 927us/step - loss: 0.1894 - accuracy: 0.7338 - val_loss: 0.1909 - val_accuracy: 0.7374\n",
      "Epoch 854/1000\n",
      "563/563 [==============================] - 1s 925us/step - loss: 0.1899 - accuracy: 0.7334 - val_loss: 0.1909 - val_accuracy: 0.7374\n",
      "Epoch 855/1000\n",
      "563/563 [==============================] - 1s 968us/step - loss: 0.1893 - accuracy: 0.7346 - val_loss: 0.1909 - val_accuracy: 0.7374\n",
      "Epoch 856/1000\n",
      "563/563 [==============================] - 1s 972us/step - loss: 0.1891 - accuracy: 0.7384 - val_loss: 0.1909 - val_accuracy: 0.7374\n",
      "Epoch 857/1000\n",
      "563/563 [==============================] - 1s 934us/step - loss: 0.1878 - accuracy: 0.7410 - val_loss: 0.1909 - val_accuracy: 0.7374\n",
      "Epoch 858/1000\n",
      "563/563 [==============================] - 1s 924us/step - loss: 0.1896 - accuracy: 0.7346 - val_loss: 0.1909 - val_accuracy: 0.7374\n",
      "Epoch 859/1000\n",
      "563/563 [==============================] - 1s 986us/step - loss: 0.1877 - accuracy: 0.7394 - val_loss: 0.1908 - val_accuracy: 0.7372\n",
      "Epoch 860/1000\n",
      "563/563 [==============================] - 1s 959us/step - loss: 0.1883 - accuracy: 0.7374 - val_loss: 0.1908 - val_accuracy: 0.7372\n",
      "Epoch 861/1000\n",
      "563/563 [==============================] - 1s 915us/step - loss: 0.1894 - accuracy: 0.7351 - val_loss: 0.1908 - val_accuracy: 0.7372\n",
      "Epoch 862/1000\n",
      "563/563 [==============================] - 1s 941us/step - loss: 0.1886 - accuracy: 0.7379 - val_loss: 0.1908 - val_accuracy: 0.7372\n",
      "Epoch 863/1000\n",
      "563/563 [==============================] - 1s 979us/step - loss: 0.1897 - accuracy: 0.7329 - val_loss: 0.1908 - val_accuracy: 0.7372\n",
      "Epoch 864/1000\n",
      "563/563 [==============================] - 1s 922us/step - loss: 0.1883 - accuracy: 0.7390 - val_loss: 0.1908 - val_accuracy: 0.7372\n",
      "Epoch 865/1000\n",
      "563/563 [==============================] - 1s 917us/step - loss: 0.1889 - accuracy: 0.7337 - val_loss: 0.1908 - val_accuracy: 0.7372\n",
      "Epoch 866/1000\n",
      "563/563 [==============================] - 1s 934us/step - loss: 0.1897 - accuracy: 0.7351 - val_loss: 0.1908 - val_accuracy: 0.7372\n",
      "Epoch 867/1000\n",
      "563/563 [==============================] - 1s 988us/step - loss: 0.1899 - accuracy: 0.7338 - val_loss: 0.1908 - val_accuracy: 0.7372\n",
      "Epoch 868/1000\n",
      "563/563 [==============================] - 1s 947us/step - loss: 0.1896 - accuracy: 0.7369 - val_loss: 0.1908 - val_accuracy: 0.7372\n",
      "Epoch 869/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1881 - accuracy: 0.7385 - val_loss: 0.1908 - val_accuracy: 0.7372\n",
      "Epoch 870/1000\n",
      "563/563 [==============================] - 1s 970us/step - loss: 0.1872 - accuracy: 0.7395 - val_loss: 0.1907 - val_accuracy: 0.7372\n",
      "Epoch 871/1000\n",
      "563/563 [==============================] - 1s 963us/step - loss: 0.1906 - accuracy: 0.7324 - val_loss: 0.1907 - val_accuracy: 0.7372\n",
      "Epoch 872/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1868 - accuracy: 0.7423 - val_loss: 0.1907 - val_accuracy: 0.7372\n",
      "Epoch 873/1000\n",
      "563/563 [==============================] - 1s 931us/step - loss: 0.1898 - accuracy: 0.7351 - val_loss: 0.1907 - val_accuracy: 0.7372\n",
      "Epoch 874/1000\n",
      "563/563 [==============================] - 1s 965us/step - loss: 0.1908 - accuracy: 0.7310 - val_loss: 0.1907 - val_accuracy: 0.7372\n",
      "Epoch 875/1000\n",
      "563/563 [==============================] - 1s 940us/step - loss: 0.1909 - accuracy: 0.7321 - val_loss: 0.1907 - val_accuracy: 0.7372\n",
      "Epoch 876/1000\n",
      "563/563 [==============================] - 1s 936us/step - loss: 0.1870 - accuracy: 0.7422 - val_loss: 0.1907 - val_accuracy: 0.7372\n",
      "Epoch 877/1000\n",
      "563/563 [==============================] - 1s 924us/step - loss: 0.1885 - accuracy: 0.7397 - val_loss: 0.1907 - val_accuracy: 0.7372\n",
      "Epoch 878/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1868 - accuracy: 0.7414 - val_loss: 0.1907 - val_accuracy: 0.7368\n",
      "Epoch 879/1000\n",
      "563/563 [==============================] - 1s 927us/step - loss: 0.1898 - accuracy: 0.7344 - val_loss: 0.1907 - val_accuracy: 0.7372\n",
      "Epoch 880/1000\n",
      "563/563 [==============================] - 1s 931us/step - loss: 0.1900 - accuracy: 0.7307 - val_loss: 0.1907 - val_accuracy: 0.7368\n",
      "Epoch 881/1000\n",
      "563/563 [==============================] - 1s 959us/step - loss: 0.1897 - accuracy: 0.7339 - val_loss: 0.1907 - val_accuracy: 0.7372\n",
      "Epoch 882/1000\n",
      "563/563 [==============================] - 1s 959us/step - loss: 0.1874 - accuracy: 0.7406 - val_loss: 0.1907 - val_accuracy: 0.7372\n",
      "Epoch 883/1000\n",
      "563/563 [==============================] - 1s 936us/step - loss: 0.1901 - accuracy: 0.7300 - val_loss: 0.1906 - val_accuracy: 0.7372\n",
      "Epoch 884/1000\n",
      "563/563 [==============================] - 1s 913us/step - loss: 0.1882 - accuracy: 0.7354 - val_loss: 0.1906 - val_accuracy: 0.7372\n",
      "Epoch 885/1000\n",
      "563/563 [==============================] - 1s 959us/step - loss: 0.1881 - accuracy: 0.7384 - val_loss: 0.1906 - val_accuracy: 0.7372\n",
      "Epoch 886/1000\n",
      "563/563 [==============================] - 1s 941us/step - loss: 0.1881 - accuracy: 0.7377 - val_loss: 0.1906 - val_accuracy: 0.7372\n",
      "Epoch 887/1000\n",
      "563/563 [==============================] - 1s 918us/step - loss: 0.1898 - accuracy: 0.7334 - val_loss: 0.1906 - val_accuracy: 0.7372\n",
      "Epoch 888/1000\n",
      "563/563 [==============================] - 1s 940us/step - loss: 0.1864 - accuracy: 0.7399 - val_loss: 0.1906 - val_accuracy: 0.7372\n",
      "Epoch 889/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1867 - accuracy: 0.7393 - val_loss: 0.1906 - val_accuracy: 0.7372\n",
      "Epoch 890/1000\n",
      "563/563 [==============================] - 1s 947us/step - loss: 0.1881 - accuracy: 0.7375 - val_loss: 0.1906 - val_accuracy: 0.7372\n",
      "Epoch 891/1000\n",
      "563/563 [==============================] - 1s 943us/step - loss: 0.1883 - accuracy: 0.7373 - val_loss: 0.1906 - val_accuracy: 0.7368\n",
      "Epoch 892/1000\n",
      "563/563 [==============================] - 1s 924us/step - loss: 0.1879 - accuracy: 0.7364 - val_loss: 0.1906 - val_accuracy: 0.7372\n",
      "Epoch 893/1000\n",
      "563/563 [==============================] - 1s 963us/step - loss: 0.1882 - accuracy: 0.7383 - val_loss: 0.1906 - val_accuracy: 0.7368\n",
      "Epoch 894/1000\n",
      "563/563 [==============================] - 1s 933us/step - loss: 0.1890 - accuracy: 0.7355 - val_loss: 0.1906 - val_accuracy: 0.7368\n",
      "Epoch 895/1000\n",
      "563/563 [==============================] - 1s 936us/step - loss: 0.1899 - accuracy: 0.7340 - val_loss: 0.1906 - val_accuracy: 0.7368\n",
      "Epoch 896/1000\n",
      "563/563 [==============================] - 1s 940us/step - loss: 0.1898 - accuracy: 0.7319 - val_loss: 0.1905 - val_accuracy: 0.7368\n",
      "Epoch 897/1000\n",
      "563/563 [==============================] - 1s 977us/step - loss: 0.1895 - accuracy: 0.7347 - val_loss: 0.1905 - val_accuracy: 0.7368\n",
      "Epoch 898/1000\n",
      "563/563 [==============================] - 1s 913us/step - loss: 0.1887 - accuracy: 0.7395 - val_loss: 0.1905 - val_accuracy: 0.7372\n",
      "Epoch 899/1000\n",
      "563/563 [==============================] - 1s 927us/step - loss: 0.1861 - accuracy: 0.7427 - val_loss: 0.1905 - val_accuracy: 0.7372\n",
      "Epoch 900/1000\n",
      "563/563 [==============================] - 1s 959us/step - loss: 0.1887 - accuracy: 0.7368 - val_loss: 0.1905 - val_accuracy: 0.7368\n",
      "Epoch 901/1000\n",
      "563/563 [==============================] - 1s 965us/step - loss: 0.1882 - accuracy: 0.7365 - val_loss: 0.1905 - val_accuracy: 0.7368\n",
      "Epoch 902/1000\n",
      "563/563 [==============================] - 1s 917us/step - loss: 0.1875 - accuracy: 0.7395 - val_loss: 0.1905 - val_accuracy: 0.7368\n",
      "Epoch 903/1000\n",
      "563/563 [==============================] - 1s 954us/step - loss: 0.1877 - accuracy: 0.7397 - val_loss: 0.1905 - val_accuracy: 0.7368\n",
      "Epoch 904/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1890 - accuracy: 0.7361 - val_loss: 0.1905 - val_accuracy: 0.7372\n",
      "Epoch 905/1000\n",
      "563/563 [==============================] - 1s 938us/step - loss: 0.1867 - accuracy: 0.7396 - val_loss: 0.1905 - val_accuracy: 0.7368\n",
      "Epoch 906/1000\n",
      "563/563 [==============================] - 1s 954us/step - loss: 0.1860 - accuracy: 0.7451 - val_loss: 0.1905 - val_accuracy: 0.7368\n",
      "Epoch 907/1000\n",
      "563/563 [==============================] - 1s 920us/step - loss: 0.1901 - accuracy: 0.7318 - val_loss: 0.1905 - val_accuracy: 0.7368\n",
      "Epoch 908/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1871 - accuracy: 0.7374 - val_loss: 0.1904 - val_accuracy: 0.7368\n",
      "Epoch 909/1000\n",
      "563/563 [==============================] - 1s 906us/step - loss: 0.1872 - accuracy: 0.7378 - val_loss: 0.1904 - val_accuracy: 0.7368\n",
      "Epoch 910/1000\n",
      "563/563 [==============================] - 1s 931us/step - loss: 0.1902 - accuracy: 0.7313 - val_loss: 0.1904 - val_accuracy: 0.7368\n",
      "Epoch 911/1000\n",
      "563/563 [==============================] - 1s 940us/step - loss: 0.1876 - accuracy: 0.7392 - val_loss: 0.1904 - val_accuracy: 0.7368\n",
      "Epoch 912/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1877 - accuracy: 0.7397 - val_loss: 0.1904 - val_accuracy: 0.7370\n",
      "Epoch 913/1000\n",
      "563/563 [==============================] - 1s 977us/step - loss: 0.1894 - accuracy: 0.7354 - val_loss: 0.1904 - val_accuracy: 0.7370\n",
      "Epoch 914/1000\n",
      "563/563 [==============================] - 1s 915us/step - loss: 0.1896 - accuracy: 0.7346 - val_loss: 0.1904 - val_accuracy: 0.7370\n",
      "Epoch 915/1000\n",
      "563/563 [==============================] - 1s 998us/step - loss: 0.1881 - accuracy: 0.7365 - val_loss: 0.1904 - val_accuracy: 0.7374\n",
      "Epoch 916/1000\n",
      "563/563 [==============================] - 1s 972us/step - loss: 0.1894 - accuracy: 0.7310 - val_loss: 0.1904 - val_accuracy: 0.7374\n",
      "Epoch 917/1000\n",
      "563/563 [==============================] - 1s 929us/step - loss: 0.1869 - accuracy: 0.7404 - val_loss: 0.1904 - val_accuracy: 0.7374\n",
      "Epoch 918/1000\n",
      "563/563 [==============================] - 1s 931us/step - loss: 0.1871 - accuracy: 0.7392 - val_loss: 0.1904 - val_accuracy: 0.7370\n",
      "Epoch 919/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1888 - accuracy: 0.7367 - val_loss: 0.1904 - val_accuracy: 0.7374\n",
      "Epoch 920/1000\n",
      "563/563 [==============================] - 1s 915us/step - loss: 0.1881 - accuracy: 0.7385 - val_loss: 0.1904 - val_accuracy: 0.7374\n",
      "Epoch 921/1000\n",
      "563/563 [==============================] - 1s 927us/step - loss: 0.1880 - accuracy: 0.7372 - val_loss: 0.1904 - val_accuracy: 0.7370\n",
      "Epoch 922/1000\n",
      "563/563 [==============================] - 1s 965us/step - loss: 0.1896 - accuracy: 0.7330 - val_loss: 0.1903 - val_accuracy: 0.7374\n",
      "Epoch 923/1000\n",
      "563/563 [==============================] - 1s 963us/step - loss: 0.1904 - accuracy: 0.7305 - val_loss: 0.1903 - val_accuracy: 0.7374\n",
      "Epoch 924/1000\n",
      "563/563 [==============================] - 1s 926us/step - loss: 0.1886 - accuracy: 0.7387 - val_loss: 0.1903 - val_accuracy: 0.7379\n",
      "Epoch 925/1000\n",
      "563/563 [==============================] - 1s 954us/step - loss: 0.1883 - accuracy: 0.7380 - val_loss: 0.1903 - val_accuracy: 0.7375\n",
      "Epoch 926/1000\n",
      "563/563 [==============================] - 1s 979us/step - loss: 0.1898 - accuracy: 0.7357 - val_loss: 0.1903 - val_accuracy: 0.7375\n",
      "Epoch 927/1000\n",
      "563/563 [==============================] - 1s 965us/step - loss: 0.1863 - accuracy: 0.7400 - val_loss: 0.1903 - val_accuracy: 0.7379\n",
      "Epoch 928/1000\n",
      "563/563 [==============================] - 1s 936us/step - loss: 0.1876 - accuracy: 0.7395 - val_loss: 0.1903 - val_accuracy: 0.7379\n",
      "Epoch 929/1000\n",
      "563/563 [==============================] - 1s 924us/step - loss: 0.1861 - accuracy: 0.7440 - val_loss: 0.1903 - val_accuracy: 0.7379\n",
      "Epoch 930/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1890 - accuracy: 0.7363 - val_loss: 0.1903 - val_accuracy: 0.7375\n",
      "Epoch 931/1000\n",
      "563/563 [==============================] - 1s 934us/step - loss: 0.1868 - accuracy: 0.7413 - val_loss: 0.1903 - val_accuracy: 0.7379\n",
      "Epoch 932/1000\n",
      "563/563 [==============================] - 1s 945us/step - loss: 0.1871 - accuracy: 0.7434 - val_loss: 0.1903 - val_accuracy: 0.7379\n",
      "Epoch 933/1000\n",
      "563/563 [==============================] - 1s 933us/step - loss: 0.1894 - accuracy: 0.7360 - val_loss: 0.1903 - val_accuracy: 0.7379\n",
      "Epoch 934/1000\n",
      "563/563 [==============================] - 1s 979us/step - loss: 0.1885 - accuracy: 0.7360 - val_loss: 0.1902 - val_accuracy: 0.7379\n",
      "Epoch 935/1000\n",
      "563/563 [==============================] - 1s 929us/step - loss: 0.1891 - accuracy: 0.7331 - val_loss: 0.1902 - val_accuracy: 0.7375\n",
      "Epoch 936/1000\n",
      "563/563 [==============================] - 1s 931us/step - loss: 0.1879 - accuracy: 0.7371 - val_loss: 0.1902 - val_accuracy: 0.7379\n",
      "Epoch 937/1000\n",
      "563/563 [==============================] - 1s 974us/step - loss: 0.1877 - accuracy: 0.7380 - val_loss: 0.1902 - val_accuracy: 0.7379\n",
      "Epoch 938/1000\n",
      "563/563 [==============================] - 1s 958us/step - loss: 0.1882 - accuracy: 0.7364 - val_loss: 0.1902 - val_accuracy: 0.7379\n",
      "Epoch 939/1000\n",
      "563/563 [==============================] - 1s 908us/step - loss: 0.1875 - accuracy: 0.7378 - val_loss: 0.1902 - val_accuracy: 0.7379\n",
      "Epoch 940/1000\n",
      "563/563 [==============================] - 1s 920us/step - loss: 0.1867 - accuracy: 0.7400 - val_loss: 0.1902 - val_accuracy: 0.7375\n",
      "Epoch 941/1000\n",
      "563/563 [==============================] - 1s 974us/step - loss: 0.1870 - accuracy: 0.7410 - val_loss: 0.1902 - val_accuracy: 0.7379\n",
      "Epoch 942/1000\n",
      "563/563 [==============================] - 1s 956us/step - loss: 0.1877 - accuracy: 0.7381 - val_loss: 0.1902 - val_accuracy: 0.7375\n",
      "Epoch 943/1000\n",
      "563/563 [==============================] - 1s 947us/step - loss: 0.1870 - accuracy: 0.7408 - val_loss: 0.1902 - val_accuracy: 0.7375\n",
      "Epoch 944/1000\n",
      "563/563 [==============================] - 1s 915us/step - loss: 0.1893 - accuracy: 0.7351 - val_loss: 0.1902 - val_accuracy: 0.7379\n",
      "Epoch 945/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1877 - accuracy: 0.7377 - val_loss: 0.1902 - val_accuracy: 0.7375\n",
      "Epoch 946/1000\n",
      "563/563 [==============================] - 1s 961us/step - loss: 0.1879 - accuracy: 0.7373 - val_loss: 0.1902 - val_accuracy: 0.7375\n",
      "Epoch 947/1000\n",
      "563/563 [==============================] - 1s 977us/step - loss: 0.1905 - accuracy: 0.7312 - val_loss: 0.1902 - val_accuracy: 0.7375\n",
      "Epoch 948/1000\n",
      "563/563 [==============================] - 1s 949us/step - loss: 0.1895 - accuracy: 0.7334 - val_loss: 0.1902 - val_accuracy: 0.7379\n",
      "Epoch 949/1000\n",
      "563/563 [==============================] - 1s 968us/step - loss: 0.1880 - accuracy: 0.7388 - val_loss: 0.1901 - val_accuracy: 0.7375\n",
      "Epoch 950/1000\n",
      "563/563 [==============================] - 1s 929us/step - loss: 0.1848 - accuracy: 0.7479 - val_loss: 0.1901 - val_accuracy: 0.7375\n",
      "Epoch 951/1000\n",
      "563/563 [==============================] - 1s 930us/step - loss: 0.1884 - accuracy: 0.7358 - val_loss: 0.1901 - val_accuracy: 0.7375\n",
      "Epoch 952/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1896 - accuracy: 0.7350 - val_loss: 0.1901 - val_accuracy: 0.7380\n",
      "Epoch 953/1000\n",
      "563/563 [==============================] - 1s 961us/step - loss: 0.1880 - accuracy: 0.7374 - val_loss: 0.1901 - val_accuracy: 0.7380\n",
      "Epoch 954/1000\n",
      "563/563 [==============================] - 1s 947us/step - loss: 0.1854 - accuracy: 0.7441 - val_loss: 0.1901 - val_accuracy: 0.7380\n",
      "Epoch 955/1000\n",
      "563/563 [==============================] - 1s 925us/step - loss: 0.1890 - accuracy: 0.7350 - val_loss: 0.1901 - val_accuracy: 0.7376\n",
      "Epoch 956/1000\n",
      "563/563 [==============================] - 1s 990us/step - loss: 0.1872 - accuracy: 0.7355 - val_loss: 0.1901 - val_accuracy: 0.7380\n",
      "Epoch 957/1000\n",
      "563/563 [==============================] - 1s 922us/step - loss: 0.1876 - accuracy: 0.7396 - val_loss: 0.1901 - val_accuracy: 0.7380\n",
      "Epoch 958/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1871 - accuracy: 0.7383 - val_loss: 0.1901 - val_accuracy: 0.7375\n",
      "Epoch 959/1000\n",
      "563/563 [==============================] - 1s 981us/step - loss: 0.1864 - accuracy: 0.7403 - val_loss: 0.1901 - val_accuracy: 0.7380\n",
      "Epoch 960/1000\n",
      "563/563 [==============================] - 1s 975us/step - loss: 0.1877 - accuracy: 0.7394 - val_loss: 0.1901 - val_accuracy: 0.7376\n",
      "Epoch 961/1000\n",
      "563/563 [==============================] - 1s 934us/step - loss: 0.1863 - accuracy: 0.7413 - val_loss: 0.1901 - val_accuracy: 0.7380\n",
      "Epoch 962/1000\n",
      "563/563 [==============================] - 1s 936us/step - loss: 0.1875 - accuracy: 0.7343 - val_loss: 0.1901 - val_accuracy: 0.7380\n",
      "Epoch 963/1000\n",
      "563/563 [==============================] - 1s 974us/step - loss: 0.1886 - accuracy: 0.7348 - val_loss: 0.1900 - val_accuracy: 0.7380\n",
      "Epoch 964/1000\n",
      "563/563 [==============================] - 1s 939us/step - loss: 0.1874 - accuracy: 0.7381 - val_loss: 0.1900 - val_accuracy: 0.7380\n",
      "Epoch 965/1000\n",
      "563/563 [==============================] - 1s 909us/step - loss: 0.1880 - accuracy: 0.7353 - val_loss: 0.1900 - val_accuracy: 0.7376\n",
      "Epoch 966/1000\n",
      "563/563 [==============================] - 1s 931us/step - loss: 0.1882 - accuracy: 0.7367 - val_loss: 0.1900 - val_accuracy: 0.7376\n",
      "Epoch 967/1000\n",
      "563/563 [==============================] - 1s 966us/step - loss: 0.1894 - accuracy: 0.7328 - val_loss: 0.1900 - val_accuracy: 0.7376\n",
      "Epoch 968/1000\n",
      "563/563 [==============================] - 1s 938us/step - loss: 0.1867 - accuracy: 0.7397 - val_loss: 0.1900 - val_accuracy: 0.7380\n",
      "Epoch 969/1000\n",
      "563/563 [==============================] - 1s 936us/step - loss: 0.1900 - accuracy: 0.7316 - val_loss: 0.1900 - val_accuracy: 0.7380\n",
      "Epoch 970/1000\n",
      "563/563 [==============================] - 1s 913us/step - loss: 0.1884 - accuracy: 0.7353 - val_loss: 0.1900 - val_accuracy: 0.7376\n",
      "Epoch 971/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1895 - accuracy: 0.7332 - val_loss: 0.1900 - val_accuracy: 0.7376\n",
      "Epoch 972/1000\n",
      "563/563 [==============================] - 1s 931us/step - loss: 0.1877 - accuracy: 0.7384 - val_loss: 0.1900 - val_accuracy: 0.7380\n",
      "Epoch 973/1000\n",
      "563/563 [==============================] - 1s 917us/step - loss: 0.1866 - accuracy: 0.7392 - val_loss: 0.1900 - val_accuracy: 0.7376\n",
      "Epoch 974/1000\n",
      "563/563 [==============================] - 1s 936us/step - loss: 0.1885 - accuracy: 0.7375 - val_loss: 0.1900 - val_accuracy: 0.7380\n",
      "Epoch 975/1000\n",
      "563/563 [==============================] - 1s 974us/step - loss: 0.1867 - accuracy: 0.7400 - val_loss: 0.1900 - val_accuracy: 0.7380\n",
      "Epoch 976/1000\n",
      "563/563 [==============================] - 1s 990us/step - loss: 0.1883 - accuracy: 0.7359 - val_loss: 0.1900 - val_accuracy: 0.7380\n",
      "Epoch 977/1000\n",
      "563/563 [==============================] - 1s 947us/step - loss: 0.1889 - accuracy: 0.7346 - val_loss: 0.1900 - val_accuracy: 0.7380\n",
      "Epoch 978/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1871 - accuracy: 0.7410 - val_loss: 0.1899 - val_accuracy: 0.7380\n",
      "Epoch 979/1000\n",
      "563/563 [==============================] - 1s 940us/step - loss: 0.1876 - accuracy: 0.7381 - val_loss: 0.1899 - val_accuracy: 0.7380\n",
      "Epoch 980/1000\n",
      "563/563 [==============================] - 1s 982us/step - loss: 0.1864 - accuracy: 0.7414 - val_loss: 0.1899 - val_accuracy: 0.7380\n",
      "Epoch 981/1000\n",
      "563/563 [==============================] - 1s 956us/step - loss: 0.1897 - accuracy: 0.7310 - val_loss: 0.1899 - val_accuracy: 0.7380\n",
      "Epoch 982/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.1888 - accuracy: 0.7359 - val_loss: 0.1899 - val_accuracy: 0.7380\n",
      "Epoch 983/1000\n",
      "563/563 [==============================] - 1s 997us/step - loss: 0.1886 - accuracy: 0.7369 - val_loss: 0.1899 - val_accuracy: 0.7376\n",
      "Epoch 984/1000\n",
      "563/563 [==============================] - 1s 945us/step - loss: 0.1876 - accuracy: 0.7403 - val_loss: 0.1899 - val_accuracy: 0.7376\n",
      "Epoch 985/1000\n",
      "563/563 [==============================] - 1s 950us/step - loss: 0.1871 - accuracy: 0.7377 - val_loss: 0.1899 - val_accuracy: 0.7380\n",
      "Epoch 986/1000\n",
      "563/563 [==============================] - 1s 979us/step - loss: 0.1877 - accuracy: 0.7399 - val_loss: 0.1899 - val_accuracy: 0.7376\n",
      "Epoch 987/1000\n",
      "563/563 [==============================] - 1s 949us/step - loss: 0.1884 - accuracy: 0.7334 - val_loss: 0.1899 - val_accuracy: 0.7376\n",
      "Epoch 988/1000\n",
      "563/563 [==============================] - 1s 925us/step - loss: 0.1860 - accuracy: 0.7389 - val_loss: 0.1899 - val_accuracy: 0.7376\n",
      "Epoch 989/1000\n",
      "563/563 [==============================] - 1s 975us/step - loss: 0.1879 - accuracy: 0.7401 - val_loss: 0.1899 - val_accuracy: 0.7376\n",
      "Epoch 990/1000\n",
      "563/563 [==============================] - 1s 982us/step - loss: 0.1885 - accuracy: 0.7343 - val_loss: 0.1899 - val_accuracy: 0.7376\n",
      "Epoch 991/1000\n",
      "563/563 [==============================] - 1s 925us/step - loss: 0.1865 - accuracy: 0.7391 - val_loss: 0.1899 - val_accuracy: 0.7380\n",
      "Epoch 992/1000\n",
      "563/563 [==============================] - 1s 917us/step - loss: 0.1877 - accuracy: 0.7390 - val_loss: 0.1899 - val_accuracy: 0.7376\n",
      "Epoch 993/1000\n",
      "563/563 [==============================] - 1s 995us/step - loss: 0.1876 - accuracy: 0.7381 - val_loss: 0.1899 - val_accuracy: 0.7376\n",
      "Epoch 994/1000\n",
      "563/563 [==============================] - 1s 927us/step - loss: 0.1863 - accuracy: 0.7391 - val_loss: 0.1898 - val_accuracy: 0.7380\n",
      "Epoch 995/1000\n",
      "563/563 [==============================] - 1s 934us/step - loss: 0.1897 - accuracy: 0.7310 - val_loss: 0.1898 - val_accuracy: 0.7380\n",
      "Epoch 996/1000\n",
      "563/563 [==============================] - 1s 941us/step - loss: 0.1865 - accuracy: 0.7393 - val_loss: 0.1898 - val_accuracy: 0.7376\n",
      "Epoch 997/1000\n",
      "563/563 [==============================] - 1s 959us/step - loss: 0.1862 - accuracy: 0.7400 - val_loss: 0.1898 - val_accuracy: 0.7380\n",
      "Epoch 998/1000\n",
      "563/563 [==============================] - 1s 925us/step - loss: 0.1868 - accuracy: 0.7408 - val_loss: 0.1898 - val_accuracy: 0.7376\n",
      "Epoch 999/1000\n",
      "563/563 [==============================] - 1s 927us/step - loss: 0.1882 - accuracy: 0.7363 - val_loss: 0.1898 - val_accuracy: 0.7380\n",
      "Epoch 1000/1000\n",
      "563/563 [==============================] - 1s 991us/step - loss: 0.1866 - accuracy: 0.7383 - val_loss: 0.1898 - val_accuracy: 0.7376\n"
     ]
    }
   ],
   "source": [
    "# Fit the model using 50 epochs and the training data\n",
    "fit_model_A28 = nn_A28.fit(X_train_scaled, y_train, validation_split=0.3, epochs=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alternative Model 28 Results\n",
      "Loss: 0.1905694156885147, Accuracy: 0.7293294668197632\n"
     ]
    }
   ],
   "source": [
    "print(\"Alternative Model 28 Results\")\n",
    "# Evaluate the model loss and accuracy metrics using the evaluate method and the test data\n",
    "model_loss, model_accuracy = nn_A28.evaluate(X_test_scaled,y_test,verbose=0)\n",
    "# Display the model loss and accuracy results\n",
    "print(f\"Loss: {model_loss}, Accuracy: {model_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alternative Model 29+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "114"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the the number of inputs (features) to the model\n",
    "number_input_features = len(X_train.iloc[0])\n",
    "# Review the number of features\n",
    "number_input_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the number of neurons in the output layer\n",
    "number_output_neurons_A29 = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the number of hidden nodes for the first hidden layer\n",
    "hidden_nodes_layer1_A29 =  (number_input_features + number_output_neurons_A29) // 2\n",
    "# Review the number of hidden nodes in the first layer\n",
    "hidden_nodes_layer1_A29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the number of hidden nodes for the second hidden layer\n",
    "hidden_nodes_layer2_A29 =  (hidden_nodes_layer1_A29 + number_output_neurons_A29) // 2\n",
    "# Review the number of hidden nodes in the second layer\n",
    "hidden_nodes_layer2_A29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the number of hidden nodes for the third hidden layer\n",
    "hidden_nodes_layer3_A29 =  (hidden_nodes_layer2_A29 + number_output_neurons_A29) // 2\n",
    "# Review the number of hidden nodes in the third layer\n",
    "hidden_nodes_layer3_A29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the number of hidden nodes for the fourth hidden layer\n",
    "hidden_nodes_layer4_A29 =  (hidden_nodes_layer3_A29 + number_output_neurons_A29) // 2\n",
    "# Review the number of hidden nodes in the fourth layer\n",
    "hidden_nodes_layer4_A29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the number of hidden nodes for the fifth hidden layer\n",
    "hidden_nodes_layer5_A29 =  (hidden_nodes_layer4_A29 + number_output_neurons_A29) // 2\n",
    "# Review the number of hidden nodes in the fifth layer\n",
    "hidden_nodes_layer5_A29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the number of hidden nodes for the Sixth hidden layer\n",
    "hidden_nodes_layer6_A29 =  (hidden_nodes_layer5_A29 + number_output_neurons_A29) // 2\n",
    "# Review the number of hidden nodes in the sixth layer\n",
    "hidden_nodes_layer6_A29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_38 (Dense)             (None, 57)                6555      \n",
      "_________________________________________________________________\n",
      "dense_39 (Dense)             (None, 29)                1682      \n",
      "_________________________________________________________________\n",
      "dense_40 (Dense)             (None, 15)                450       \n",
      "_________________________________________________________________\n",
      "dense_41 (Dense)             (None, 8)                 128       \n",
      "_________________________________________________________________\n",
      "dense_42 (Dense)             (None, 4)                 36        \n",
      "_________________________________________________________________\n",
      "dense_43 (Dense)             (None, 2)                 10        \n",
      "_________________________________________________________________\n",
      "dense_44 (Dense)             (None, 1)                 3         \n",
      "=================================================================\n",
      "Total params: 8,864\n",
      "Trainable params: 8,864\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Create and Display the Sequential Model Instance \n",
    "# for Model A29\n",
    "nn_A29 = Sequential() \n",
    "\n",
    "# Add the first hidden layer\n",
    "nn_A29.add(Dense(units=hidden_nodes_layer1_A29, input_dim=number_input_features, activation=\"relu\"))\n",
    "# Add the second hidden layer\n",
    "nn_A29.add(Dense(units=hidden_nodes_layer2_A29, input_dim=number_input_features, activation=\"relu\"))\n",
    "# Add the third hidden layer\n",
    "nn_A29.add(Dense(units=hidden_nodes_layer3_A29, input_dim=number_input_features, activation=\"relu\"))\n",
    "# Add the fourth hidden layer\n",
    "nn_A29.add(Dense(units=hidden_nodes_layer4_A29, input_dim=number_input_features, activation=\"relu\"))\n",
    "# Add the fifth hidden layer\n",
    "nn_A29.add(Dense(units=hidden_nodes_layer5_A29, input_dim=number_input_features, activation=\"relu\"))\n",
    "# Add the sixth hidden layer\n",
    "nn_A29.add(Dense(units=hidden_nodes_layer6_A29, activation=\"relu\"))\n",
    "\n",
    "# Add the output layer to the model specifying the number of output neurons and activation function\n",
    "nn_A29.add(Dense(units=number_output_neurons_A29, activation=\"sigmoid\"))\n",
    "\n",
    "# Display the Sequential model summary\n",
    "nn_A29.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the Sequential model\n",
    "nn_A29.compile(loss=\"mse\", optimizer=\"sgd\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2498 - accuracy: 0.5275 - val_loss: 0.2489 - val_accuracy: 0.5459\n",
      "Epoch 2/1000\n",
      "563/563 [==============================] - 1s 950us/step - loss: 0.2494 - accuracy: 0.5281 - val_loss: 0.2485 - val_accuracy: 0.5459\n",
      "Epoch 3/1000\n",
      "563/563 [==============================] - 1s 940us/step - loss: 0.2492 - accuracy: 0.5285 - val_loss: 0.2483 - val_accuracy: 0.5459\n",
      "Epoch 4/1000\n",
      "563/563 [==============================] - 1s 925us/step - loss: 0.2492 - accuracy: 0.5278 - val_loss: 0.2483 - val_accuracy: 0.5459\n",
      "Epoch 5/1000\n",
      "563/563 [==============================] - 1s 956us/step - loss: 0.2490 - accuracy: 0.5322 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 6/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2491 - accuracy: 0.5295 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 7/1000\n",
      "563/563 [==============================] - 1s 986us/step - loss: 0.2496 - accuracy: 0.5215 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 8/1000\n",
      "563/563 [==============================] - 1s 943us/step - loss: 0.2493 - accuracy: 0.5262 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 9/1000\n",
      "563/563 [==============================] - 1s 911us/step - loss: 0.2488 - accuracy: 0.5345 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 10/1000\n",
      "563/563 [==============================] - 1s 972us/step - loss: 0.2495 - accuracy: 0.5233 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 11/1000\n",
      "563/563 [==============================] - 1s 934us/step - loss: 0.2489 - accuracy: 0.5337 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 12/1000\n",
      "563/563 [==============================] - 1s 917us/step - loss: 0.2495 - accuracy: 0.5236 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 13/1000\n",
      "563/563 [==============================] - 1s 952us/step - loss: 0.2493 - accuracy: 0.5268 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 14/1000\n",
      "563/563 [==============================] - 1s 940us/step - loss: 0.2490 - accuracy: 0.5313 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 15/1000\n",
      "563/563 [==============================] - 1s 924us/step - loss: 0.2491 - accuracy: 0.5296 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 16/1000\n",
      "563/563 [==============================] - 1s 927us/step - loss: 0.2493 - accuracy: 0.5266 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 17/1000\n",
      "563/563 [==============================] - 1s 958us/step - loss: 0.2493 - accuracy: 0.5263 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 18/1000\n",
      "563/563 [==============================] - 1s 974us/step - loss: 0.2492 - accuracy: 0.5288 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 19/1000\n",
      "563/563 [==============================] - 1s 925us/step - loss: 0.2493 - accuracy: 0.5275 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 20/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2492 - accuracy: 0.5276 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 21/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2492 - accuracy: 0.5286 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 22/1000\n",
      "563/563 [==============================] - 1s 933us/step - loss: 0.2489 - accuracy: 0.5330 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 23/1000\n",
      "563/563 [==============================] - 1s 925us/step - loss: 0.2493 - accuracy: 0.5267 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 24/1000\n",
      "563/563 [==============================] - 1s 941us/step - loss: 0.2492 - accuracy: 0.5286 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 25/1000\n",
      "563/563 [==============================] - 1s 990us/step - loss: 0.2489 - accuracy: 0.5331 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 26/1000\n",
      "563/563 [==============================] - 1s 945us/step - loss: 0.2492 - accuracy: 0.5281 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 27/1000\n",
      "563/563 [==============================] - 1s 922us/step - loss: 0.2489 - accuracy: 0.5329 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 28/1000\n",
      "563/563 [==============================] - 1s 938us/step - loss: 0.2492 - accuracy: 0.5279 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 29/1000\n",
      "563/563 [==============================] - 1s 970us/step - loss: 0.2490 - accuracy: 0.5314 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 30/1000\n",
      "563/563 [==============================] - 1s 909us/step - loss: 0.2491 - accuracy: 0.5305 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 31/1000\n",
      "563/563 [==============================] - 1s 908us/step - loss: 0.2491 - accuracy: 0.5304 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 32/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2493 - accuracy: 0.5261 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 33/1000\n",
      "563/563 [==============================] - 1s 945us/step - loss: 0.2493 - accuracy: 0.5260 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 34/1000\n",
      "563/563 [==============================] - 1s 908us/step - loss: 0.2491 - accuracy: 0.5293 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 35/1000\n",
      "563/563 [==============================] - 1s 927us/step - loss: 0.2495 - accuracy: 0.5236 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 36/1000\n",
      "563/563 [==============================] - 1s 958us/step - loss: 0.2492 - accuracy: 0.5282 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 37/1000\n",
      "563/563 [==============================] - 1s 934us/step - loss: 0.2490 - accuracy: 0.5315 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 38/1000\n",
      "563/563 [==============================] - 1s 927us/step - loss: 0.2492 - accuracy: 0.5275 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 39/1000\n",
      "563/563 [==============================] - 1s 913us/step - loss: 0.2494 - accuracy: 0.5255 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 40/1000\n",
      "563/563 [==============================] - 1s 965us/step - loss: 0.2496 - accuracy: 0.5204 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 41/1000\n",
      "563/563 [==============================] - 1s 918us/step - loss: 0.2496 - accuracy: 0.5220 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 42/1000\n",
      "563/563 [==============================] - 1s 934us/step - loss: 0.2492 - accuracy: 0.5284 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 43/1000\n",
      "563/563 [==============================] - 1s 952us/step - loss: 0.2492 - accuracy: 0.5292 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 44/1000\n",
      "563/563 [==============================] - 1s 954us/step - loss: 0.2493 - accuracy: 0.5272 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 45/1000\n",
      "563/563 [==============================] - 1s 924us/step - loss: 0.2492 - accuracy: 0.5290 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 46/1000\n",
      "563/563 [==============================] - 1s 933us/step - loss: 0.2493 - accuracy: 0.5262 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 47/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2493 - accuracy: 0.5262 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 48/1000\n",
      "563/563 [==============================] - 1s 988us/step - loss: 0.2493 - accuracy: 0.5266 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 49/1000\n",
      "563/563 [==============================] - 1s 965us/step - loss: 0.2492 - accuracy: 0.5282 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 50/1000\n",
      "563/563 [==============================] - 1s 922us/step - loss: 0.2492 - accuracy: 0.5281 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 51/1000\n",
      "563/563 [==============================] - 1s 988us/step - loss: 0.2493 - accuracy: 0.5265 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 52/1000\n",
      "563/563 [==============================] - 1s 929us/step - loss: 0.2494 - accuracy: 0.5257 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 53/1000\n",
      "563/563 [==============================] - 1s 951us/step - loss: 0.2495 - accuracy: 0.5235 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 54/1000\n",
      "563/563 [==============================] - 1s 934us/step - loss: 0.2492 - accuracy: 0.5281 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 55/1000\n",
      "563/563 [==============================] - 1s 981us/step - loss: 0.2490 - accuracy: 0.5312 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 56/1000\n",
      "563/563 [==============================] - 1s 915us/step - loss: 0.2494 - accuracy: 0.5250 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 57/1000\n",
      "563/563 [==============================] - 1s 908us/step - loss: 0.2493 - accuracy: 0.5273 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 58/1000\n",
      "563/563 [==============================] - 1s 966us/step - loss: 0.2491 - accuracy: 0.5310 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 59/1000\n",
      "563/563 [==============================] - 1s 945us/step - loss: 0.2492 - accuracy: 0.5280 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 60/1000\n",
      "563/563 [==============================] - 1s 927us/step - loss: 0.2494 - accuracy: 0.5257 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 61/1000\n",
      "563/563 [==============================] - 1s 925us/step - loss: 0.2489 - accuracy: 0.5336 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 62/1000\n",
      "563/563 [==============================] - 1s 977us/step - loss: 0.2491 - accuracy: 0.5297 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 63/1000\n",
      "563/563 [==============================] - 1s 963us/step - loss: 0.2493 - accuracy: 0.5275 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 64/1000\n",
      "563/563 [==============================] - 1s 972us/step - loss: 0.2494 - accuracy: 0.5247 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 65/1000\n",
      "563/563 [==============================] - 1s 949us/step - loss: 0.2497 - accuracy: 0.5190 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 66/1000\n",
      "563/563 [==============================] - 1s 966us/step - loss: 0.2492 - accuracy: 0.5293 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 67/1000\n",
      "563/563 [==============================] - 1s 925us/step - loss: 0.2496 - accuracy: 0.5214 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 68/1000\n",
      "563/563 [==============================] - 1s 909us/step - loss: 0.2492 - accuracy: 0.5280 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 69/1000\n",
      "563/563 [==============================] - 1s 943us/step - loss: 0.2489 - accuracy: 0.5333 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 70/1000\n",
      "563/563 [==============================] - 1s 979us/step - loss: 0.2491 - accuracy: 0.5299 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 71/1000\n",
      "563/563 [==============================] - 1s 958us/step - loss: 0.2494 - accuracy: 0.5240 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 72/1000\n",
      "563/563 [==============================] - 1s 963us/step - loss: 0.2495 - accuracy: 0.5225 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 73/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2492 - accuracy: 0.5287 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 74/1000\n",
      "563/563 [==============================] - 1s 960us/step - loss: 0.2491 - accuracy: 0.5299 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 75/1000\n",
      "563/563 [==============================] - 1s 929us/step - loss: 0.2498 - accuracy: 0.5174 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 76/1000\n",
      "563/563 [==============================] - 1s 910us/step - loss: 0.2492 - accuracy: 0.5286 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 77/1000\n",
      "563/563 [==============================] - 1s 977us/step - loss: 0.2490 - accuracy: 0.5324 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 78/1000\n",
      "563/563 [==============================] - 1s 922us/step - loss: 0.2492 - accuracy: 0.5280 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 79/1000\n",
      "563/563 [==============================] - 1s 930us/step - loss: 0.2495 - accuracy: 0.5230 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 80/1000\n",
      "563/563 [==============================] - 1s 920us/step - loss: 0.2495 - accuracy: 0.5238 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 81/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2490 - accuracy: 0.5310 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 82/1000\n",
      "563/563 [==============================] - 1s 936us/step - loss: 0.2495 - accuracy: 0.5235 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 83/1000\n",
      "563/563 [==============================] - 1s 915us/step - loss: 0.2493 - accuracy: 0.5268 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 84/1000\n",
      "563/563 [==============================] - 1s 952us/step - loss: 0.2497 - accuracy: 0.5202 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 85/1000\n",
      "563/563 [==============================] - 1s 968us/step - loss: 0.2492 - accuracy: 0.5287 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 86/1000\n",
      "563/563 [==============================] - 1s 904us/step - loss: 0.2491 - accuracy: 0.5301 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 87/1000\n",
      "563/563 [==============================] - 1s 918us/step - loss: 0.2495 - accuracy: 0.5223 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 88/1000\n",
      "563/563 [==============================] - 1s 968us/step - loss: 0.2496 - accuracy: 0.5211 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 89/1000\n",
      "563/563 [==============================] - 1s 940us/step - loss: 0.2495 - accuracy: 0.5227 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 90/1000\n",
      "563/563 [==============================] - 1s 925us/step - loss: 0.2492 - accuracy: 0.5282 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 91/1000\n",
      "563/563 [==============================] - 1s 961us/step - loss: 0.2492 - accuracy: 0.5279 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 92/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2492 - accuracy: 0.5284 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 93/1000\n",
      "563/563 [==============================] - 1s 931us/step - loss: 0.2487 - accuracy: 0.5367 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 94/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2492 - accuracy: 0.5280 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 95/1000\n",
      "563/563 [==============================] - 1s 961us/step - loss: 0.2492 - accuracy: 0.5290 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 96/1000\n",
      "563/563 [==============================] - 1s 966us/step - loss: 0.2491 - accuracy: 0.5295 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 97/1000\n",
      "563/563 [==============================] - 1s 938us/step - loss: 0.2492 - accuracy: 0.5283 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 98/1000\n",
      "563/563 [==============================] - 1s 913us/step - loss: 0.2492 - accuracy: 0.5284 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 99/1000\n",
      "563/563 [==============================] - 1s 947us/step - loss: 0.2493 - accuracy: 0.5264 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 100/1000\n",
      "563/563 [==============================] - 1s 952us/step - loss: 0.2490 - accuracy: 0.5314 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 101/1000\n",
      "563/563 [==============================] - 1s 915us/step - loss: 0.2494 - accuracy: 0.5244 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 102/1000\n",
      "563/563 [==============================] - 1s 899us/step - loss: 0.2486 - accuracy: 0.5383 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 103/1000\n",
      "563/563 [==============================] - 1s 963us/step - loss: 0.2491 - accuracy: 0.5307 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 104/1000\n",
      "563/563 [==============================] - 1s 941us/step - loss: 0.2492 - accuracy: 0.5280 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 105/1000\n",
      "563/563 [==============================] - 1s 915us/step - loss: 0.2494 - accuracy: 0.5255 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 106/1000\n",
      "563/563 [==============================] - 1s 945us/step - loss: 0.2496 - accuracy: 0.5216 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 107/1000\n",
      "563/563 [==============================] - 1s 997us/step - loss: 0.2491 - accuracy: 0.5299 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 108/1000\n",
      "563/563 [==============================] - 1s 940us/step - loss: 0.2487 - accuracy: 0.5371 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 109/1000\n",
      "563/563 [==============================] - 1s 940us/step - loss: 0.2487 - accuracy: 0.5363 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 110/1000\n",
      "563/563 [==============================] - 1s 963us/step - loss: 0.2489 - accuracy: 0.5331 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 111/1000\n",
      "563/563 [==============================] - 1s 993us/step - loss: 0.2490 - accuracy: 0.5321 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 112/1000\n",
      "563/563 [==============================] - 1s 918us/step - loss: 0.2492 - accuracy: 0.5290 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 113/1000\n",
      "563/563 [==============================] - 1s 922us/step - loss: 0.2491 - accuracy: 0.5297 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 114/1000\n",
      "563/563 [==============================] - 1s 952us/step - loss: 0.2494 - accuracy: 0.5255 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 115/1000\n",
      "563/563 [==============================] - 1s 979us/step - loss: 0.2493 - accuracy: 0.5263 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 116/1000\n",
      "563/563 [==============================] - 1s 909us/step - loss: 0.2495 - accuracy: 0.5236 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 117/1000\n",
      "563/563 [==============================] - 1s 934us/step - loss: 0.2492 - accuracy: 0.5285 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 118/1000\n",
      "563/563 [==============================] - 1s 999us/step - loss: 0.2494 - accuracy: 0.5251 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 119/1000\n",
      "563/563 [==============================] - 1s 954us/step - loss: 0.2494 - accuracy: 0.5246 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 120/1000\n",
      "563/563 [==============================] - 1s 922us/step - loss: 0.2490 - accuracy: 0.5314 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 121/1000\n",
      "563/563 [==============================] - 1s 909us/step - loss: 0.2495 - accuracy: 0.5238 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 122/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2492 - accuracy: 0.5287 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 123/1000\n",
      "563/563 [==============================] - 1s 931us/step - loss: 0.2489 - accuracy: 0.5335 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 124/1000\n",
      "563/563 [==============================] - 1s 901us/step - loss: 0.2493 - accuracy: 0.5268 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 125/1000\n",
      "563/563 [==============================] - 1s 922us/step - loss: 0.2493 - accuracy: 0.5261 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 126/1000\n",
      "563/563 [==============================] - 1s 988us/step - loss: 0.2491 - accuracy: 0.5304 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 127/1000\n",
      "563/563 [==============================] - 1s 933us/step - loss: 0.2489 - accuracy: 0.5331 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 128/1000\n",
      "563/563 [==============================] - 1s 926us/step - loss: 0.2493 - accuracy: 0.5270 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 129/1000\n",
      "563/563 [==============================] - 1s 965us/step - loss: 0.2492 - accuracy: 0.5283 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 130/1000\n",
      "563/563 [==============================] - 1s 982us/step - loss: 0.2499 - accuracy: 0.5163 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 131/1000\n",
      "563/563 [==============================] - 1s 933us/step - loss: 0.2494 - accuracy: 0.5248 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 132/1000\n",
      "563/563 [==============================] - 1s 954us/step - loss: 0.2490 - accuracy: 0.5317 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 133/1000\n",
      "563/563 [==============================] - 1s 998us/step - loss: 0.2494 - accuracy: 0.5250 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 134/1000\n",
      "563/563 [==============================] - 1s 911us/step - loss: 0.2495 - accuracy: 0.5239 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 135/1000\n",
      "563/563 [==============================] - 1s 943us/step - loss: 0.2494 - accuracy: 0.5246 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 136/1000\n",
      "563/563 [==============================] - 1s 925us/step - loss: 0.2491 - accuracy: 0.5309 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 137/1000\n",
      "563/563 [==============================] - 1s 998us/step - loss: 0.2496 - accuracy: 0.5206 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 138/1000\n",
      "563/563 [==============================] - 1s 909us/step - loss: 0.2493 - accuracy: 0.5259 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 139/1000\n",
      "563/563 [==============================] - 1s 940us/step - loss: 0.2491 - accuracy: 0.5300 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 140/1000\n",
      "563/563 [==============================] - 1s 981us/step - loss: 0.2492 - accuracy: 0.5288 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 141/1000\n",
      "563/563 [==============================] - 1s 968us/step - loss: 0.2495 - accuracy: 0.5239 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 142/1000\n",
      "563/563 [==============================] - 1s 906us/step - loss: 0.2487 - accuracy: 0.5372 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 143/1000\n",
      "563/563 [==============================] - 1s 938us/step - loss: 0.2492 - accuracy: 0.5279 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 144/1000\n",
      "563/563 [==============================] - 1s 990us/step - loss: 0.2492 - accuracy: 0.5288 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 145/1000\n",
      "563/563 [==============================] - 1s 940us/step - loss: 0.2491 - accuracy: 0.5297 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 146/1000\n",
      "563/563 [==============================] - 1s 947us/step - loss: 0.2491 - accuracy: 0.5299 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 147/1000\n",
      "563/563 [==============================] - 1s 925us/step - loss: 0.2490 - accuracy: 0.5317 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 148/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2490 - accuracy: 0.5320 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 149/1000\n",
      "563/563 [==============================] - 1s 943us/step - loss: 0.2493 - accuracy: 0.5271 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 150/1000\n",
      "563/563 [==============================] - 1s 908us/step - loss: 0.2491 - accuracy: 0.5301 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 151/1000\n",
      "563/563 [==============================] - 1s 918us/step - loss: 0.2495 - accuracy: 0.5232 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 152/1000\n",
      "563/563 [==============================] - 1s 977us/step - loss: 0.2496 - accuracy: 0.5215 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 153/1000\n",
      "563/563 [==============================] - 1s 909us/step - loss: 0.2492 - accuracy: 0.5283 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 154/1000\n",
      "563/563 [==============================] - 1s 927us/step - loss: 0.2493 - accuracy: 0.5262 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 155/1000\n",
      "563/563 [==============================] - 1s 938us/step - loss: 0.2495 - accuracy: 0.5235 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 156/1000\n",
      "563/563 [==============================] - 1s 958us/step - loss: 0.2492 - accuracy: 0.5286 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 157/1000\n",
      "563/563 [==============================] - 1s 922us/step - loss: 0.2490 - accuracy: 0.5318 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 158/1000\n",
      "563/563 [==============================] - 1s 949us/step - loss: 0.2493 - accuracy: 0.5268 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 159/1000\n",
      "563/563 [==============================] - 1s 936us/step - loss: 0.2490 - accuracy: 0.5313 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 160/1000\n",
      "563/563 [==============================] - 1s 936us/step - loss: 0.2488 - accuracy: 0.5352 - val_loss: 0.2483 - val_accuracy: 0.5459\n",
      "Epoch 161/1000\n",
      "563/563 [==============================] - 1s 931us/step - loss: 0.2491 - accuracy: 0.5306 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 162/1000\n",
      "563/563 [==============================] - 1s 950us/step - loss: 0.2493 - accuracy: 0.5273 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 163/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2492 - accuracy: 0.5275 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 164/1000\n",
      "563/563 [==============================] - 1s 917us/step - loss: 0.2495 - accuracy: 0.5236 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 165/1000\n",
      "563/563 [==============================] - 1s 927us/step - loss: 0.2493 - accuracy: 0.5272 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 166/1000\n",
      "563/563 [==============================] - 1s 918us/step - loss: 0.2492 - accuracy: 0.5285 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 167/1000\n",
      "563/563 [==============================] - 1s 974us/step - loss: 0.2489 - accuracy: 0.5346 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 168/1000\n",
      "563/563 [==============================] - 1s 917us/step - loss: 0.2490 - accuracy: 0.5313 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 169/1000\n",
      "563/563 [==============================] - 1s 934us/step - loss: 0.2497 - accuracy: 0.5193 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 170/1000\n",
      "563/563 [==============================] - 1s 925us/step - loss: 0.2493 - accuracy: 0.5260 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 171/1000\n",
      "563/563 [==============================] - 1s 965us/step - loss: 0.2493 - accuracy: 0.5259 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 172/1000\n",
      "563/563 [==============================] - 1s 924us/step - loss: 0.2491 - accuracy: 0.5305 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 173/1000\n",
      "563/563 [==============================] - 1s 909us/step - loss: 0.2496 - accuracy: 0.5218 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 174/1000\n",
      "563/563 [==============================] - 1s 936us/step - loss: 0.2494 - accuracy: 0.5257 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 175/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2493 - accuracy: 0.5268 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 176/1000\n",
      "563/563 [==============================] - 1s 908us/step - loss: 0.2490 - accuracy: 0.5324 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 177/1000\n",
      "563/563 [==============================] - 1s 901us/step - loss: 0.2491 - accuracy: 0.5297 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 178/1000\n",
      "563/563 [==============================] - 1s 997us/step - loss: 0.2493 - accuracy: 0.5274 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 179/1000\n",
      "563/563 [==============================] - 1s 933us/step - loss: 0.2496 - accuracy: 0.5220 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 180/1000\n",
      "563/563 [==============================] - 1s 899us/step - loss: 0.2491 - accuracy: 0.5299 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 181/1000\n",
      "563/563 [==============================] - 1s 929us/step - loss: 0.2489 - accuracy: 0.5333 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 182/1000\n",
      "563/563 [==============================] - 1s 974us/step - loss: 0.2494 - accuracy: 0.5255 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 183/1000\n",
      "563/563 [==============================] - 1s 908us/step - loss: 0.2496 - accuracy: 0.5205 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 184/1000\n",
      "563/563 [==============================] - 1s 954us/step - loss: 0.2492 - accuracy: 0.5285 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 185/1000\n",
      "563/563 [==============================] - 1s 952us/step - loss: 0.2493 - accuracy: 0.5264 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 186/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2490 - accuracy: 0.5318 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 187/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2492 - accuracy: 0.5283 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 188/1000\n",
      "563/563 [==============================] - 1s 983us/step - loss: 0.2492 - accuracy: 0.5284 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 189/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2487 - accuracy: 0.5373 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 190/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2492 - accuracy: 0.5279 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 191/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2491 - accuracy: 0.5310 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 192/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2494 - accuracy: 0.5256 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 193/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2494 - accuracy: 0.5242 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 194/1000\n",
      "563/563 [==============================] - 1s 970us/step - loss: 0.2490 - accuracy: 0.5314 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 195/1000\n",
      "563/563 [==============================] - 1s 968us/step - loss: 0.2492 - accuracy: 0.5282 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 196/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2493 - accuracy: 0.5268 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 197/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2494 - accuracy: 0.5249 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 198/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2491 - accuracy: 0.5307 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 199/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2493 - accuracy: 0.5264 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 200/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2493 - accuracy: 0.5268 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 201/1000\n",
      "563/563 [==============================] - 1s 991us/step - loss: 0.2487 - accuracy: 0.5373 - val_loss: 0.2483 - val_accuracy: 0.5459\n",
      "Epoch 202/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2493 - accuracy: 0.5275 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 203/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2490 - accuracy: 0.5313 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 204/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2491 - accuracy: 0.5294 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 205/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2489 - accuracy: 0.5330 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 206/1000\n",
      "563/563 [==============================] - 1s 974us/step - loss: 0.2493 - accuracy: 0.5266 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 207/1000\n",
      "563/563 [==============================] - 1s 981us/step - loss: 0.2491 - accuracy: 0.5302 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 208/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2490 - accuracy: 0.5311 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 209/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2490 - accuracy: 0.5317 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 210/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2496 - accuracy: 0.5207 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 211/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2493 - accuracy: 0.5261 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 212/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2496 - accuracy: 0.5212 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 213/1000\n",
      "563/563 [==============================] - 1s 965us/step - loss: 0.2494 - accuracy: 0.5254 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 214/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2489 - accuracy: 0.5334 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 215/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2491 - accuracy: 0.5298 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 216/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2491 - accuracy: 0.5298 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 217/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2491 - accuracy: 0.5309 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 218/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2493 - accuracy: 0.5264 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 219/1000\n",
      "563/563 [==============================] - 1s 909us/step - loss: 0.2489 - accuracy: 0.5336 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 220/1000\n",
      "563/563 [==============================] - 1s 909us/step - loss: 0.2497 - accuracy: 0.5194 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 221/1000\n",
      "563/563 [==============================] - 1s 981us/step - loss: 0.2492 - accuracy: 0.5287 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 222/1000\n",
      "563/563 [==============================] - 1s 922us/step - loss: 0.2494 - accuracy: 0.5257 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 223/1000\n",
      "563/563 [==============================] - 1s 915us/step - loss: 0.2494 - accuracy: 0.5242 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 224/1000\n",
      "563/563 [==============================] - 1s 918us/step - loss: 0.2491 - accuracy: 0.5303 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 225/1000\n",
      "563/563 [==============================] - 1s 991us/step - loss: 0.2488 - accuracy: 0.5353 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 226/1000\n",
      "563/563 [==============================] - 1s 897us/step - loss: 0.2491 - accuracy: 0.5297 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 227/1000\n",
      "563/563 [==============================] - 1s 902us/step - loss: 0.2493 - accuracy: 0.5258 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 228/1000\n",
      "563/563 [==============================] - 1s 906us/step - loss: 0.2489 - accuracy: 0.5333 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 229/1000\n",
      "563/563 [==============================] - 1s 981us/step - loss: 0.2489 - accuracy: 0.5329 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 230/1000\n",
      "563/563 [==============================] - 1s 911us/step - loss: 0.2491 - accuracy: 0.5302 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 231/1000\n",
      "563/563 [==============================] - 1s 917us/step - loss: 0.2491 - accuracy: 0.5299 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 232/1000\n",
      "563/563 [==============================] - 1s 941us/step - loss: 0.2492 - accuracy: 0.5288 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 233/1000\n",
      "563/563 [==============================] - 1s 949us/step - loss: 0.2490 - accuracy: 0.5325 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 234/1000\n",
      "563/563 [==============================] - 1s 913us/step - loss: 0.2493 - accuracy: 0.5259 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 235/1000\n",
      "563/563 [==============================] - 1s 917us/step - loss: 0.2491 - accuracy: 0.5306 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 236/1000\n",
      "563/563 [==============================] - 1s 963us/step - loss: 0.2491 - accuracy: 0.5296 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 237/1000\n",
      "563/563 [==============================] - 1s 959us/step - loss: 0.2495 - accuracy: 0.5231 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 238/1000\n",
      "563/563 [==============================] - 1s 902us/step - loss: 0.2490 - accuracy: 0.5310 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 239/1000\n",
      "563/563 [==============================] - 1s 909us/step - loss: 0.2491 - accuracy: 0.5296 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 240/1000\n",
      "563/563 [==============================] - 1s 963us/step - loss: 0.2493 - accuracy: 0.5271 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 241/1000\n",
      "563/563 [==============================] - 1s 974us/step - loss: 0.2492 - accuracy: 0.5283 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 242/1000\n",
      "563/563 [==============================] - 1s 920us/step - loss: 0.2496 - accuracy: 0.5216 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 243/1000\n",
      "563/563 [==============================] - 1s 911us/step - loss: 0.2489 - accuracy: 0.5330 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 244/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2491 - accuracy: 0.5306 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 245/1000\n",
      "563/563 [==============================] - 1s 912us/step - loss: 0.2494 - accuracy: 0.5247 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 246/1000\n",
      "563/563 [==============================] - 1s 909us/step - loss: 0.2492 - accuracy: 0.5281 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 247/1000\n",
      "563/563 [==============================] - 1s 941us/step - loss: 0.2495 - accuracy: 0.5223 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 248/1000\n",
      "563/563 [==============================] - 1s 977us/step - loss: 0.2490 - accuracy: 0.5321 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 249/1000\n",
      "563/563 [==============================] - 1s 917us/step - loss: 0.2490 - accuracy: 0.5317 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 250/1000\n",
      "563/563 [==============================] - 1s 906us/step - loss: 0.2492 - accuracy: 0.5281 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 251/1000\n",
      "563/563 [==============================] - 1s 974us/step - loss: 0.2490 - accuracy: 0.5311 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 252/1000\n",
      "563/563 [==============================] - 1s 984us/step - loss: 0.2493 - accuracy: 0.5261 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 253/1000\n",
      "563/563 [==============================] - 1s 927us/step - loss: 0.2490 - accuracy: 0.5312 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 254/1000\n",
      "563/563 [==============================] - 1s 931us/step - loss: 0.2492 - accuracy: 0.5291 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 255/1000\n",
      "563/563 [==============================] - 1s 972us/step - loss: 0.2490 - accuracy: 0.5312 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 256/1000\n",
      "563/563 [==============================] - 1s 924us/step - loss: 0.2489 - accuracy: 0.5344 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 257/1000\n",
      "563/563 [==============================] - 1s 915us/step - loss: 0.2495 - accuracy: 0.5226 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 258/1000\n",
      "563/563 [==============================] - 1s 943us/step - loss: 0.2492 - accuracy: 0.5292 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 259/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2493 - accuracy: 0.5262 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 260/1000\n",
      "563/563 [==============================] - 1s 918us/step - loss: 0.2491 - accuracy: 0.5304 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 261/1000\n",
      "563/563 [==============================] - 1s 917us/step - loss: 0.2488 - accuracy: 0.5362 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 262/1000\n",
      "563/563 [==============================] - 1s 927us/step - loss: 0.2489 - accuracy: 0.5332 - val_loss: 0.2483 - val_accuracy: 0.5459\n",
      "Epoch 263/1000\n",
      "563/563 [==============================] - 1s 975us/step - loss: 0.2492 - accuracy: 0.5292 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 264/1000\n",
      "563/563 [==============================] - 1s 908us/step - loss: 0.2492 - accuracy: 0.5288 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 265/1000\n",
      "563/563 [==============================] - 1s 917us/step - loss: 0.2492 - accuracy: 0.5282 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 266/1000\n",
      "563/563 [==============================] - 1s 947us/step - loss: 0.2491 - accuracy: 0.5298 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 267/1000\n",
      "563/563 [==============================] - 1s 975us/step - loss: 0.2490 - accuracy: 0.5325 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 268/1000\n",
      "563/563 [==============================] - 1s 988us/step - loss: 0.2493 - accuracy: 0.5274 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 269/1000\n",
      "563/563 [==============================] - 1s 925us/step - loss: 0.2487 - accuracy: 0.5371 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 270/1000\n",
      "563/563 [==============================] - 1s 990us/step - loss: 0.2493 - accuracy: 0.5271 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 271/1000\n",
      "563/563 [==============================] - 1s 918us/step - loss: 0.2490 - accuracy: 0.5323 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 272/1000\n",
      "563/563 [==============================] - 1s 908us/step - loss: 0.2494 - accuracy: 0.5256 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 273/1000\n",
      "563/563 [==============================] - 1s 908us/step - loss: 0.2490 - accuracy: 0.5314 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 274/1000\n",
      "563/563 [==============================] - 1s 990us/step - loss: 0.2490 - accuracy: 0.5328 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 275/1000\n",
      "563/563 [==============================] - 1s 936us/step - loss: 0.2489 - accuracy: 0.5329 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 276/1000\n",
      "563/563 [==============================] - 1s 892us/step - loss: 0.2487 - accuracy: 0.5366 - val_loss: 0.2483 - val_accuracy: 0.5459\n",
      "Epoch 277/1000\n",
      "563/563 [==============================] - 1s 920us/step - loss: 0.2496 - accuracy: 0.5218 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 278/1000\n",
      "563/563 [==============================] - 1s 974us/step - loss: 0.2489 - accuracy: 0.5338 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 279/1000\n",
      "563/563 [==============================] - 1s 917us/step - loss: 0.2494 - accuracy: 0.5254 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 280/1000\n",
      "563/563 [==============================] - 1s 941us/step - loss: 0.2494 - accuracy: 0.5246 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 281/1000\n",
      "563/563 [==============================] - 1s 922us/step - loss: 0.2491 - accuracy: 0.5305 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 282/1000\n",
      "563/563 [==============================] - 1s 984us/step - loss: 0.2490 - accuracy: 0.5318 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 283/1000\n",
      "563/563 [==============================] - 1s 901us/step - loss: 0.2495 - accuracy: 0.5233 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 284/1000\n",
      "563/563 [==============================] - 1s 929us/step - loss: 0.2490 - accuracy: 0.5312 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 285/1000\n",
      "563/563 [==============================] - 1s 943us/step - loss: 0.2498 - accuracy: 0.5180 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 286/1000\n",
      "563/563 [==============================] - 1s 934us/step - loss: 0.2492 - accuracy: 0.5277 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 287/1000\n",
      "563/563 [==============================] - 1s 911us/step - loss: 0.2493 - accuracy: 0.5275 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 288/1000\n",
      "563/563 [==============================] - 1s 899us/step - loss: 0.2494 - accuracy: 0.5251 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 289/1000\n",
      "563/563 [==============================] - 1s 970us/step - loss: 0.2495 - accuracy: 0.5231 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 290/1000\n",
      "563/563 [==============================] - 1s 924us/step - loss: 0.2492 - accuracy: 0.5276 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 291/1000\n",
      "563/563 [==============================] - 1s 917us/step - loss: 0.2489 - accuracy: 0.5340 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 292/1000\n",
      "563/563 [==============================] - 1s 988us/step - loss: 0.2490 - accuracy: 0.5316 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 293/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2494 - accuracy: 0.5241 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 294/1000\n",
      "563/563 [==============================] - 1s 988us/step - loss: 0.2495 - accuracy: 0.5239 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 295/1000\n",
      "563/563 [==============================] - 1s 906us/step - loss: 0.2494 - accuracy: 0.5242 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 296/1000\n",
      "563/563 [==============================] - 1s 938us/step - loss: 0.2492 - accuracy: 0.5291 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 297/1000\n",
      "563/563 [==============================] - 1s 981us/step - loss: 0.2491 - accuracy: 0.5296 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 298/1000\n",
      "563/563 [==============================] - 1s 909us/step - loss: 0.2492 - accuracy: 0.5284 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 299/1000\n",
      "563/563 [==============================] - 1s 901us/step - loss: 0.2497 - accuracy: 0.5193 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 300/1000\n",
      "563/563 [==============================] - 1s 929us/step - loss: 0.2493 - accuracy: 0.5263 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 301/1000\n",
      "563/563 [==============================] - 1s 936us/step - loss: 0.2492 - accuracy: 0.5284 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 302/1000\n",
      "563/563 [==============================] - 1s 908us/step - loss: 0.2488 - accuracy: 0.5356 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 303/1000\n",
      "563/563 [==============================] - 1s 906us/step - loss: 0.2489 - accuracy: 0.5334 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 304/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2492 - accuracy: 0.5287 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 305/1000\n",
      "563/563 [==============================] - 1s 956us/step - loss: 0.2491 - accuracy: 0.5309 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 306/1000\n",
      "563/563 [==============================] - 1s 901us/step - loss: 0.2492 - accuracy: 0.5283 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 307/1000\n",
      "563/563 [==============================] - 1s 911us/step - loss: 0.2492 - accuracy: 0.5277 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 308/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2485 - accuracy: 0.5395 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 309/1000\n",
      "563/563 [==============================] - 1s 927us/step - loss: 0.2490 - accuracy: 0.5319 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 310/1000\n",
      "563/563 [==============================] - 1s 933us/step - loss: 0.2495 - accuracy: 0.5221 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 311/1000\n",
      "563/563 [==============================] - 1s 933us/step - loss: 0.2494 - accuracy: 0.5248 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 312/1000\n",
      "563/563 [==============================] - 1s 963us/step - loss: 0.2493 - accuracy: 0.5272 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 313/1000\n",
      "563/563 [==============================] - 1s 981us/step - loss: 0.2489 - accuracy: 0.5342 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 314/1000\n",
      "563/563 [==============================] - 1s 901us/step - loss: 0.2493 - accuracy: 0.5272 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 315/1000\n",
      "563/563 [==============================] - 1s 981us/step - loss: 0.2493 - accuracy: 0.5275 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 316/1000\n",
      "563/563 [==============================] - 1s 936us/step - loss: 0.2495 - accuracy: 0.5236 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 317/1000\n",
      "563/563 [==============================] - 1s 936us/step - loss: 0.2491 - accuracy: 0.5296 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 318/1000\n",
      "563/563 [==============================] - 1s 924us/step - loss: 0.2493 - accuracy: 0.5270 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 319/1000\n",
      "563/563 [==============================] - 1s 975us/step - loss: 0.2491 - accuracy: 0.5299 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 320/1000\n",
      "563/563 [==============================] - 1s 913us/step - loss: 0.2492 - accuracy: 0.5289 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 321/1000\n",
      "563/563 [==============================] - 1s 918us/step - loss: 0.2489 - accuracy: 0.5341 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 322/1000\n",
      "563/563 [==============================] - 1s 899us/step - loss: 0.2491 - accuracy: 0.5307 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 323/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2495 - accuracy: 0.5234 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 324/1000\n",
      "563/563 [==============================] - 1s 915us/step - loss: 0.2492 - accuracy: 0.5277 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 325/1000\n",
      "563/563 [==============================] - 1s 918us/step - loss: 0.2492 - accuracy: 0.5280 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 326/1000\n",
      "563/563 [==============================] - 1s 920us/step - loss: 0.2493 - accuracy: 0.5265 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 327/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2492 - accuracy: 0.5290 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 328/1000\n",
      "563/563 [==============================] - 1s 972us/step - loss: 0.2492 - accuracy: 0.5286 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 329/1000\n",
      "563/563 [==============================] - 1s 933us/step - loss: 0.2492 - accuracy: 0.5284 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 330/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2494 - accuracy: 0.5249 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 331/1000\n",
      "563/563 [==============================] - 1s 936us/step - loss: 0.2491 - accuracy: 0.5295 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 332/1000\n",
      "563/563 [==============================] - 1s 917us/step - loss: 0.2494 - accuracy: 0.5251 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 333/1000\n",
      "563/563 [==============================] - 1s 909us/step - loss: 0.2492 - accuracy: 0.5284 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 334/1000\n",
      "563/563 [==============================] - 1s 966us/step - loss: 0.2493 - accuracy: 0.5262 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 335/1000\n",
      "563/563 [==============================] - 1s 915us/step - loss: 0.2488 - accuracy: 0.5361 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 336/1000\n",
      "563/563 [==============================] - 1s 934us/step - loss: 0.2494 - accuracy: 0.5253 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 337/1000\n",
      "563/563 [==============================] - 1s 924us/step - loss: 0.2489 - accuracy: 0.5335 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 338/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2497 - accuracy: 0.5199 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 339/1000\n",
      "563/563 [==============================] - 1s 913us/step - loss: 0.2492 - accuracy: 0.5289 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 340/1000\n",
      "563/563 [==============================] - 1s 922us/step - loss: 0.2491 - accuracy: 0.5300 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 341/1000\n",
      "563/563 [==============================] - 1s 909us/step - loss: 0.2491 - accuracy: 0.5296 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 342/1000\n",
      "563/563 [==============================] - 1s 965us/step - loss: 0.2492 - accuracy: 0.5288 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 343/1000\n",
      "563/563 [==============================] - 1s 908us/step - loss: 0.2493 - accuracy: 0.5268 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 344/1000\n",
      "563/563 [==============================] - 1s 963us/step - loss: 0.2493 - accuracy: 0.5258 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 345/1000\n",
      "563/563 [==============================] - 1s 933us/step - loss: 0.2492 - accuracy: 0.5279 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 346/1000\n",
      "563/563 [==============================] - 1s 952us/step - loss: 0.2493 - accuracy: 0.5263 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 347/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2495 - accuracy: 0.5240 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 348/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2494 - accuracy: 0.5245 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 349/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2491 - accuracy: 0.5304 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 350/1000\n",
      "563/563 [==============================] - 1s 991us/step - loss: 0.2492 - accuracy: 0.5289 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 351/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2493 - accuracy: 0.5264 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 352/1000\n",
      "563/563 [==============================] - 1s 988us/step - loss: 0.2495 - accuracy: 0.5227 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 353/1000\n",
      "563/563 [==============================] - 1s 952us/step - loss: 0.2492 - accuracy: 0.5292 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 354/1000\n",
      "563/563 [==============================] - 1s 925us/step - loss: 0.2491 - accuracy: 0.5299 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 355/1000\n",
      "563/563 [==============================] - 1s 890us/step - loss: 0.2493 - accuracy: 0.5275 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 356/1000\n",
      "563/563 [==============================] - 1s 975us/step - loss: 0.2497 - accuracy: 0.5202 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 357/1000\n",
      "563/563 [==============================] - 1s 913us/step - loss: 0.2486 - accuracy: 0.5386 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 358/1000\n",
      "563/563 [==============================] - 1s 905us/step - loss: 0.2492 - accuracy: 0.5287 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 359/1000\n",
      "563/563 [==============================] - 1s 908us/step - loss: 0.2493 - accuracy: 0.5259 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 360/1000\n",
      "563/563 [==============================] - 1s 968us/step - loss: 0.2492 - accuracy: 0.5281 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 361/1000\n",
      "563/563 [==============================] - 1s 906us/step - loss: 0.2494 - accuracy: 0.5250 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 362/1000\n",
      "563/563 [==============================] - 1s 925us/step - loss: 0.2491 - accuracy: 0.5308 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 363/1000\n",
      "563/563 [==============================] - 1s 945us/step - loss: 0.2490 - accuracy: 0.5320 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 364/1000\n",
      "563/563 [==============================] - 1s 974us/step - loss: 0.2494 - accuracy: 0.5251 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 365/1000\n",
      "563/563 [==============================] - 1s 895us/step - loss: 0.2491 - accuracy: 0.5295 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 366/1000\n",
      "563/563 [==============================] - 1s 929us/step - loss: 0.2496 - accuracy: 0.5204 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 367/1000\n",
      "563/563 [==============================] - 1s 945us/step - loss: 0.2490 - accuracy: 0.5322 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 368/1000\n",
      "563/563 [==============================] - 1s 948us/step - loss: 0.2493 - accuracy: 0.5262 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 369/1000\n",
      "563/563 [==============================] - 1s 918us/step - loss: 0.2489 - accuracy: 0.5332 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 370/1000\n",
      "563/563 [==============================] - 1s 917us/step - loss: 0.2493 - accuracy: 0.5269 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 371/1000\n",
      "563/563 [==============================] - 1s 963us/step - loss: 0.2491 - accuracy: 0.5304 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 372/1000\n",
      "563/563 [==============================] - 1s 938us/step - loss: 0.2497 - accuracy: 0.5189 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 373/1000\n",
      "563/563 [==============================] - 1s 922us/step - loss: 0.2491 - accuracy: 0.5307 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 374/1000\n",
      "563/563 [==============================] - 1s 899us/step - loss: 0.2494 - accuracy: 0.5252 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 375/1000\n",
      "563/563 [==============================] - 1s 974us/step - loss: 0.2496 - accuracy: 0.5214 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 376/1000\n",
      "563/563 [==============================] - 1s 913us/step - loss: 0.2493 - accuracy: 0.5265 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 377/1000\n",
      "563/563 [==============================] - 1s 895us/step - loss: 0.2488 - accuracy: 0.5352 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 378/1000\n",
      "563/563 [==============================] - 1s 906us/step - loss: 0.2491 - accuracy: 0.5310 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 379/1000\n",
      "563/563 [==============================] - 1s 956us/step - loss: 0.2490 - accuracy: 0.5317 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 380/1000\n",
      "563/563 [==============================] - 1s 933us/step - loss: 0.2495 - accuracy: 0.5231 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 381/1000\n",
      "563/563 [==============================] - 1s 902us/step - loss: 0.2491 - accuracy: 0.5300 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 382/1000\n",
      "563/563 [==============================] - 1s 913us/step - loss: 0.2494 - accuracy: 0.5246 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 383/1000\n",
      "563/563 [==============================] - 1s 952us/step - loss: 0.2496 - accuracy: 0.5222 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 384/1000\n",
      "563/563 [==============================] - 1s 909us/step - loss: 0.2489 - accuracy: 0.5343 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 385/1000\n",
      "563/563 [==============================] - 1s 913us/step - loss: 0.2494 - accuracy: 0.5248 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 386/1000\n",
      "563/563 [==============================] - 1s 931us/step - loss: 0.2492 - accuracy: 0.5279 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 387/1000\n",
      "563/563 [==============================] - 1s 965us/step - loss: 0.2494 - accuracy: 0.5246 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 388/1000\n",
      "563/563 [==============================] - 1s 924us/step - loss: 0.2492 - accuracy: 0.5289 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 389/1000\n",
      "563/563 [==============================] - 1s 929us/step - loss: 0.2491 - accuracy: 0.5303 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 390/1000\n",
      "563/563 [==============================] - 1s 950us/step - loss: 0.2492 - accuracy: 0.5283 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 391/1000\n",
      "563/563 [==============================] - 1s 927us/step - loss: 0.2493 - accuracy: 0.5260 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 392/1000\n",
      "563/563 [==============================] - 1s 995us/step - loss: 0.2494 - accuracy: 0.5246 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 393/1000\n",
      "563/563 [==============================] - 1s 918us/step - loss: 0.2492 - accuracy: 0.5281 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 394/1000\n",
      "563/563 [==============================] - 1s 986us/step - loss: 0.2493 - accuracy: 0.5269 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 395/1000\n",
      "563/563 [==============================] - 1s 913us/step - loss: 0.2494 - accuracy: 0.5251 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 396/1000\n",
      "563/563 [==============================] - 1s 899us/step - loss: 0.2491 - accuracy: 0.5300 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 397/1000\n",
      "563/563 [==============================] - 1s 915us/step - loss: 0.2489 - accuracy: 0.5331 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 398/1000\n",
      "563/563 [==============================] - 1s 986us/step - loss: 0.2493 - accuracy: 0.5258 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 399/1000\n",
      "563/563 [==============================] - 1s 897us/step - loss: 0.2492 - accuracy: 0.5293 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 400/1000\n",
      "563/563 [==============================] - 1s 936us/step - loss: 0.2497 - accuracy: 0.5198 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 401/1000\n",
      "563/563 [==============================] - 1s 909us/step - loss: 0.2494 - accuracy: 0.5258 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 402/1000\n",
      "563/563 [==============================] - 1s 952us/step - loss: 0.2492 - accuracy: 0.5276 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 403/1000\n",
      "563/563 [==============================] - 1s 908us/step - loss: 0.2493 - accuracy: 0.5269 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 404/1000\n",
      "563/563 [==============================] - 1s 902us/step - loss: 0.2493 - accuracy: 0.5271 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 405/1000\n",
      "563/563 [==============================] - 1s 945us/step - loss: 0.2491 - accuracy: 0.5296 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 406/1000\n",
      "563/563 [==============================] - 1s 949us/step - loss: 0.2496 - accuracy: 0.5218 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 407/1000\n",
      "563/563 [==============================] - 1s 915us/step - loss: 0.2490 - accuracy: 0.5311 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 408/1000\n",
      "563/563 [==============================] - 1s 906us/step - loss: 0.2487 - accuracy: 0.5364 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 409/1000\n",
      "563/563 [==============================] - 1s 954us/step - loss: 0.2495 - accuracy: 0.5223 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 410/1000\n",
      "563/563 [==============================] - 1s 961us/step - loss: 0.2492 - accuracy: 0.5278 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 411/1000\n",
      "563/563 [==============================] - 1s 890us/step - loss: 0.2492 - accuracy: 0.5275 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 412/1000\n",
      "563/563 [==============================] - 1s 920us/step - loss: 0.2495 - accuracy: 0.5236 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 413/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2493 - accuracy: 0.5274 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 414/1000\n",
      "563/563 [==============================] - 1s 917us/step - loss: 0.2492 - accuracy: 0.5284 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 415/1000\n",
      "563/563 [==============================] - 1s 915us/step - loss: 0.2492 - accuracy: 0.5282 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 416/1000\n",
      "563/563 [==============================] - 1s 902us/step - loss: 0.2493 - accuracy: 0.5270 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 417/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2495 - accuracy: 0.5238 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 418/1000\n",
      "563/563 [==============================] - ETA: 0s - loss: 0.2489 - accuracy: 0.53 - 1s 929us/step - loss: 0.2489 - accuracy: 0.5331 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 419/1000\n",
      "563/563 [==============================] - 1s 899us/step - loss: 0.2491 - accuracy: 0.5298 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 420/1000\n",
      "563/563 [==============================] - ETA: 0s - loss: 0.2488 - accuracy: 0.53 - 1s 952us/step - loss: 0.2488 - accuracy: 0.5356 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 421/1000\n",
      "563/563 [==============================] - 1s 975us/step - loss: 0.2488 - accuracy: 0.5360 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 422/1000\n",
      "563/563 [==============================] - 1s 915us/step - loss: 0.2492 - accuracy: 0.5278 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 423/1000\n",
      "563/563 [==============================] - 1s 936us/step - loss: 0.2491 - accuracy: 0.5311 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 424/1000\n",
      "563/563 [==============================] - 1s 947us/step - loss: 0.2492 - accuracy: 0.5281 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 425/1000\n",
      "563/563 [==============================] - 1s 974us/step - loss: 0.2491 - accuracy: 0.5306 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 426/1000\n",
      "563/563 [==============================] - 1s 965us/step - loss: 0.2493 - accuracy: 0.5264 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 427/1000\n",
      "563/563 [==============================] - 1s 934us/step - loss: 0.2495 - accuracy: 0.5233 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 428/1000\n",
      "563/563 [==============================] - 1s 970us/step - loss: 0.2493 - accuracy: 0.5264 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 429/1000\n",
      "563/563 [==============================] - 1s 949us/step - loss: 0.2490 - accuracy: 0.5326 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 430/1000\n",
      "563/563 [==============================] - 1s 922us/step - loss: 0.2492 - accuracy: 0.5278 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 431/1000\n",
      "563/563 [==============================] - 1s 908us/step - loss: 0.2495 - accuracy: 0.5235 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 432/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2491 - accuracy: 0.5300 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 433/1000\n",
      "563/563 [==============================] - 1s 911us/step - loss: 0.2492 - accuracy: 0.5290 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 434/1000\n",
      "563/563 [==============================] - 1s 918us/step - loss: 0.2494 - accuracy: 0.5256 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 435/1000\n",
      "563/563 [==============================] - 1s 933us/step - loss: 0.2490 - accuracy: 0.5313 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 436/1000\n",
      "563/563 [==============================] - 1s 940us/step - loss: 0.2492 - accuracy: 0.5283 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 437/1000\n",
      "563/563 [==============================] - 1s 915us/step - loss: 0.2494 - accuracy: 0.5243 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 438/1000\n",
      "563/563 [==============================] - 1s 904us/step - loss: 0.2494 - accuracy: 0.5253 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 439/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2489 - accuracy: 0.5334 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 440/1000\n",
      "563/563 [==============================] - 1s 961us/step - loss: 0.2490 - accuracy: 0.5327 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 441/1000\n",
      "563/563 [==============================] - 1s 918us/step - loss: 0.2491 - accuracy: 0.5311 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 442/1000\n",
      "563/563 [==============================] - 1s 892us/step - loss: 0.2494 - accuracy: 0.5248 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 443/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2492 - accuracy: 0.5278 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 444/1000\n",
      "563/563 [==============================] - 1s 927us/step - loss: 0.2492 - accuracy: 0.5286 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 445/1000\n",
      "563/563 [==============================] - 1s 911us/step - loss: 0.2494 - accuracy: 0.5252 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 446/1000\n",
      "563/563 [==============================] - 1s 899us/step - loss: 0.2490 - accuracy: 0.5312 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 447/1000\n",
      "563/563 [==============================] - 1s 974us/step - loss: 0.2492 - accuracy: 0.5290 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 448/1000\n",
      "563/563 [==============================] - 1s 911us/step - loss: 0.2491 - accuracy: 0.5298 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 449/1000\n",
      "563/563 [==============================] - 1s 917us/step - loss: 0.2490 - accuracy: 0.5313 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 450/1000\n",
      "563/563 [==============================] - 1s 911us/step - loss: 0.2493 - accuracy: 0.5263 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 451/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2490 - accuracy: 0.5315 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 452/1000\n",
      "563/563 [==============================] - 1s 924us/step - loss: 0.2493 - accuracy: 0.5258 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 453/1000\n",
      "563/563 [==============================] - 1s 901us/step - loss: 0.2494 - accuracy: 0.5251 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 454/1000\n",
      "563/563 [==============================] - 1s 933us/step - loss: 0.2491 - accuracy: 0.5305 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 455/1000\n",
      "563/563 [==============================] - 1s 979us/step - loss: 0.2488 - accuracy: 0.5358 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 456/1000\n",
      "563/563 [==============================] - 1s 924us/step - loss: 0.2490 - accuracy: 0.5320 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 457/1000\n",
      "563/563 [==============================] - 1s 908us/step - loss: 0.2492 - accuracy: 0.5284 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 458/1000\n",
      "563/563 [==============================] - 1s 949us/step - loss: 0.2489 - accuracy: 0.5344 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 459/1000\n",
      "563/563 [==============================] - 1s 929us/step - loss: 0.2493 - accuracy: 0.5267 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 460/1000\n",
      "563/563 [==============================] - 1s 920us/step - loss: 0.2490 - accuracy: 0.5326 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 461/1000\n",
      "563/563 [==============================] - 1s 895us/step - loss: 0.2493 - accuracy: 0.5271 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 462/1000\n",
      "563/563 [==============================] - 1s 974us/step - loss: 0.2494 - accuracy: 0.5244 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 463/1000\n",
      "563/563 [==============================] - 1s 921us/step - loss: 0.2493 - accuracy: 0.5271 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 464/1000\n",
      "563/563 [==============================] - 1s 920us/step - loss: 0.2492 - accuracy: 0.5289 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 465/1000\n",
      "563/563 [==============================] - 1s 915us/step - loss: 0.2491 - accuracy: 0.5300 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 466/1000\n",
      "563/563 [==============================] - 1s 974us/step - loss: 0.2496 - accuracy: 0.5208 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 467/1000\n",
      "563/563 [==============================] - 1s 904us/step - loss: 0.2492 - accuracy: 0.5279 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 468/1000\n",
      "563/563 [==============================] - 1s 913us/step - loss: 0.2492 - accuracy: 0.5278 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 469/1000\n",
      "563/563 [==============================] - 1s 925us/step - loss: 0.2493 - accuracy: 0.5275 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 470/1000\n",
      "563/563 [==============================] - 1s 966us/step - loss: 0.2490 - accuracy: 0.5323 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 471/1000\n",
      "563/563 [==============================] - 1s 992us/step - loss: 0.2491 - accuracy: 0.5306 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 472/1000\n",
      "563/563 [==============================] - 1s 899us/step - loss: 0.2493 - accuracy: 0.5274 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 473/1000\n",
      "563/563 [==============================] - 1s 922us/step - loss: 0.2493 - accuracy: 0.5259 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 474/1000\n",
      "563/563 [==============================] - 1s 952us/step - loss: 0.2492 - accuracy: 0.5279 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 475/1000\n",
      "563/563 [==============================] - 1s 893us/step - loss: 0.2490 - accuracy: 0.5315 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 476/1000\n",
      "563/563 [==============================] - 1s 917us/step - loss: 0.2492 - accuracy: 0.5284 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 477/1000\n",
      "563/563 [==============================] - 1s 938us/step - loss: 0.2488 - accuracy: 0.5353 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 478/1000\n",
      "563/563 [==============================] - 1s 931us/step - loss: 0.2491 - accuracy: 0.5301 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 479/1000\n",
      "563/563 [==============================] - 1s 918us/step - loss: 0.2493 - accuracy: 0.5274 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 480/1000\n",
      "563/563 [==============================] - 1s 947us/step - loss: 0.2496 - accuracy: 0.5218 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 481/1000\n",
      "563/563 [==============================] - 1s 956us/step - loss: 0.2492 - accuracy: 0.5288 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 482/1000\n",
      "563/563 [==============================] - 1s 920us/step - loss: 0.2491 - accuracy: 0.5301 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 483/1000\n",
      "563/563 [==============================] - 1s 911us/step - loss: 0.2493 - accuracy: 0.5261 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 484/1000\n",
      "563/563 [==============================] - 1s 925us/step - loss: 0.2492 - accuracy: 0.5286 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 485/1000\n",
      "563/563 [==============================] - 1s 981us/step - loss: 0.2493 - accuracy: 0.5263 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 486/1000\n",
      "563/563 [==============================] - 1s 908us/step - loss: 0.2490 - accuracy: 0.5315 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 487/1000\n",
      "563/563 [==============================] - 1s 975us/step - loss: 0.2494 - accuracy: 0.5243 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 488/1000\n",
      "563/563 [==============================] - 1s 985us/step - loss: 0.2491 - accuracy: 0.5308 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 489/1000\n",
      "563/563 [==============================] - 1s 970us/step - loss: 0.2489 - accuracy: 0.5334 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 490/1000\n",
      "563/563 [==============================] - 1s 925us/step - loss: 0.2493 - accuracy: 0.5267 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 491/1000\n",
      "563/563 [==============================] - 1s 918us/step - loss: 0.2493 - accuracy: 0.5271 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 492/1000\n",
      "563/563 [==============================] - 1s 922us/step - loss: 0.2495 - accuracy: 0.5235 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 493/1000\n",
      "563/563 [==============================] - 1s 972us/step - loss: 0.2493 - accuracy: 0.5264 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 494/1000\n",
      "563/563 [==============================] - 1s 924us/step - loss: 0.2493 - accuracy: 0.5259 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 495/1000\n",
      "563/563 [==============================] - 1s 909us/step - loss: 0.2487 - accuracy: 0.5368 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 496/1000\n",
      "563/563 [==============================] - 1s 940us/step - loss: 0.2490 - accuracy: 0.5315 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 497/1000\n",
      "563/563 [==============================] - 1s 958us/step - loss: 0.2493 - accuracy: 0.5264 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 498/1000\n",
      "563/563 [==============================] - 1s 908us/step - loss: 0.2496 - accuracy: 0.5209 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 499/1000\n",
      "563/563 [==============================] - 1s 901us/step - loss: 0.2494 - accuracy: 0.5241 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 500/1000\n",
      "563/563 [==============================] - 1s 994us/step - loss: 0.2494 - accuracy: 0.5249 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 501/1000\n",
      "563/563 [==============================] - 1s 918us/step - loss: 0.2492 - accuracy: 0.5290 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 502/1000\n",
      "563/563 [==============================] - 1s 902us/step - loss: 0.2495 - accuracy: 0.5236 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 503/1000\n",
      "563/563 [==============================] - 1s 917us/step - loss: 0.2490 - accuracy: 0.5312 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 504/1000\n",
      "563/563 [==============================] - 1s 972us/step - loss: 0.2493 - accuracy: 0.5259 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 505/1000\n",
      "563/563 [==============================] - 1s 925us/step - loss: 0.2492 - accuracy: 0.5285 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 506/1000\n",
      "563/563 [==============================] - 1s 901us/step - loss: 0.2488 - accuracy: 0.5355 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 507/1000\n",
      "563/563 [==============================] - 1s 920us/step - loss: 0.2489 - accuracy: 0.5336 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 508/1000\n",
      "563/563 [==============================] - 1s 966us/step - loss: 0.2489 - accuracy: 0.5339 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 509/1000\n",
      "563/563 [==============================] - 1s 899us/step - loss: 0.2488 - accuracy: 0.5359 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 510/1000\n",
      "563/563 [==============================] - 1s 927us/step - loss: 0.2493 - accuracy: 0.5262 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 511/1000\n",
      "563/563 [==============================] - 1s 952us/step - loss: 0.2493 - accuracy: 0.5268 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 512/1000\n",
      "563/563 [==============================] - 1s 931us/step - loss: 0.2493 - accuracy: 0.5260 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 513/1000\n",
      "563/563 [==============================] - 1s 909us/step - loss: 0.2490 - accuracy: 0.5323 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 514/1000\n",
      "563/563 [==============================] - 1s 895us/step - loss: 0.2488 - accuracy: 0.5348 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 515/1000\n",
      "563/563 [==============================] - 1s 954us/step - loss: 0.2492 - accuracy: 0.5277 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 516/1000\n",
      "563/563 [==============================] - 1s 954us/step - loss: 0.2497 - accuracy: 0.5189 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 517/1000\n",
      "563/563 [==============================] - 1s 909us/step - loss: 0.2488 - accuracy: 0.5346 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 518/1000\n",
      "563/563 [==============================] - 1s 917us/step - loss: 0.2491 - accuracy: 0.5309 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 519/1000\n",
      "563/563 [==============================] - 1s 972us/step - loss: 0.2494 - accuracy: 0.5250 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 520/1000\n",
      "563/563 [==============================] - 1s 983us/step - loss: 0.2488 - accuracy: 0.5351 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 521/1000\n",
      "563/563 [==============================] - 1s 915us/step - loss: 0.2492 - accuracy: 0.5279 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 522/1000\n",
      "563/563 [==============================] - 1s 998us/step - loss: 0.2493 - accuracy: 0.5264 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 523/1000\n",
      "563/563 [==============================] - 1s 966us/step - loss: 0.2492 - accuracy: 0.5277 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 524/1000\n",
      "563/563 [==============================] - 1s 911us/step - loss: 0.2495 - accuracy: 0.5235 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 525/1000\n",
      "563/563 [==============================] - 1s 925us/step - loss: 0.2493 - accuracy: 0.5272 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 526/1000\n",
      "563/563 [==============================] - 1s 945us/step - loss: 0.2492 - accuracy: 0.5287 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 527/1000\n",
      "563/563 [==============================] - 1s 995us/step - loss: 0.2494 - accuracy: 0.5255 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 528/1000\n",
      "563/563 [==============================] - 1s 894us/step - loss: 0.2493 - accuracy: 0.5259 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 529/1000\n",
      "563/563 [==============================] - 1s 891us/step - loss: 0.2489 - accuracy: 0.5344 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 530/1000\n",
      "563/563 [==============================] - 1s 945us/step - loss: 0.2491 - accuracy: 0.5298 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 531/1000\n",
      "563/563 [==============================] - 1s 934us/step - loss: 0.2494 - accuracy: 0.5252 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 532/1000\n",
      "563/563 [==============================] - 1s 922us/step - loss: 0.2492 - accuracy: 0.5292 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 533/1000\n",
      "563/563 [==============================] - 1s 920us/step - loss: 0.2492 - accuracy: 0.5290 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 534/1000\n",
      "563/563 [==============================] - 1s 959us/step - loss: 0.2491 - accuracy: 0.5299 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 535/1000\n",
      "563/563 [==============================] - 1s 911us/step - loss: 0.2495 - accuracy: 0.5231 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 536/1000\n",
      "563/563 [==============================] - 1s 920us/step - loss: 0.2495 - accuracy: 0.5238 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 537/1000\n",
      "563/563 [==============================] - 1s 928us/step - loss: 0.2493 - accuracy: 0.5261 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 538/1000\n",
      "563/563 [==============================] - 1s 977us/step - loss: 0.2488 - accuracy: 0.5359 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 539/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2495 - accuracy: 0.5234 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 540/1000\n",
      "563/563 [==============================] - 1s 963us/step - loss: 0.2489 - accuracy: 0.5334 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 541/1000\n",
      "563/563 [==============================] - 1s 927us/step - loss: 0.2495 - accuracy: 0.5221 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 542/1000\n",
      "563/563 [==============================] - 1s 954us/step - loss: 0.2492 - accuracy: 0.5285 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 543/1000\n",
      "563/563 [==============================] - 1s 911us/step - loss: 0.2493 - accuracy: 0.5268 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 544/1000\n",
      "563/563 [==============================] - 1s 897us/step - loss: 0.2492 - accuracy: 0.5290 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 545/1000\n",
      "563/563 [==============================] - 1s 959us/step - loss: 0.2489 - accuracy: 0.5332 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 546/1000\n",
      "563/563 [==============================] - 1s 941us/step - loss: 0.2494 - accuracy: 0.5246 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 547/1000\n",
      "563/563 [==============================] - 1s 899us/step - loss: 0.2495 - accuracy: 0.5229 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 548/1000\n",
      "563/563 [==============================] - 1s 917us/step - loss: 0.2491 - accuracy: 0.5305 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 549/1000\n",
      "563/563 [==============================] - 1s 956us/step - loss: 0.2492 - accuracy: 0.5280 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 550/1000\n",
      "563/563 [==============================] - 1s 931us/step - loss: 0.2491 - accuracy: 0.5300 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 551/1000\n",
      "563/563 [==============================] - 1s 913us/step - loss: 0.2494 - accuracy: 0.5255 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 552/1000\n",
      "563/563 [==============================] - 1s 908us/step - loss: 0.2488 - accuracy: 0.5358 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 553/1000\n",
      "563/563 [==============================] - 1s 950us/step - loss: 0.2494 - accuracy: 0.5247 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 554/1000\n",
      "563/563 [==============================] - 1s 913us/step - loss: 0.2492 - accuracy: 0.5279 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 555/1000\n",
      "563/563 [==============================] - 1s 893us/step - loss: 0.2493 - accuracy: 0.5274 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 556/1000\n",
      "563/563 [==============================] - 1s 890us/step - loss: 0.2488 - accuracy: 0.5346 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 557/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2495 - accuracy: 0.5237 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 558/1000\n",
      "563/563 [==============================] - 1s 917us/step - loss: 0.2493 - accuracy: 0.5264 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 559/1000\n",
      "563/563 [==============================] - 1s 910us/step - loss: 0.2495 - accuracy: 0.5232 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 560/1000\n",
      "563/563 [==============================] - 1s 931us/step - loss: 0.2492 - accuracy: 0.5283 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 561/1000\n",
      "563/563 [==============================] - 1s 965us/step - loss: 0.2491 - accuracy: 0.5304 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 562/1000\n",
      "563/563 [==============================] - 1s 917us/step - loss: 0.2491 - accuracy: 0.5298 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 563/1000\n",
      "563/563 [==============================] - 1s 913us/step - loss: 0.2492 - accuracy: 0.5284 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 564/1000\n",
      "563/563 [==============================] - 1s 950us/step - loss: 0.2493 - accuracy: 0.5257 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 565/1000\n",
      "563/563 [==============================] - 1s 975us/step - loss: 0.2490 - accuracy: 0.5324 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 566/1000\n",
      "563/563 [==============================] - 1s 911us/step - loss: 0.2490 - accuracy: 0.5318 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 567/1000\n",
      "563/563 [==============================] - 1s 893us/step - loss: 0.2490 - accuracy: 0.5317 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 568/1000\n",
      "563/563 [==============================] - 1s 952us/step - loss: 0.2486 - accuracy: 0.5392 - val_loss: 0.2483 - val_accuracy: 0.5459\n",
      "Epoch 569/1000\n",
      "563/563 [==============================] - 1s 936us/step - loss: 0.2490 - accuracy: 0.5316 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 570/1000\n",
      "563/563 [==============================] - 1s 906us/step - loss: 0.2492 - accuracy: 0.5291 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 571/1000\n",
      "563/563 [==============================] - 1s 911us/step - loss: 0.2488 - accuracy: 0.5351 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 572/1000\n",
      "563/563 [==============================] - 1s 970us/step - loss: 0.2495 - accuracy: 0.5238 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 573/1000\n",
      "563/563 [==============================] - 1s 918us/step - loss: 0.2492 - accuracy: 0.5284 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 574/1000\n",
      "563/563 [==============================] - 1s 915us/step - loss: 0.2490 - accuracy: 0.5324 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 575/1000\n",
      "563/563 [==============================] - 1s 906us/step - loss: 0.2492 - accuracy: 0.5287 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 576/1000\n",
      "563/563 [==============================] - 1s 984us/step - loss: 0.2492 - accuracy: 0.5279 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 577/1000\n",
      "563/563 [==============================] - 1s 902us/step - loss: 0.2490 - accuracy: 0.5313 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 578/1000\n",
      "563/563 [==============================] - 1s 904us/step - loss: 0.2497 - accuracy: 0.5191 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 579/1000\n",
      "563/563 [==============================] - 1s 904us/step - loss: 0.2496 - accuracy: 0.5216 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 580/1000\n",
      "563/563 [==============================] - 1s 959us/step - loss: 0.2492 - accuracy: 0.5288 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 581/1000\n",
      "563/563 [==============================] - 1s 899us/step - loss: 0.2492 - accuracy: 0.5292 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 582/1000\n",
      "563/563 [==============================] - 1s 913us/step - loss: 0.2491 - accuracy: 0.5301 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 583/1000\n",
      "563/563 [==============================] - 1s 925us/step - loss: 0.2493 - accuracy: 0.5262 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 584/1000\n",
      "563/563 [==============================] - 1s 954us/step - loss: 0.2491 - accuracy: 0.5299 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 585/1000\n",
      "563/563 [==============================] - 1s 893us/step - loss: 0.2490 - accuracy: 0.5327 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 586/1000\n",
      "563/563 [==============================] - 1s 908us/step - loss: 0.2490 - accuracy: 0.5325 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 587/1000\n",
      "563/563 [==============================] - 1s 936us/step - loss: 0.2489 - accuracy: 0.5337 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 588/1000\n",
      "563/563 [==============================] - 1s 941us/step - loss: 0.2494 - accuracy: 0.5246 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 589/1000\n",
      "563/563 [==============================] - 1s 908us/step - loss: 0.2493 - accuracy: 0.5263 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 590/1000\n",
      "563/563 [==============================] - 1s 901us/step - loss: 0.2489 - accuracy: 0.5330 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 591/1000\n",
      "563/563 [==============================] - 1s 956us/step - loss: 0.2491 - accuracy: 0.5307 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 592/1000\n",
      "563/563 [==============================] - 1s 943us/step - loss: 0.2494 - accuracy: 0.5242 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 593/1000\n",
      "563/563 [==============================] - 1s 943us/step - loss: 0.2495 - accuracy: 0.5235 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 594/1000\n",
      "563/563 [==============================] - 1s 902us/step - loss: 0.2490 - accuracy: 0.5322 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 595/1000\n",
      "563/563 [==============================] - 1s 981us/step - loss: 0.2494 - accuracy: 0.5243 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 596/1000\n",
      "563/563 [==============================] - 1s 913us/step - loss: 0.2494 - accuracy: 0.5254 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 597/1000\n",
      "563/563 [==============================] - 1s 918us/step - loss: 0.2487 - accuracy: 0.5366 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 598/1000\n",
      "563/563 [==============================] - 1s 920us/step - loss: 0.2488 - accuracy: 0.5346 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 599/1000\n",
      "563/563 [==============================] - 1s 984us/step - loss: 0.2492 - accuracy: 0.5279 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 600/1000\n",
      "563/563 [==============================] - 0s 885us/step - loss: 0.2493 - accuracy: 0.5270 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 601/1000\n",
      "563/563 [==============================] - 1s 897us/step - loss: 0.2493 - accuracy: 0.5273 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 602/1000\n",
      "563/563 [==============================] - 1s 924us/step - loss: 0.2491 - accuracy: 0.5299 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 603/1000\n",
      "563/563 [==============================] - 1s 991us/step - loss: 0.2495 - accuracy: 0.5232 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 604/1000\n",
      "563/563 [==============================] - 1s 906us/step - loss: 0.2490 - accuracy: 0.5319 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 605/1000\n",
      "563/563 [==============================] - 1s 913us/step - loss: 0.2492 - accuracy: 0.5278 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 606/1000\n",
      "563/563 [==============================] - 1s 929us/step - loss: 0.2491 - accuracy: 0.5298 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 607/1000\n",
      "563/563 [==============================] - 1s 938us/step - loss: 0.2492 - accuracy: 0.5276 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 608/1000\n",
      "563/563 [==============================] - 1s 908us/step - loss: 0.2495 - accuracy: 0.5237 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 609/1000\n",
      "563/563 [==============================] - 1s 897us/step - loss: 0.2493 - accuracy: 0.5271 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 610/1000\n",
      "563/563 [==============================] - 1s 940us/step - loss: 0.2493 - accuracy: 0.5274 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 611/1000\n",
      "563/563 [==============================] - 1s 945us/step - loss: 0.2492 - accuracy: 0.5275 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 612/1000\n",
      "563/563 [==============================] - 1s 899us/step - loss: 0.2491 - accuracy: 0.5305 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 613/1000\n",
      "563/563 [==============================] - 1s 911us/step - loss: 0.2494 - accuracy: 0.5253 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 614/1000\n",
      "563/563 [==============================] - 1s 970us/step - loss: 0.2490 - accuracy: 0.5313 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 615/1000\n",
      "563/563 [==============================] - 1s 927us/step - loss: 0.2492 - accuracy: 0.5293 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 616/1000\n",
      "563/563 [==============================] - 1s 913us/step - loss: 0.2496 - accuracy: 0.5205 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 617/1000\n",
      "563/563 [==============================] - 1s 917us/step - loss: 0.2492 - accuracy: 0.5292 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 618/1000\n",
      "563/563 [==============================] - 1s 961us/step - loss: 0.2492 - accuracy: 0.5277 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 619/1000\n",
      "563/563 [==============================] - 1s 933us/step - loss: 0.2493 - accuracy: 0.5266 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 620/1000\n",
      "563/563 [==============================] - 1s 913us/step - loss: 0.2491 - accuracy: 0.5295 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 621/1000\n",
      "563/563 [==============================] - 1s 899us/step - loss: 0.2491 - accuracy: 0.5300 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 622/1000\n",
      "563/563 [==============================] - 1s 990us/step - loss: 0.2491 - accuracy: 0.5303 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 623/1000\n",
      "563/563 [==============================] - 1s 909us/step - loss: 0.2497 - accuracy: 0.5194 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 624/1000\n",
      "563/563 [==============================] - 1s 922us/step - loss: 0.2491 - accuracy: 0.5310 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 625/1000\n",
      "563/563 [==============================] - 1s 924us/step - loss: 0.2495 - accuracy: 0.5232 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 626/1000\n",
      "563/563 [==============================] - 1s 958us/step - loss: 0.2495 - accuracy: 0.5240 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 627/1000\n",
      "563/563 [==============================] - 1s 918us/step - loss: 0.2491 - accuracy: 0.5306 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 628/1000\n",
      "563/563 [==============================] - 1s 895us/step - loss: 0.2491 - accuracy: 0.5307 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 629/1000\n",
      "563/563 [==============================] - 1s 965us/step - loss: 0.2493 - accuracy: 0.5270 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 630/1000\n",
      "563/563 [==============================] - 1s 936us/step - loss: 0.2492 - accuracy: 0.5280 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 631/1000\n",
      "563/563 [==============================] - 1s 925us/step - loss: 0.2490 - accuracy: 0.5322 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 632/1000\n",
      "563/563 [==============================] - 1s 899us/step - loss: 0.2493 - accuracy: 0.5264 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 633/1000\n",
      "563/563 [==============================] - ETA: 0s - loss: 0.2495 - accuracy: 0.52 - 1s 949us/step - loss: 0.2494 - accuracy: 0.5243 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 634/1000\n",
      "563/563 [==============================] - 1s 933us/step - loss: 0.2493 - accuracy: 0.5261 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 635/1000\n",
      "563/563 [==============================] - 1s 909us/step - loss: 0.2493 - accuracy: 0.5260 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 636/1000\n",
      "563/563 [==============================] - 1s 911us/step - loss: 0.2490 - accuracy: 0.5312 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 637/1000\n",
      "563/563 [==============================] - 1s 968us/step - loss: 0.2492 - accuracy: 0.5277 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 638/1000\n",
      "563/563 [==============================] - 1s 934us/step - loss: 0.2490 - accuracy: 0.5319 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 639/1000\n",
      "563/563 [==============================] - 1s 977us/step - loss: 0.2494 - accuracy: 0.5252 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 640/1000\n",
      "563/563 [==============================] - 1s 968us/step - loss: 0.2490 - accuracy: 0.5319 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 641/1000\n",
      "563/563 [==============================] - 1s 959us/step - loss: 0.2495 - accuracy: 0.5224 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 642/1000\n",
      "563/563 [==============================] - 1s 911us/step - loss: 0.2488 - accuracy: 0.5348 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 643/1000\n",
      "563/563 [==============================] - 1s 907us/step - loss: 0.2491 - accuracy: 0.5311 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 644/1000\n",
      "563/563 [==============================] - 1s 922us/step - loss: 0.2494 - accuracy: 0.5240 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 645/1000\n",
      "563/563 [==============================] - 1s 933us/step - loss: 0.2493 - accuracy: 0.5261 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 646/1000\n",
      "563/563 [==============================] - 1s 912us/step - loss: 0.2495 - accuracy: 0.5234 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 647/1000\n",
      "563/563 [==============================] - 1s 890us/step - loss: 0.2493 - accuracy: 0.5271 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 648/1000\n",
      "563/563 [==============================] - 1s 966us/step - loss: 0.2488 - accuracy: 0.5352 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 649/1000\n",
      "563/563 [==============================] - 1s 947us/step - loss: 0.2491 - accuracy: 0.5302 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 650/1000\n",
      "563/563 [==============================] - 1s 940us/step - loss: 0.2491 - accuracy: 0.5301 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 651/1000\n",
      "563/563 [==============================] - 1s 913us/step - loss: 0.2493 - accuracy: 0.5265 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 652/1000\n",
      "563/563 [==============================] - 1s 986us/step - loss: 0.2496 - accuracy: 0.5213 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 653/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2488 - accuracy: 0.5345 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 654/1000\n",
      "563/563 [==============================] - 1s 892us/step - loss: 0.2491 - accuracy: 0.5309 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 655/1000\n",
      "563/563 [==============================] - 1s 913us/step - loss: 0.2494 - accuracy: 0.5253 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 656/1000\n",
      "563/563 [==============================] - 1s 974us/step - loss: 0.2490 - accuracy: 0.5316 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 657/1000\n",
      "563/563 [==============================] - 1s 917us/step - loss: 0.2492 - accuracy: 0.5285 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 658/1000\n",
      "563/563 [==============================] - 1s 925us/step - loss: 0.2493 - accuracy: 0.5260 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 659/1000\n",
      "563/563 [==============================] - 1s 922us/step - loss: 0.2493 - accuracy: 0.5262 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 660/1000\n",
      "563/563 [==============================] - 1s 940us/step - loss: 0.2493 - accuracy: 0.5259 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 661/1000\n",
      "563/563 [==============================] - 1s 917us/step - loss: 0.2493 - accuracy: 0.5273 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 662/1000\n",
      "563/563 [==============================] - 1s 892us/step - loss: 0.2492 - accuracy: 0.5277 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 663/1000\n",
      "563/563 [==============================] - 1s 934us/step - loss: 0.2492 - accuracy: 0.5292 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 664/1000\n",
      "563/563 [==============================] - 1s 980us/step - loss: 0.2495 - accuracy: 0.5226 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 665/1000\n",
      "563/563 [==============================] - 1s 915us/step - loss: 0.2491 - accuracy: 0.5295 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 666/1000\n",
      "563/563 [==============================] - 1s 915us/step - loss: 0.2494 - accuracy: 0.5247 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 667/1000\n",
      "563/563 [==============================] - 1s 929us/step - loss: 0.2491 - accuracy: 0.5299 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 668/1000\n",
      "563/563 [==============================] - 1s 938us/step - loss: 0.2493 - accuracy: 0.5267 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 669/1000\n",
      "563/563 [==============================] - 1s 929us/step - loss: 0.2494 - accuracy: 0.5251 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 670/1000\n",
      "563/563 [==============================] - 1s 908us/step - loss: 0.2490 - accuracy: 0.5325 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 671/1000\n",
      "563/563 [==============================] - 1s 988us/step - loss: 0.2492 - accuracy: 0.5276 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 672/1000\n",
      "563/563 [==============================] - 1s 943us/step - loss: 0.2490 - accuracy: 0.5315 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 673/1000\n",
      "563/563 [==============================] - 1s 899us/step - loss: 0.2492 - accuracy: 0.5287 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 674/1000\n",
      "563/563 [==============================] - 1s 908us/step - loss: 0.2495 - accuracy: 0.5235 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 675/1000\n",
      "563/563 [==============================] - 1s 972us/step - loss: 0.2492 - accuracy: 0.5279 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 676/1000\n",
      "563/563 [==============================] - 1s 906us/step - loss: 0.2492 - accuracy: 0.5280 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 677/1000\n",
      "563/563 [==============================] - 1s 915us/step - loss: 0.2496 - accuracy: 0.5215 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 678/1000\n",
      "563/563 [==============================] - 1s 927us/step - loss: 0.2491 - accuracy: 0.5298 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 679/1000\n",
      "563/563 [==============================] - 1s 974us/step - loss: 0.2493 - accuracy: 0.5259 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 680/1000\n",
      "563/563 [==============================] - 1s 915us/step - loss: 0.2495 - accuracy: 0.5224 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 681/1000\n",
      "563/563 [==============================] - 1s 909us/step - loss: 0.2495 - accuracy: 0.5237 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 682/1000\n",
      "563/563 [==============================] - 1s 949us/step - loss: 0.2492 - accuracy: 0.5289 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 683/1000\n",
      "563/563 [==============================] - 1s 932us/step - loss: 0.2490 - accuracy: 0.5316 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 684/1000\n",
      "563/563 [==============================] - 1s 915us/step - loss: 0.2492 - accuracy: 0.5290 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 685/1000\n",
      "563/563 [==============================] - 1s 897us/step - loss: 0.2494 - accuracy: 0.5239 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 686/1000\n",
      "563/563 [==============================] - 1s 931us/step - loss: 0.2490 - accuracy: 0.5320 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 687/1000\n",
      "563/563 [==============================] - 1s 949us/step - loss: 0.2494 - accuracy: 0.5245 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 688/1000\n",
      "563/563 [==============================] - 1s 895us/step - loss: 0.2492 - accuracy: 0.5286 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 689/1000\n",
      "563/563 [==============================] - 1s 897us/step - loss: 0.2492 - accuracy: 0.5284 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 690/1000\n",
      "563/563 [==============================] - 1s 936us/step - loss: 0.2490 - accuracy: 0.5326 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 691/1000\n",
      "563/563 [==============================] - 1s 940us/step - loss: 0.2491 - accuracy: 0.5301 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 692/1000\n",
      "563/563 [==============================] - 1s 909us/step - loss: 0.2491 - accuracy: 0.5302 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 693/1000\n",
      "563/563 [==============================] - 1s 911us/step - loss: 0.2495 - accuracy: 0.5238 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 694/1000\n",
      "563/563 [==============================] - 1s 966us/step - loss: 0.2494 - accuracy: 0.5248 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 695/1000\n",
      "563/563 [==============================] - 1s 901us/step - loss: 0.2493 - accuracy: 0.5269 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 696/1000\n",
      "563/563 [==============================] - 1s 899us/step - loss: 0.2491 - accuracy: 0.5309 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 697/1000\n",
      "563/563 [==============================] - 1s 913us/step - loss: 0.2490 - accuracy: 0.5323 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 698/1000\n",
      "563/563 [==============================] - 1s 977us/step - loss: 0.2490 - accuracy: 0.5324 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 699/1000\n",
      "563/563 [==============================] - 1s 913us/step - loss: 0.2495 - accuracy: 0.5226 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 700/1000\n",
      "563/563 [==============================] - 1s 943us/step - loss: 0.2493 - accuracy: 0.5259 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 701/1000\n",
      "563/563 [==============================] - 1s 924us/step - loss: 0.2491 - accuracy: 0.5301 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 702/1000\n",
      "563/563 [==============================] - 1s 949us/step - loss: 0.2490 - accuracy: 0.5314 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 703/1000\n",
      "563/563 [==============================] - 1s 911us/step - loss: 0.2494 - accuracy: 0.5254 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 704/1000\n",
      "563/563 [==============================] - 1s 911us/step - loss: 0.2493 - accuracy: 0.5272 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 705/1000\n",
      "563/563 [==============================] - 1s 917us/step - loss: 0.2491 - accuracy: 0.5294 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 706/1000\n",
      "563/563 [==============================] - 1s 952us/step - loss: 0.2493 - accuracy: 0.5275 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 707/1000\n",
      "563/563 [==============================] - 1s 941us/step - loss: 0.2491 - accuracy: 0.5302 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 708/1000\n",
      "563/563 [==============================] - 1s 893us/step - loss: 0.2494 - accuracy: 0.5251 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 709/1000\n",
      "563/563 [==============================] - 1s 950us/step - loss: 0.2493 - accuracy: 0.5261 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 710/1000\n",
      "563/563 [==============================] - 1s 949us/step - loss: 0.2494 - accuracy: 0.5254 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 711/1000\n",
      "563/563 [==============================] - 1s 901us/step - loss: 0.2490 - accuracy: 0.5315 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 712/1000\n",
      "563/563 [==============================] - 1s 908us/step - loss: 0.2491 - accuracy: 0.5303 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 713/1000\n",
      "563/563 [==============================] - 1s 941us/step - loss: 0.2493 - accuracy: 0.5265 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 714/1000\n",
      "563/563 [==============================] - 1s 940us/step - loss: 0.2495 - accuracy: 0.5235 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 715/1000\n",
      "563/563 [==============================] - 1s 915us/step - loss: 0.2490 - accuracy: 0.5317 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 716/1000\n",
      "563/563 [==============================] - 1s 902us/step - loss: 0.2492 - accuracy: 0.5276 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 717/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2488 - accuracy: 0.5354 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 718/1000\n",
      "563/563 [==============================] - 1s 918us/step - loss: 0.2488 - accuracy: 0.5347 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 719/1000\n",
      "563/563 [==============================] - 1s 906us/step - loss: 0.2492 - accuracy: 0.5275 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 720/1000\n",
      "563/563 [==============================] - 1s 915us/step - loss: 0.2492 - accuracy: 0.5284 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 721/1000\n",
      "563/563 [==============================] - 1s 961us/step - loss: 0.2491 - accuracy: 0.5304 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 722/1000\n",
      "563/563 [==============================] - 1s 915us/step - loss: 0.2492 - accuracy: 0.5282 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 723/1000\n",
      "563/563 [==============================] - 1s 899us/step - loss: 0.2494 - accuracy: 0.5247 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 724/1000\n",
      "563/563 [==============================] - 1s 933us/step - loss: 0.2492 - accuracy: 0.5287 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 725/1000\n",
      "563/563 [==============================] - 1s 950us/step - loss: 0.2493 - accuracy: 0.5262 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 726/1000\n",
      "563/563 [==============================] - 1s 895us/step - loss: 0.2493 - accuracy: 0.5270 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 727/1000\n",
      "563/563 [==============================] - 1s 906us/step - loss: 0.2497 - accuracy: 0.5189 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 728/1000\n",
      "563/563 [==============================] - 1s 949us/step - loss: 0.2496 - accuracy: 0.5223 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 729/1000\n",
      "563/563 [==============================] - 1s 941us/step - loss: 0.2490 - accuracy: 0.5318 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 730/1000\n",
      "563/563 [==============================] - 1s 899us/step - loss: 0.2493 - accuracy: 0.5264 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 731/1000\n",
      "563/563 [==============================] - 1s 892us/step - loss: 0.2489 - accuracy: 0.5331 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 732/1000\n",
      "563/563 [==============================] - 1s 963us/step - loss: 0.2491 - accuracy: 0.5295 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 733/1000\n",
      "563/563 [==============================] - 1s 920us/step - loss: 0.2491 - accuracy: 0.5298 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 734/1000\n",
      "563/563 [==============================] - 1s 902us/step - loss: 0.2495 - accuracy: 0.5238 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 735/1000\n",
      "563/563 [==============================] - 1s 915us/step - loss: 0.2492 - accuracy: 0.5287 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 736/1000\n",
      "563/563 [==============================] - 1s 995us/step - loss: 0.2490 - accuracy: 0.5320 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 737/1000\n",
      "563/563 [==============================] - 1s 934us/step - loss: 0.2495 - accuracy: 0.5222 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 738/1000\n",
      "563/563 [==============================] - 1s 927us/step - loss: 0.2494 - accuracy: 0.5243 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 739/1000\n",
      "563/563 [==============================] - 1s 908us/step - loss: 0.2494 - accuracy: 0.5254 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 740/1000\n",
      "563/563 [==============================] - 1s 968us/step - loss: 0.2493 - accuracy: 0.5269 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 741/1000\n",
      "563/563 [==============================] - 1s 908us/step - loss: 0.2493 - accuracy: 0.5263 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 742/1000\n",
      "563/563 [==============================] - 1s 906us/step - loss: 0.2493 - accuracy: 0.5270 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 743/1000\n",
      "563/563 [==============================] - 1s 941us/step - loss: 0.2491 - accuracy: 0.5307 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 744/1000\n",
      "563/563 [==============================] - 1s 956us/step - loss: 0.2493 - accuracy: 0.5268 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 745/1000\n",
      "563/563 [==============================] - 1s 897us/step - loss: 0.2493 - accuracy: 0.5274 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 746/1000\n",
      "563/563 [==============================] - 1s 906us/step - loss: 0.2492 - accuracy: 0.5282 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 747/1000\n",
      "563/563 [==============================] - 1s 924us/step - loss: 0.2492 - accuracy: 0.5286 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 748/1000\n",
      "563/563 [==============================] - 1s 947us/step - loss: 0.2491 - accuracy: 0.5302 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 749/1000\n",
      "563/563 [==============================] - 1s 916us/step - loss: 0.2492 - accuracy: 0.5280 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 750/1000\n",
      "563/563 [==============================] - 1s 911us/step - loss: 0.2493 - accuracy: 0.5264 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 751/1000\n",
      "563/563 [==============================] - 1s 958us/step - loss: 0.2493 - accuracy: 0.5262 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 752/1000\n",
      "563/563 [==============================] - 1s 979us/step - loss: 0.2494 - accuracy: 0.5252 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 753/1000\n",
      "563/563 [==============================] - 1s 936us/step - loss: 0.2494 - accuracy: 0.5240 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 754/1000\n",
      "563/563 [==============================] - 1s 966us/step - loss: 0.2491 - accuracy: 0.5304 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 755/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2491 - accuracy: 0.5301 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 756/1000\n",
      "563/563 [==============================] - 1s 915us/step - loss: 0.2495 - accuracy: 0.5237 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 757/1000\n",
      "563/563 [==============================] - 1s 902us/step - loss: 0.2497 - accuracy: 0.5199 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 758/1000\n",
      "563/563 [==============================] - 1s 927us/step - loss: 0.2491 - accuracy: 0.5309 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 759/1000\n",
      "563/563 [==============================] - 1s 958us/step - loss: 0.2491 - accuracy: 0.5300 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 760/1000\n",
      "563/563 [==============================] - 1s 925us/step - loss: 0.2490 - accuracy: 0.5315 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 761/1000\n",
      "563/563 [==============================] - 1s 909us/step - loss: 0.2493 - accuracy: 0.5274 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 762/1000\n",
      "563/563 [==============================] - 1s 931us/step - loss: 0.2494 - accuracy: 0.5249 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 763/1000\n",
      "563/563 [==============================] - 1s 959us/step - loss: 0.2491 - accuracy: 0.5302 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 764/1000\n",
      "563/563 [==============================] - 1s 920us/step - loss: 0.2493 - accuracy: 0.5266 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 765/1000\n",
      "563/563 [==============================] - 1s 906us/step - loss: 0.2492 - accuracy: 0.5279 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 766/1000\n",
      "563/563 [==============================] - 1s 965us/step - loss: 0.2493 - accuracy: 0.5262 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 767/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2493 - accuracy: 0.5258 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 768/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2493 - accuracy: 0.5269 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 769/1000\n",
      "563/563 [==============================] - 1s 911us/step - loss: 0.2492 - accuracy: 0.5282 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 770/1000\n",
      "563/563 [==============================] - 1s 956us/step - loss: 0.2490 - accuracy: 0.5322 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 771/1000\n",
      "563/563 [==============================] - 1s 911us/step - loss: 0.2492 - accuracy: 0.5277 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 772/1000\n",
      "563/563 [==============================] - 1s 904us/step - loss: 0.2493 - accuracy: 0.5261 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 773/1000\n",
      "563/563 [==============================] - 1s 913us/step - loss: 0.2494 - accuracy: 0.5249 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 774/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2491 - accuracy: 0.5297 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 775/1000\n",
      "563/563 [==============================] - 1s 917us/step - loss: 0.2492 - accuracy: 0.5280 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 776/1000\n",
      "563/563 [==============================] - 1s 940us/step - loss: 0.2492 - accuracy: 0.5287 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 777/1000\n",
      "563/563 [==============================] - 1s 933us/step - loss: 0.2491 - accuracy: 0.5297 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 778/1000\n",
      "563/563 [==============================] - 1s 943us/step - loss: 0.2495 - accuracy: 0.5235 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 779/1000\n",
      "563/563 [==============================] - 1s 911us/step - loss: 0.2491 - accuracy: 0.5306 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 780/1000\n",
      "563/563 [==============================] - 1s 917us/step - loss: 0.2493 - accuracy: 0.5269 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 781/1000\n",
      "563/563 [==============================] - 1s 952us/step - loss: 0.2491 - accuracy: 0.5305 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 782/1000\n",
      "563/563 [==============================] - 1s 940us/step - loss: 0.2493 - accuracy: 0.5264 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 783/1000\n",
      "563/563 [==============================] - 1s 915us/step - loss: 0.2490 - accuracy: 0.5312 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 784/1000\n",
      "563/563 [==============================] - 1s 920us/step - loss: 0.2492 - accuracy: 0.5288 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 785/1000\n",
      "563/563 [==============================] - 1s 956us/step - loss: 0.2494 - accuracy: 0.5257 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 786/1000\n",
      "563/563 [==============================] - 1s 985us/step - loss: 0.2493 - accuracy: 0.5263 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 787/1000\n",
      "563/563 [==============================] - 1s 917us/step - loss: 0.2494 - accuracy: 0.5255 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 788/1000\n",
      "563/563 [==============================] - 1s 918us/step - loss: 0.2489 - accuracy: 0.5344 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 789/1000\n",
      "563/563 [==============================] - 1s 977us/step - loss: 0.2491 - accuracy: 0.5310 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 790/1000\n",
      "563/563 [==============================] - 1s 908us/step - loss: 0.2492 - accuracy: 0.5288 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 791/1000\n",
      "563/563 [==============================] - 1s 925us/step - loss: 0.2493 - accuracy: 0.5266 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 792/1000\n",
      "563/563 [==============================] - 1s 927us/step - loss: 0.2489 - accuracy: 0.5340 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 793/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2492 - accuracy: 0.5283 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 794/1000\n",
      "563/563 [==============================] - 1s 908us/step - loss: 0.2493 - accuracy: 0.5270 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 795/1000\n",
      "563/563 [==============================] - 1s 913us/step - loss: 0.2490 - accuracy: 0.5311 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 796/1000\n",
      "563/563 [==============================] - 1s 953us/step - loss: 0.2491 - accuracy: 0.5302 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 797/1000\n",
      "563/563 [==============================] - 1s 943us/step - loss: 0.2492 - accuracy: 0.5284 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 798/1000\n",
      "563/563 [==============================] - 1s 918us/step - loss: 0.2494 - accuracy: 0.5254 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 799/1000\n",
      "563/563 [==============================] - 1s 903us/step - loss: 0.2491 - accuracy: 0.5306 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 800/1000\n",
      "563/563 [==============================] - 1s 949us/step - loss: 0.2494 - accuracy: 0.5244 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 801/1000\n",
      "563/563 [==============================] - 1s 933us/step - loss: 0.2491 - accuracy: 0.5309 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 802/1000\n",
      "563/563 [==============================] - 1s 929us/step - loss: 0.2488 - accuracy: 0.5353 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 803/1000\n",
      "563/563 [==============================] - 1s 925us/step - loss: 0.2490 - accuracy: 0.5318 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 804/1000\n",
      "563/563 [==============================] - 1s 966us/step - loss: 0.2492 - accuracy: 0.5281 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 805/1000\n",
      "563/563 [==============================] - 1s 929us/step - loss: 0.2491 - accuracy: 0.5304 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 806/1000\n",
      "563/563 [==============================] - 1s 909us/step - loss: 0.2490 - accuracy: 0.5324 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 807/1000\n",
      "563/563 [==============================] - 1s 904us/step - loss: 0.2496 - accuracy: 0.5214 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 808/1000\n",
      "563/563 [==============================] - 1s 959us/step - loss: 0.2491 - accuracy: 0.5301 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 809/1000\n",
      "563/563 [==============================] - 1s 897us/step - loss: 0.2492 - accuracy: 0.5278 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 810/1000\n",
      "563/563 [==============================] - 1s 913us/step - loss: 0.2492 - accuracy: 0.5280 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 811/1000\n",
      "563/563 [==============================] - 1s 927us/step - loss: 0.2489 - accuracy: 0.5333 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 812/1000\n",
      "563/563 [==============================] - 1s 988us/step - loss: 0.2494 - accuracy: 0.5243 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 813/1000\n",
      "563/563 [==============================] - 1s 906us/step - loss: 0.2491 - accuracy: 0.5298 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 814/1000\n",
      "563/563 [==============================] - 1s 918us/step - loss: 0.2491 - accuracy: 0.5299 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 815/1000\n",
      "563/563 [==============================] - 1s 933us/step - loss: 0.2493 - accuracy: 0.5269 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 816/1000\n",
      "563/563 [==============================] - 1s 965us/step - loss: 0.2494 - accuracy: 0.5244 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 817/1000\n",
      "563/563 [==============================] - 1s 988us/step - loss: 0.2491 - accuracy: 0.5306 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 818/1000\n",
      "563/563 [==============================] - 1s 917us/step - loss: 0.2493 - accuracy: 0.5271 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 819/1000\n",
      "563/563 [==============================] - 1s 965us/step - loss: 0.2490 - accuracy: 0.5326 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 820/1000\n",
      "563/563 [==============================] - 1s 991us/step - loss: 0.2496 - accuracy: 0.5209 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 821/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2491 - accuracy: 0.5298 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 822/1000\n",
      "563/563 [==============================] - 1s 906us/step - loss: 0.2495 - accuracy: 0.5226 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 823/1000\n",
      "563/563 [==============================] - 1s 990us/step - loss: 0.2494 - accuracy: 0.5254 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 824/1000\n",
      "563/563 [==============================] - 1s 929us/step - loss: 0.2494 - accuracy: 0.5249 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 825/1000\n",
      "563/563 [==============================] - 1s 899us/step - loss: 0.2496 - accuracy: 0.5218 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 826/1000\n",
      "563/563 [==============================] - 1s 950us/step - loss: 0.2494 - accuracy: 0.5245 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 827/1000\n",
      "563/563 [==============================] - 1s 970us/step - loss: 0.2492 - accuracy: 0.5293 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 828/1000\n",
      "563/563 [==============================] - 1s 917us/step - loss: 0.2490 - accuracy: 0.5315 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 829/1000\n",
      "563/563 [==============================] - 1s 897us/step - loss: 0.2497 - accuracy: 0.5186 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 830/1000\n",
      "563/563 [==============================] - 1s 931us/step - loss: 0.2492 - accuracy: 0.5284 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 831/1000\n",
      "563/563 [==============================] - 1s 943us/step - loss: 0.2494 - accuracy: 0.5251 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 832/1000\n",
      "563/563 [==============================] - 1s 922us/step - loss: 0.2492 - accuracy: 0.5281 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 833/1000\n",
      "563/563 [==============================] - 1s 911us/step - loss: 0.2491 - accuracy: 0.5293 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 834/1000\n",
      "563/563 [==============================] - 1s 943us/step - loss: 0.2491 - accuracy: 0.5300 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 835/1000\n",
      "563/563 [==============================] - 1s 933us/step - loss: 0.2493 - accuracy: 0.5258 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 836/1000\n",
      "563/563 [==============================] - 1s 899us/step - loss: 0.2493 - accuracy: 0.5260 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 837/1000\n",
      "563/563 [==============================] - 1s 899us/step - loss: 0.2493 - accuracy: 0.5262 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 838/1000\n",
      "563/563 [==============================] - 1s 974us/step - loss: 0.2490 - accuracy: 0.5312 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 839/1000\n",
      "563/563 [==============================] - 1s 920us/step - loss: 0.2489 - accuracy: 0.5344 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 840/1000\n",
      "563/563 [==============================] - 1s 911us/step - loss: 0.2489 - accuracy: 0.5335 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 841/1000\n",
      "563/563 [==============================] - 1s 913us/step - loss: 0.2492 - accuracy: 0.5283 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 842/1000\n",
      "563/563 [==============================] - 1s 975us/step - loss: 0.2492 - accuracy: 0.5277 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 843/1000\n",
      "563/563 [==============================] - 1s 904us/step - loss: 0.2491 - accuracy: 0.5306 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 844/1000\n",
      "563/563 [==============================] - 1s 908us/step - loss: 0.2488 - accuracy: 0.5362 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 845/1000\n",
      "563/563 [==============================] - 1s 925us/step - loss: 0.2493 - accuracy: 0.5261 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 846/1000\n",
      "563/563 [==============================] - 1s 968us/step - loss: 0.2490 - accuracy: 0.5317 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 847/1000\n",
      "563/563 [==============================] - 1s 936us/step - loss: 0.2492 - accuracy: 0.5287 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 848/1000\n",
      "563/563 [==============================] - 1s 899us/step - loss: 0.2493 - accuracy: 0.5266 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 849/1000\n",
      "563/563 [==============================] - 1s 990us/step - loss: 0.2491 - accuracy: 0.5295 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 850/1000\n",
      "563/563 [==============================] - 1s 963us/step - loss: 0.2492 - accuracy: 0.5291 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 851/1000\n",
      "563/563 [==============================] - 1s 902us/step - loss: 0.2492 - accuracy: 0.5279 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 852/1000\n",
      "563/563 [==============================] - 1s 917us/step - loss: 0.2496 - accuracy: 0.5213 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 853/1000\n",
      "563/563 [==============================] - 1s 976us/step - loss: 0.2493 - accuracy: 0.5258 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 854/1000\n",
      "563/563 [==============================] - 1s 909us/step - loss: 0.2495 - accuracy: 0.5226 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 855/1000\n",
      "563/563 [==============================] - 1s 909us/step - loss: 0.2492 - accuracy: 0.5292 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 856/1000\n",
      "563/563 [==============================] - 0s 881us/step - loss: 0.2490 - accuracy: 0.5323 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 857/1000\n",
      "563/563 [==============================] - 1s 958us/step - loss: 0.2495 - accuracy: 0.5234 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 858/1000\n",
      "563/563 [==============================] - 1s 911us/step - loss: 0.2492 - accuracy: 0.5277 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 859/1000\n",
      "563/563 [==============================] - 1s 911us/step - loss: 0.2493 - accuracy: 0.5265 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 860/1000\n",
      "563/563 [==============================] - 1s 911us/step - loss: 0.2493 - accuracy: 0.5271 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 861/1000\n",
      "563/563 [==============================] - 1s 995us/step - loss: 0.2489 - accuracy: 0.5338 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 862/1000\n",
      "563/563 [==============================] - 1s 929us/step - loss: 0.2490 - accuracy: 0.5320 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 863/1000\n",
      "563/563 [==============================] - 1s 890us/step - loss: 0.2489 - accuracy: 0.5340 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 864/1000\n",
      "563/563 [==============================] - 1s 895us/step - loss: 0.2493 - accuracy: 0.5262 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 865/1000\n",
      "563/563 [==============================] - 1s 968us/step - loss: 0.2492 - accuracy: 0.5276 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 866/1000\n",
      "563/563 [==============================] - 1s 918us/step - loss: 0.2492 - accuracy: 0.5283 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 867/1000\n",
      "563/563 [==============================] - 1s 924us/step - loss: 0.2493 - accuracy: 0.5271 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 868/1000\n",
      "563/563 [==============================] - 1s 977us/step - loss: 0.2493 - accuracy: 0.5265 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 869/1000\n",
      "563/563 [==============================] - 1s 988us/step - loss: 0.2491 - accuracy: 0.5298 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 870/1000\n",
      "563/563 [==============================] - 1s 913us/step - loss: 0.2493 - accuracy: 0.5261 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 871/1000\n",
      "563/563 [==============================] - 1s 909us/step - loss: 0.2497 - accuracy: 0.5199 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 872/1000\n",
      "563/563 [==============================] - 1s 943us/step - loss: 0.2491 - accuracy: 0.5304 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 873/1000\n",
      "563/563 [==============================] - 1s 945us/step - loss: 0.2488 - accuracy: 0.5356 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 874/1000\n",
      "563/563 [==============================] - 1s 893us/step - loss: 0.2492 - accuracy: 0.5279 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 875/1000\n",
      "563/563 [==============================] - 1s 908us/step - loss: 0.2487 - accuracy: 0.5367 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 876/1000\n",
      "563/563 [==============================] - 1s 968us/step - loss: 0.2491 - accuracy: 0.5294 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 877/1000\n",
      "563/563 [==============================] - 1s 941us/step - loss: 0.2493 - accuracy: 0.5265 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 878/1000\n",
      "563/563 [==============================] - 1s 947us/step - loss: 0.2494 - accuracy: 0.5241 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 879/1000\n",
      "563/563 [==============================] - 1s 918us/step - loss: 0.2490 - accuracy: 0.5321 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 880/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2497 - accuracy: 0.5194 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 881/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2490 - accuracy: 0.5311 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 882/1000\n",
      "563/563 [==============================] - 1s 908us/step - loss: 0.2492 - accuracy: 0.5277 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 883/1000\n",
      "563/563 [==============================] - 1s 931us/step - loss: 0.2494 - accuracy: 0.5248 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 884/1000\n",
      "563/563 [==============================] - 1s 952us/step - loss: 0.2492 - accuracy: 0.5281 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 885/1000\n",
      "563/563 [==============================] - 1s 897us/step - loss: 0.2491 - accuracy: 0.5307 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 886/1000\n",
      "563/563 [==============================] - 1s 901us/step - loss: 0.2489 - accuracy: 0.5329 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 887/1000\n",
      "563/563 [==============================] - 1s 922us/step - loss: 0.2491 - accuracy: 0.5301 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 888/1000\n",
      "563/563 [==============================] - 1s 974us/step - loss: 0.2492 - accuracy: 0.5281 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 889/1000\n",
      "563/563 [==============================] - 1s 899us/step - loss: 0.2492 - accuracy: 0.5282 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 890/1000\n",
      "563/563 [==============================] - 1s 931us/step - loss: 0.2487 - accuracy: 0.5367 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 891/1000\n",
      "563/563 [==============================] - 1s 950us/step - loss: 0.2491 - accuracy: 0.5307 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 892/1000\n",
      "563/563 [==============================] - 1s 927us/step - loss: 0.2492 - accuracy: 0.5276 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 893/1000\n",
      "563/563 [==============================] - 1s 908us/step - loss: 0.2495 - accuracy: 0.5235 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 894/1000\n",
      "563/563 [==============================] - 1s 927us/step - loss: 0.2492 - accuracy: 0.5280 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 895/1000\n",
      "563/563 [==============================] - 1s 958us/step - loss: 0.2490 - accuracy: 0.5318 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 896/1000\n",
      "563/563 [==============================] - 1s 924us/step - loss: 0.2491 - accuracy: 0.5297 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 897/1000\n",
      "563/563 [==============================] - 1s 908us/step - loss: 0.2489 - accuracy: 0.5334 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 898/1000\n",
      "563/563 [==============================] - 1s 918us/step - loss: 0.2492 - accuracy: 0.5281 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 899/1000\n",
      "563/563 [==============================] - 1s 972us/step - loss: 0.2491 - accuracy: 0.5305 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 900/1000\n",
      "563/563 [==============================] - 1s 918us/step - loss: 0.2495 - accuracy: 0.5231 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 901/1000\n",
      "563/563 [==============================] - 1s 901us/step - loss: 0.2491 - accuracy: 0.5299 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 902/1000\n",
      "563/563 [==============================] - 1s 933us/step - loss: 0.2492 - accuracy: 0.5293 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 903/1000\n",
      "563/563 [==============================] - 1s 938us/step - loss: 0.2491 - accuracy: 0.5309 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 904/1000\n",
      "563/563 [==============================] - 1s 931us/step - loss: 0.2491 - accuracy: 0.5306 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 905/1000\n",
      "563/563 [==============================] - 1s 925us/step - loss: 0.2491 - accuracy: 0.5300 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 906/1000\n",
      "563/563 [==============================] - 1s 936us/step - loss: 0.2493 - accuracy: 0.5274 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 907/1000\n",
      "563/563 [==============================] - 1s 965us/step - loss: 0.2492 - accuracy: 0.5292 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 908/1000\n",
      "563/563 [==============================] - 1s 895us/step - loss: 0.2490 - accuracy: 0.5319 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 909/1000\n",
      "563/563 [==============================] - 1s 904us/step - loss: 0.2492 - accuracy: 0.5280 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 910/1000\n",
      "563/563 [==============================] - 1s 962us/step - loss: 0.2492 - accuracy: 0.5279 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 911/1000\n",
      "563/563 [==============================] - 1s 925us/step - loss: 0.2495 - accuracy: 0.5236 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 912/1000\n",
      "563/563 [==============================] - 1s 901us/step - loss: 0.2492 - accuracy: 0.5276 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 913/1000\n",
      "563/563 [==============================] - 1s 997us/step - loss: 0.2492 - accuracy: 0.5285 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 914/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2495 - accuracy: 0.5236 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 915/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2497 - accuracy: 0.5197 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 916/1000\n",
      "563/563 [==============================] - 1s 943us/step - loss: 0.2494 - accuracy: 0.5257 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 917/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2491 - accuracy: 0.5303 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 918/1000\n",
      "563/563 [==============================] - 1s 941us/step - loss: 0.2493 - accuracy: 0.5271 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 919/1000\n",
      "563/563 [==============================] - 1s 904us/step - loss: 0.2492 - accuracy: 0.5280 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 920/1000\n",
      "563/563 [==============================] - 1s 892us/step - loss: 0.2495 - accuracy: 0.5230 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 921/1000\n",
      "563/563 [==============================] - 1s 959us/step - loss: 0.2491 - accuracy: 0.5300 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 922/1000\n",
      "563/563 [==============================] - 1s 924us/step - loss: 0.2492 - accuracy: 0.5288 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 923/1000\n",
      "563/563 [==============================] - 1s 924us/step - loss: 0.2492 - accuracy: 0.5290 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 924/1000\n",
      "563/563 [==============================] - 1s 911us/step - loss: 0.2493 - accuracy: 0.5266 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 925/1000\n",
      "563/563 [==============================] - 1s 995us/step - loss: 0.2496 - accuracy: 0.5211 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 926/1000\n",
      "563/563 [==============================] - 1s 958us/step - loss: 0.2487 - accuracy: 0.5370 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 927/1000\n",
      "563/563 [==============================] - 1s 899us/step - loss: 0.2491 - accuracy: 0.5310 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 928/1000\n",
      "563/563 [==============================] - 1s 917us/step - loss: 0.2493 - accuracy: 0.5266 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 929/1000\n",
      "563/563 [==============================] - 1s 991us/step - loss: 0.2491 - accuracy: 0.5293 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 930/1000\n",
      "563/563 [==============================] - 1s 918us/step - loss: 0.2491 - accuracy: 0.5306 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 931/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2493 - accuracy: 0.5272 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 932/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2490 - accuracy: 0.5315 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 933/1000\n",
      "563/563 [==============================] - 1s 979us/step - loss: 0.2490 - accuracy: 0.5322 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 934/1000\n",
      "563/563 [==============================] - 1s 945us/step - loss: 0.2489 - accuracy: 0.5342 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 935/1000\n",
      "563/563 [==============================] - 1s 893us/step - loss: 0.2492 - accuracy: 0.5290 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 936/1000\n",
      "563/563 [==============================] - 1s 938us/step - loss: 0.2489 - accuracy: 0.5336 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 937/1000\n",
      "563/563 [==============================] - 1s 936us/step - loss: 0.2491 - accuracy: 0.5299 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 938/1000\n",
      "563/563 [==============================] - 1s 909us/step - loss: 0.2490 - accuracy: 0.5327 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 939/1000\n",
      "563/563 [==============================] - 1s 922us/step - loss: 0.2493 - accuracy: 0.5268 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 940/1000\n",
      "563/563 [==============================] - 1s 988us/step - loss: 0.2493 - accuracy: 0.5258 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 941/1000\n",
      "563/563 [==============================] - 1s 920us/step - loss: 0.2491 - accuracy: 0.5301 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 942/1000\n",
      "563/563 [==============================] - 1s 911us/step - loss: 0.2490 - accuracy: 0.5315 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 943/1000\n",
      "563/563 [==============================] - 1s 899us/step - loss: 0.2495 - accuracy: 0.5223 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 944/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2492 - accuracy: 0.5281 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 945/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2491 - accuracy: 0.5294 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 946/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2496 - accuracy: 0.5209 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 947/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2495 - accuracy: 0.5222 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 948/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2490 - accuracy: 0.5312 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 949/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2490 - accuracy: 0.5314 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 950/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2492 - accuracy: 0.5286 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 951/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2494 - accuracy: 0.5253 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 952/1000\n",
      "563/563 [==============================] - 1s 998us/step - loss: 0.2495 - accuracy: 0.5238 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 953/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2493 - accuracy: 0.5264 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 954/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2490 - accuracy: 0.5323 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 955/1000\n",
      "563/563 [==============================] - 1s 896us/step - loss: 0.2489 - accuracy: 0.5332 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 956/1000\n",
      "563/563 [==============================] - 1s 906us/step - loss: 0.2489 - accuracy: 0.5332 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 957/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2493 - accuracy: 0.5269 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 958/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2493 - accuracy: 0.5258 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 959/1000\n",
      "563/563 [==============================] - 1s 961us/step - loss: 0.2492 - accuracy: 0.5293 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 960/1000\n",
      "563/563 [==============================] - 1s 911us/step - loss: 0.2493 - accuracy: 0.5267 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 961/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2489 - accuracy: 0.5343 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 962/1000\n",
      "563/563 [==============================] - 1s 902us/step - loss: 0.2494 - accuracy: 0.5243 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 963/1000\n",
      "563/563 [==============================] - 1s 904us/step - loss: 0.2491 - accuracy: 0.5298 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 964/1000\n",
      "563/563 [==============================] - 1s 931us/step - loss: 0.2489 - accuracy: 0.5329 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 965/1000\n",
      "563/563 [==============================] - 1s 981us/step - loss: 0.2491 - accuracy: 0.5298 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 966/1000\n",
      "563/563 [==============================] - 1s 895us/step - loss: 0.2491 - accuracy: 0.5311 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 967/1000\n",
      "563/563 [==============================] - 1s 901us/step - loss: 0.2491 - accuracy: 0.5307 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 968/1000\n",
      "563/563 [==============================] - 1s 940us/step - loss: 0.2490 - accuracy: 0.5323 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 969/1000\n",
      "563/563 [==============================] - 1s 947us/step - loss: 0.2490 - accuracy: 0.5319 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 970/1000\n",
      "563/563 [==============================] - 1s 918us/step - loss: 0.2488 - accuracy: 0.5359 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 971/1000\n",
      "563/563 [==============================] - 1s 890us/step - loss: 0.2492 - accuracy: 0.5291 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 972/1000\n",
      "563/563 [==============================] - 1s 934us/step - loss: 0.2495 - accuracy: 0.5236 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 973/1000\n",
      "563/563 [==============================] - 1s 922us/step - loss: 0.2494 - accuracy: 0.5255 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 974/1000\n",
      "563/563 [==============================] - 1s 915us/step - loss: 0.2494 - accuracy: 0.5240 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 975/1000\n",
      "563/563 [==============================] - 1s 901us/step - loss: 0.2490 - accuracy: 0.5319 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 976/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2490 - accuracy: 0.5320 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 977/1000\n",
      "563/563 [==============================] - 1s 977us/step - loss: 0.2491 - accuracy: 0.5294 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 978/1000\n",
      "563/563 [==============================] - 1s 933us/step - loss: 0.2496 - accuracy: 0.5216 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 979/1000\n",
      "563/563 [==============================] - 1s 915us/step - loss: 0.2493 - accuracy: 0.5258 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 980/1000\n",
      "563/563 [==============================] - 1s 981us/step - loss: 0.2493 - accuracy: 0.5268 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 981/1000\n",
      "563/563 [==============================] - 1s 920us/step - loss: 0.2492 - accuracy: 0.5280 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 982/1000\n",
      "563/563 [==============================] - 1s 901us/step - loss: 0.2493 - accuracy: 0.5265 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 983/1000\n",
      "563/563 [==============================] - 1s 956us/step - loss: 0.2490 - accuracy: 0.5314 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 984/1000\n",
      "563/563 [==============================] - 1s 954us/step - loss: 0.2493 - accuracy: 0.5266 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 985/1000\n",
      "563/563 [==============================] - 1s 918us/step - loss: 0.2492 - accuracy: 0.5281 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 986/1000\n",
      "563/563 [==============================] - 1s 904us/step - loss: 0.2494 - accuracy: 0.5245 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 987/1000\n",
      "563/563 [==============================] - 1s 958us/step - loss: 0.2494 - accuracy: 0.5246 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 988/1000\n",
      "563/563 [==============================] - 1s 945us/step - loss: 0.2493 - accuracy: 0.5270 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 989/1000\n",
      "563/563 [==============================] - 1s 933us/step - loss: 0.2492 - accuracy: 0.5286 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 990/1000\n",
      "563/563 [==============================] - 1s 906us/step - loss: 0.2492 - accuracy: 0.5289 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 991/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2490 - accuracy: 0.5314 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 992/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2493 - accuracy: 0.5268 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 993/1000\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.2494 - accuracy: 0.5250 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 994/1000\n",
      "563/563 [==============================] - 1s 970us/step - loss: 0.2493 - accuracy: 0.5259 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 995/1000\n",
      "563/563 [==============================] - 1s 961us/step - loss: 0.2495 - accuracy: 0.5240 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 996/1000\n",
      "563/563 [==============================] - 1s 922us/step - loss: 0.2490 - accuracy: 0.5316 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 997/1000\n",
      "563/563 [==============================] - 1s 909us/step - loss: 0.2491 - accuracy: 0.5310 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 998/1000\n",
      "563/563 [==============================] - 1s 974us/step - loss: 0.2493 - accuracy: 0.5259 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 999/1000\n",
      "563/563 [==============================] - 1s 954us/step - loss: 0.2494 - accuracy: 0.5240 - val_loss: 0.2482 - val_accuracy: 0.5459\n",
      "Epoch 1000/1000\n",
      "563/563 [==============================] - 1s 901us/step - loss: 0.2490 - accuracy: 0.5324 - val_loss: 0.2482 - val_accuracy: 0.5459\n"
     ]
    }
   ],
   "source": [
    "# Fit the model using 50 epochs and the training data\n",
    "fit_model_A29 = nn_A29.fit(X_train_scaled, y_train, validation_split=0.3, epochs=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alternative Model 29 Results\n",
      "Loss: 0.24914748966693878, Accuracy: 0.5292128324508667\n"
     ]
    }
   ],
   "source": [
    "print(\"Alternative Model 29 Results\")\n",
    "# Evaluate the model loss and accuracy metrics using the evaluate method and the test data\n",
    "model_loss, model_accuracy = nn_A29.evaluate(X_test_scaled,y_test,verbose=0)\n",
    "# Display the model loss and accuracy results\n",
    "print(f\"Loss: {model_loss}, Accuracy: {model_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: After finishing your models, display the accuracy scores achieved by each model, and compare the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Model Results\n",
      "Loss: 0.5549191236495972, Accuracy: 0.7295626997947693\n"
     ]
    }
   ],
   "source": [
    "print(\"Original Model Results\")\n",
    "# Evaluate the model loss and accuracy metrics using the evaluate method and the test data\n",
    "# model_loss_00, model_accuracy_00 = nn.evaluate(X_test_scaled,y_test,verbose=1)\n",
    "# Display the model loss and accuracy results\n",
    "print(f\"Loss: {model_loss_00}, Accuracy: {model_accuracy_00}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alternative Model 41 Results\n",
      "Loss: 0.1888144314289093, Accuracy: 0.7306122183799744\n"
     ]
    }
   ],
   "source": [
    "print(\"Alternative Model 41 Results\")\n",
    "# Evaluate the model loss and accuracy metrics using the evaluate method and the test data\n",
    "model_loss, model_accuracy = nn_A41.evaluate(X_test_scaled,y_test,verbose=0)\n",
    "# Display the model loss and accuracy results\n",
    "print(f\"Loss: {model_loss}, Accuracy: {model_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alternative Model 25 Results\n",
      "Loss: 0.18960236012935638, Accuracy: 0.7255976796150208\n"
     ]
    }
   ],
   "source": [
    "print(\"Alternative Model 25 Results\")\n",
    "# Evaluate the model loss and accuracy metrics using the evaluate method and the test data\n",
    "model_loss, model_accuracy = nn_A25.evaluate(X_test_scaled,y_test,verbose=0)\n",
    "# Display the model loss and accuracy results\n",
    "print(f\"Loss: {model_loss}, Accuracy: {model_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alternative Model 26 Results\n",
      "Loss: 0.19804491102695465, Accuracy: 0.7269970774650574\n"
     ]
    }
   ],
   "source": [
    "print(\"Alternative Model 26 Results\")\n",
    "# Evaluate the model loss and accuracy metrics using the evaluate method and the test data\n",
    "model_loss, model_accuracy = nn_A26.evaluate(X_test_scaled,y_test,verbose=0)\n",
    "# Display the model loss and accuracy results\n",
    "print(f\"Loss: {model_loss}, Accuracy: {model_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alternative Model 27 Results\n",
      "Loss: 0.19206713140010834, Accuracy: 0.7251312136650085\n"
     ]
    }
   ],
   "source": [
    "print(\"Alternative Model 27 Results\")\n",
    "# Evaluate the model loss and accuracy metrics using the evaluate method and the test data\n",
    "model_loss, model_accuracy = nn_A27.evaluate(X_test_scaled,y_test,verbose=0)\n",
    "# Display the model loss and accuracy results\n",
    "print(f\"Loss: {model_loss}, Accuracy: {model_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alternative Model 28 Results\n",
      "Loss: 0.1905694156885147, Accuracy: 0.7293294668197632\n"
     ]
    }
   ],
   "source": [
    "print(\"Alternative Model 28 Results\")\n",
    "# Evaluate the model loss and accuracy metrics using the evaluate method and the test data\n",
    "model_loss, model_accuracy = nn_A28.evaluate(X_test_scaled,y_test,verbose=0)\n",
    "# Display the model loss and accuracy results\n",
    "print(f\"Loss: {model_loss}, Accuracy: {model_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alternative Model 29 Results\n",
      "Loss: 0.24914748966693878, Accuracy: 0.5292128324508667\n"
     ]
    }
   ],
   "source": [
    "print(\"Alternative Model 29 Results\")\n",
    "# Evaluate the model loss and accuracy metrics using the evaluate method and the test data\n",
    "model_loss, model_accuracy = nn_A29.evaluate(X_test_scaled,y_test,verbose=0)\n",
    "# Display the model loss and accuracy results\n",
    "print(f\"Loss: {model_loss}, Accuracy: {model_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Day 14-2 cell [62]\n",
    "plt.plot(fit_model_A41.history[\"loss\"])\n",
    "plt.plot(fit_model_A25.history[\"loss\"])\n",
    "plt.plot(fit_model_A26.history[\"loss\"])\n",
    "plt.plot(fit_model_A27.history[\"loss\"])\n",
    "plt.plot(fit_model_A28.history[\"loss\"])\n",
    "plt.plot(fit_model_A29.history[\"loss\"])\n",
    "\n",
    "plt.title(\"loss_function - Training\")\n",
    "plt.legend([\"41 - 1/6 Layers Mean to 1\",\n",
    "            \"25 - ADAM\",\n",
    "            \"26 - ADELTA\",\n",
    "            \"27 - RMSPROP\",\n",
    "            \"28 - ADAGRAD\",\n",
    "            \"29 - SGD\"\n",
    "           ])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2(b): Plot and Compare the Results of each Alternative Model to the Original "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Day 14-2 cell [62]\n",
    "plt.plot(fit_model_00.history[\"loss\"])\n",
    "plt.plot(fit_model_A41.history[\"loss\"])\n",
    "plt.plot(fit_model_A25.history[\"loss\"])\n",
    "plt.plot(fit_model_A26.history[\"loss\"])\n",
    "plt.plot(fit_model_A27.history[\"loss\"])\n",
    "plt.plot(fit_model_A28.history[\"loss\"])\n",
    "plt.plot(fit_model_A29.history[\"loss\"])\n",
    "\n",
    "plt.title(\"loss_function - Training\")\n",
    "plt.legend([\"Baseline\", \n",
    "            \"41 - 1/6 Layers Mean to 1\",\n",
    "            \"25 - ADAM\",\n",
    "            \"26 - ADELTA\",\n",
    "            \"27 - RMSPROP\",\n",
    "            \"28 - ADAGRAD\",\n",
    "            \"29 - SGD\"\n",
    "           ])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAEICAYAAABRSj9aAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAABd+klEQVR4nO3dd3hUVfrA8e97p6Q3QhJKKKH3GhAUFQt2QLGBZdX1t8jaC7ZVd1lXtth31dXFVeyAiIoFFVRUFkTpvUOAECCF9GQy5Z7fHzMZkpCESUiByfk8zzyZ2849d2byzplzz32vKKXQNE3TgpfR3BXQNE3TGpcO9JqmaUFOB3pN07QgpwO9pmlakNOBXtM0LcjpQK9pmhbkdKDXgoqIhInI5yKSLyJzm3jfm0RkdFPusyGJyPUisrCh19Wan+hx9FowEZEbgbuA05VS7kbcz1tAulLq8cbaR4D1eA24wTdpBwQo800vUUpd3CwV004qukWvNRrxaurPWCdge2MG+ZOJUmqKUipSKRUJ/BWYUz5dMciLiLX5aqk1Nx3og5yIPCIiu0SkUEQ2i8gVVZb/TkS2VFg+xDe/g4h8LCJZIpIjIi/75k8TkfcqbN9ZRFR5IBGRH0RkuogsBUqALiJyS4V97BaR26rUYbyIrBWRAl9dLxKRq0VkVZX1HhCRT2s51j8DfwSuFZEiEbk1wPr+RUSW+uq3UERaV1h/lIgsE5E8EdkvIjeLyGTgeuAh334+962bJiLn+56HiMiLIpLhe7woIiG+ZaNFJN13PJkiclBEbgnwLQ2Yrz4Pi8h6oFhErLV9HnzH9r8K00pEpojIDhHJFZFXRETqsa5FRJ4TkWwR2SMid1Z8D7QmoJTSjyB+AFcD7fB+qV8LFANtKyw7AAzD+5O/G94WsQVYB7wARAChwCjfNtOA9yqU3xlQgNU3/QOwD+gLWAEbcCnQ1bePs/F+AQzxrT8cyAfG+OrYHugFhABHgN4V9rUGuPI4x1u1foHUdxfQAwjzTf/dt6wjUAhM8h1HPDDIt+wt4Kkq+04Dzvc9fxJYDiQCCcAy4C++ZaMBt28dG3CJ7zWJO8H3uuqxpgFrgQ5AWACfh5uB/1XYXgFfALG+1yILuKge604BNgPJQBzwbcX3QD8a/6Fb9EFOKTVXKZWhlDKVUnOAHXiDK8D/AU8rpVYor51Kqb2+5e2AB5VSxUoph1LqfzXsojpvKaU2KaXcSimXUupLpdQu3z5+BBYCZ/rWvRV4Uym1yFfHA0qprUqpMmAOvv5nEemLN0h/cYIvSXVmKqW2K6VKgQ+BQb751wPfKqVm+Y4jRym1NsAyrweeVEplKqWygD8DN1ZY7vItdymlFgBFQM+GOJgq/qWU2u87tuN9Hqrzd6VUnlJqH7CYo69NXda9BvinUipdKZUL/P0Ej0mrIx3og5yI/MbXLZInInlAP6C8a6ID3tZsVR2Avar+/dz7q9ThYhFZLiJHfHW4JIA6ALwNXOfrArgR+ND3BdDQDlV4XgJEBlC342kH7K0wvdc3r1xOlde34n79RORMX/dQkYhsqkc9qr4XtX0eqlPTa1OXddtVqUelOmmNTwf6ICYinYDXgTuBeKVULLARbxcKeP/hulaz6X6gYw19qMVAeIXpNtWs4x/K5euXngc8CyT56rAggDqglFoOOPG2/q8D3q1uveMIpL41qbFuVDjGGmTg7QYr19E3r06UUkvU0ZOrfeu6PZXfi+N9HhrLQbzdNuU6NPL+tCp0oA9uEXj/0bMAfCf8+lVY/l9gqogMFa9uvmDwK95/zr+LSISIhIrIGb5t1gJniUhHEYkBHj1OHex4+9uzALeIXAxcUGH5G8AtInKeiBgi0l5EelVY/g7wMuCuY/dRubrWt6L3gfNF5Brficx4ERnkW3YY6FLLtrOAx0UkwXdy94/Ae7Ws3xSO93loLB8C9/je21jg4SbYp1aBDvRBTCm1GXgO+BlvYOoPLK2wfC4wHfgA70nHT4FWSikPMBbvydl9QDreE3copRbh7TtfD6ziOH3mSqlC4G68/+y5eFvmn1VY/itwC94Tv/nAj1RuCb+LNxjVpzVf5/pW2XYf3m6mB/CeGF4LDPQtfgPo4+sC+bSazZ8CVvr2uwFY7ZvXbI73eWhEr+M9L7Me7wn1BXhPRnuaYN8a+oIp7SQnImFAJt5ROjuauz7aifP9qntNKdXpuCtrDUK36LWT3e+BFTrIn7rEm5biEl/3V3vgT8AnzV2vlkS36LWTloik4T1ReLlSak2F+Zuo3L1T7jal1PtNVD0tQCISjrdLrhdQCnwJ3KOUKmjWirUgOtBrmqYFOd11o2maFuROylwTrVu3Vp07d27uamiapp0yVq1ala2USqhu2UkZ6Dt37szKlSubuxqapmmnDBHZW9My3XWjaZoW5HSg1zRNC3IBBXrx5gffJiI7ReSRapY/6EuUtFZENoqIR0Ra+ZalicgG3zLdH6NpmtbEjttHLyIW4BW8+cLTgRUi8pnvcmoAlFLPAM/41h8L3KeUOlKhmHOUUtkNWnNN0zQtIIG06IcDO5VSu5VSTmA2ML6W9SfhTeikaZqmnQQCCfTtqZw/Ot037xi+K+AuwpuWtpwCForIKvHegq1aIjJZRFaKyMqsrKwAqqVpmqYFIpBAX12u6poupx0LLK3SbXOGUmoIcDFwh4icVd2GSqkZSqlUpVRqQkK1Q0E1TdO0eghkHH06lW8UkEzNN1CYSJVuG6VUhu9vpoh8grcr6Ke6V1XTNK1hbMzeiCEGfeL7oJSi0FVImCUMQwwshgUAt+nGVCZ2ix3w3l87x5FDXEgceWV5RNojyS7NZlfeLoYkDiHcFo4hBkopHB4HYdYwABxuB29teovYkFiGtxlOqbuU7NJsTm9/OjbD1iTHG0igXwF0F5EUvDeSnog3p3glvps6nI3vHp++eRGAoZQq9D2/AO8NkTWtWSilOFB0gAhbBHGhcXXaDsB7V8Pa7c7fTUp0SrXr5jnysBpWDDEItx298ZXbdHO45DDtItohIiil+GbvN7QJb8OgxEF4TA+5Zbm0Dmvtr8/B4oOUuEroFtfNX05GUQbh1nBiQ2PJL8sn15HL6szVXNT5In4++DNWsfJT+k/cPeRuYkJi2J2/m0JnIQMTBnK4+DBrs9ZiKpOLOl9Uqf5bcrbgMl30btWbrUe20iu+Fxax8Nmuzyh2FRNtjyYpPIkwaxhO08mP6T/SK64X53c6n18O/kJJWQElpTkYoTG0jWiLIQapCQNxCSw7sIyzks/iu72LyM3exhm9rmTG+hlsz93Ob/r+hsEJg1Eo1mSuYWjSUD7f9TmDEwfz2rrX6BXfi8EJgwm1hlLkKiLaHs2flv2JPvF9eGLEE2SVZlHiKiE5KplPd37Kusx1OE0nyzKWAWA37CgULtPlP9Z/nfNPlmcs54Nt3jZr3/i+CMLGnI0AhFhCKPM03B0t20W0o1erXhS5ipgycArD2gxrsLLLBZTUTEQuAV4ELHhv5DxdRKYAKKVe861zM967vk+ssF0XjqYjtQIfKKWmH29/qamp6pS4MlYpUCb4WgBZJVlEh0QTYgmpYV2FM/1XJLo9ttgOuEwXVrFS6i6lwFlAm4g2uE03btNNqDWU/YX7ibZHExMSg8t0sfLQSuLD4skqyaJNRBu6xHRBRCh0FrIpZxNbc7bSJ74PrULjeGnNy6QXpPHkmX+lb3xfDubuxh4aw4fbP6TwyC4e7D6JnPhO3Pndnfz59D/TPa47+WX57Cvcx5L0JWw7so3Jva5n7s55dIztDmWFrDy8kj1lOfw+ZTx5IRGcnXwW6sAq9hz4mVYm2JSim7OMtMQebIpPJjWyM6WH1pFsDedIx+HYN81nx+6FbGvXh8tKPZQ4C/m8dRv+u+dzzojuxmkhCeQVHaIoNIpMewjndjiPEZ3OZeHehWw9spXDRQeICYmjjbMUa2wn1hzZwrqsdTyRNJprzvozK/csJM8QNmZt4ED6UnoQwuueTCINO21DW5NecpAjptP/ljx71jMUZG5ijyOb4sIDfJyzhmhrBKVmGS7TzcWRXQlHyHYV8qszh1Ll5ozWA8n2ONiWu+2Yt/hmezJvOdOPmX9GbE9stggKDq1ltcX0z/9TVH/axXXjufSFbDeLa/yYDfVY2GiFMlX9fTraRLThUPEh+rfuz4bsDQBc2PlCvkn7psYyAzGitJTlYWEnVEZthpU6WBEW2mjlN7bU2J448/axnlLsCM7j3l0yML9e/6v/10BdiMgqpVRqtctOxuyVjRXoHS4Pmw8WMKRjHB6Ph1/Sctl/pIRhnVsxe8V+7olbRsSKlyGmA7QdACVHICIBM+UsPl8/k9KYdizNXkeMgrGJw+m+byWhe38mo8NQXjEK+VZKAbjdjOYnVza/cUCX8DYcyd7Cj63asioikq3uAkQpokyTUsPAVaXVF2sJo9R0UVbhvtEdItqyv/hgg78eWtMIMU3KjJPj2sTe1hi2uPPrvJ1FKfqXOQlVZqXgP8hRxtpQb8NmiMNBqKnYZ7PS0eWmyDBY71uW4HZzdkkpK8JC2WurubsizuOhRAS3CB7f/0aY73+ls9NFV5eLMKX4IjICgNHFJfwQ4f1l9GRWDmZUEu8bJXR2uXGKUGAICW4PuRYLrTwerioswi1CN6eL7XYb7d1u2rs9/LZNInkWg5cPZ/FteDjb7DYePJKHO6oN42INrikoIs700CoskdlSxJ+zj9DP6axU9wJDsClvg+eIxcIOuw0BEt0e2rjdrA8NIdX35XZ6qYNiEdzR7bEUHKDQYrDJbifGCGH41BozGdRKB3qfB2f/Sun6zxiSCL/Ne6nSsvUhdkpEaO/28EBia7aE2Bt8/1UNdJSxLrRy6z/O4/1Q1qbiP9eJivF4yPftr5PLxV6bDcM0uKEwj3diogE4rdTBL2GhRJomkaaJRcEB29Fev25OJwWhUWSaZSQowYbBJW4Lb1vLcIlwli2BFa5sSqu0eIYQwiFXMQ85bUSLld+Gu7ixwxguXf4uiabJXYnxbAo5epzRHg/jioq5La+Aud1P59Wyvdyem89/YqNxGAaXFhVzWkgif7R5W8dDHA7SbDbOCu9IWOZWvoiMYHJePjvCI9lggZGlDmJND6mJQzlgD2HVoV8Z5Ciju8vFrOgoJhUUUiZC+KX/ZF3eNtrmpnNOxlZyMzeRbbGwvV1f+tlbUarcZLTqRFZUAsOz95FshLE+ri2RhzYSv/VrIpXC0Xc86+PacebpDyOOPF7Z+Cav7fb+2H049UGuajuKsh0LcXY/j8jXL2CbWUSMaZJy7pOsD4/CkruHvuHt2Pbto7iBTi43EUpRJEKxYbA8LBSbUoyUSJarIspE6Opy0cbtYbfNSqqjrFI/7X6rhcOn38GgtsMxvn4Uw/RAYQZM/ADP7OuYFxXJoB7jSO51BaWHN9CqJB9Z8y70uBD36EcxXj8HbBEYbgeqOBNP73FYe4+FDsPBHgmFByEyCbX7RzZveI8+9njk/D/iyt7Jdsr44PAyxnYdS/uyMsyIeKLESlxcN2TdLNRndyJXzIAdCyH1Fsosdhw7FhJTVgg9L4b9KziUn0ZmzwsYENkBZ5t+FLuKj3bFHVwPK9+EVTO90z0ugu1fH/vhv+Ap6DYGEnuBo8D7y/yv7XzbXAwX/AVadwdnCbwxBg57u24Y9jvYuQhCYyE5FUrzvPUyLN7yygrhnfFQdBh+Mx9apcDO7+CjW6r/Jxx6C8Qkw6+vQ5fRMOE/tf3L1qhFB/r5aw8wffZiZtmfoqtxkP/GRPPPVrEAPJZ9hO22UBaGJBLlaI+hrCQVdqIoJI8D0dtx2IoxlIHbcGEoA6fFAQiIIsI0OXv/aPLN1mxss4Riez6DDw9hS/w2TjPsLA3JZ0RxEdnOXhSElzGhfUfOXzWXX7pdQlH6JHaGbGRMaSwDR3WkKHMjlu3fkp4bz9CkzZhnPYC769mEZ6xj6df3sLs4mWgpIsfdgSG2n+nfqj93F6eQ5wzl+Yh1tMrZyJaSC+gc/gMydjruT5/FgotVYX+gZ4eD5LXuSMKAMeSu28Zu1xYG2O2EbH6Ttdld2Zp3E93iv6ejZxW28NYcyI9lY/EVRIYUk9Q3jnajPST+8iOePldTtOILDljPoW3fBL7dMRfj10EYWLh4cj/2rMum69BEsvYVEhZlpzDHQddhHoodEUTao4lvH4nb5eHQrnzadY+lTJURZtjBWQShMdW/ef85m8OZG4gdPoWQ0+/B4/ZgpC9H5t2C5/T7KVvyBuFj7uJQfF+cc6YSfelzRA4eQ4kqxm4JIfSpJG85N38J3/0F9i+HcS/BgGvBGgLpKyEsDuK7Vt5vxlrUf85GzrwPTvs9RCX5FylTUZx1hEgOQ1Kf437+lGkivta86TExLN7npQ4Hi/YvIsuRya39b/WfA/C4TCRnB0fee5j4qx/B0WoopYVOcg4U0X1YErL/V9Qnt6P6jEP6Xo7MOAuPsmC07Ye6+BnMdqmYZU7c274lPCYMWveEiAQK9u0nZPEjWPb/hDOmN0yagxHVmtBIX+vaUQDZ272BK28fRLf3d0lWqzQPQqJgz0/w7uUw8k648Li9soFxloA9/Pjr1ZXb16/+VKL377RqftlM830W/5QHFX9t7/wO3puAGvY7Ss54DKUUSpl4XG6U6fFOm9553ucmyvR205mmB4/bjXIUwftXQ6suqEtfQB1aD4l9UYbF37VrWG107DegXofXIgP91kMF7Pzoz3iyf2ZJwiHWhYZwyGrF5glh2L5LiCqLJyW3f73Lj48uIKcguk7bxMZbyctxH39Fn+jWoZQWuXA5gu8eykMv6kR06zDcLhOr3WDX6kzcTpOC7FJK8p1EtgqhbddYDIvgcZvsWp2Fx20ev2CgQ59W5GeWEFW4iiTrFgpTrie/wEKUNZuconjyMkvp2DeeuDbh7FmXRVmJm1FXd+dIRjFrFu0jrk04uYdKsNoMouJDiYwLwR5mZdfqLCw2A4/LJDYpnN5ntCUs0o7pMVm5II2i3OOfoLPaDSxWA2epG1uolejWoZgeRUm+E0exq9Ztw6IMIuNsZO4tAGWiVBm2kBDcTg/gDTIoB97RzwJiwzAMfPEGQWHgxiQEBEIj7bTtEsPhtCzikix06JNAWJQiIjYM0+NCeY4GLdP0YPqClzJNykpKyEnfi6OoCDN3H57QeBDj6G82X1ypGl/80/75qsqkb7q8JFVlu+Ot75tnekxyDx7A9LhR6ugYcYUC0+PdzrBQuXreZQoFYqm8f5R3opHDZXhMLL+f8V69tm1xgf6N/+3hpwVv0yZpHj/bk8gPzaZNYQodc/sy6OC5x6wf2sHEsd8gPNrO4As6sv3Xw2TtKwQgtk0YeYdKj9kmKb6IwzmRALTtFsPBnce2DqrOD4uyERJuIyLGTp8z25GZVkhZqZtdqzKxhlgoLXAeU0a5YZelsOKLPQAYhmCagb9vUa1CiW8fgWExsIYYtE6OAiAk3Er+x09R6EkiuksX8kIHk3uwmMhID8n9k1n28c4aywwJtzJ8bBfyM0tYv9h7ArJ9j1iSUqIpLXTh8Zhs/+VwpW3Kg2QgDItgemo+xhCrA4crBFCIHO3/VsoNmN6T5JQ/fEEQF5gOb0AyS0CV+f6BTUTsKOVCsICUd3JUCC5KVZpWlZZ5fNt4A6wgUP6Qo89jE8MpzHWiFISGWyg6sgsRwWpzYJoGbqfTX4bFaqB8McliNXA781Cek+s8TUh4BOExsVhsNixW69FROr6/Uh5ey6PsMfOl4h//ikfLodL6ZSVuPG6TiNgQKq9wtOUtgMet8HgUMQmt2bWmkOjWYYRF2YlN9P5FhKIjDkLCbVhs3i9e//b+siqUWaX+YZFRWGx2Co84yN5fQsc+CaxZtI/ifBcgDBrTkbZdYxExEEPIySgmunUkYRF2xBDvPsT3OalQtoiBxWqlTdfux3/xq9GiAv38Nekc+eh+tpvDSc4dAsDqdosYengMFQct9BrRhh6ntcFhltK9b3s8HhOL72f1L5/tZuWCNDr1i+eS2wdQmOMgItaOxWrgKHbhdppEhTth4eMw5s8QFofL6cFR5KK00MmKL/Yw4vKuxLePpCjXgYhQUuAkoWPUceuvlGLtov1sXprBgHOSadMlBqfDTfsecShT+WLH0Q+ho8jF7nVZxCWFExppI2NHHn3OaEdhroNfP9vD2df1xBZSy0/wX18HaygMnAiWY0+SOUocHNiyHsNiISQ8AmWalBTm43G5sIWEYLo9lJU4EcMbyJXvJyhK4XK6KM7Lw2q343aW4SpzYrrKMJXBvk37cJflgWUQ7Xu0x1FcyuE9h7HZHfQZ1Q5biIHpMSktciEoMnbmYQ+1ENsmnINLPqGwzEGxE0AwLN7ALAIed+C/mE4WYVHR2EJDsdpD/O+tMk3vV4nvC0YMC12GDCc0Mor8LO9nKjYpBpejjLDoUCxWK4bFgsVmw3S7sYeH4y4r85ZT3uotOAgH11Pa5iwy9xbSOjmSMoeb/MxCxAilrESRscPp+1J0IxKGNxIZnP/bfiR2isYwDAyLBTEMIlvFY1TTvVOQXcqB7Xn0HNEGwzj6Wc1OL6JV23AMi4HHbbJj5WEsVoMugxM4sC2XTT9lcOHkfuxanUlS52gO7sxj15os+oxqx7ZfDrFzZSYAA8/twOALOlKQXcry+btJ7hXHr5/vCei1jmsbgc1ukLnX25Cz2AyueXQYrdpFcGhPPhExIUS1OnYkkNvp4cfZ20nsGEX7nnF8/q+1tf6C6zOqHQd35pF7qMQ/LzYpnLzD3unBF3RkzcJ9wNGG27h7B9GhV6uAjqM6LSbQuzwmT/51Gr2ORJJXeuzxhkbaSBnYmhHjuxIeXfPJ1vWL01kyZzsdescx7p7Bda5Hc1FKUVZSTN6hgxRkZ1JWXIyztJSi3Bxff6GvH9H3E1T5gkhZSQkepxO3y4np8WB63Hjc3r9FuUcozG64lBRWm93bX6kCa9lXR1C0iXLR0X4Is+dlqPhuKNPEsFiwh4Vjtdm8wchiwTAsGBbfwzAIiYhERLDYbIRGRhESHuHtlnC7sdpDUKaJy1nmbYMbBiCIIf6/UmV69pO/4nEL100bivj6WpUyq+2v9fbrHp0Oj4klPCYWq73xT/wHyulwYw+1cnhPAR/9o/L/YOsOkWTvLyK+fQRXPZKK1VY5yOdkFFFW4uaTZ1cDMPr6nhRkO3CWutn8vwxMU9G5fzzWEAt71mYH3BV3ouLaRpB7sBirzcBdwy9Ke5gVZ6m3kXD2pB78+sUeSgtdRMaFBNQlB3DVI6mkbz3C8k9317uut796TkDXalSntkB/Ut5hqr7eX76XDiWHyCudyIrkBSSNNFizeTvjN91NXJtwrn1sOBbb8Ye5hUV5W7Y1fShOFqbHwy+ffkj+4UMc2LaZ/MzD/hNAFVlsNgzDcjRAifcno/i6Fqx2O7aQUGyhoVgsVsRiwWKxYLWHkxgTxxnX3EBMYhKO4mKsNhthUdFY7XZcZWX+AFr+07P8F4f4pkOjolCmibO0FKvdTkSsd2SE2+kkM203juJC309WG/awMGISkxDD8H3YxdfzUeFnrgjGO5djObgGPE64+GzocWFTvuyV3Pz0RQDYQ4PjX6n8OJJSornjtXO9LVCBdd/tZ9MS7wXxOQeKydxbSPb+IvZuzCZ9Sy5DLurEygVplcr64f1jrzVI25Djf15bELWHWWnfI5Y9644mve0/OhmXw83W5Yeq3eaaPwwjNNLGO3/wXgzVa0QbzriqO6GRNpRSiAiH9xTw6YtrcJd5GHJRJ/IOl7B7TZY/yAP8OGu7//nxgvz5t/Th25mbGXpxJ5I6R5PYKYr9W3KJahXCsMtSKCtxk5NeROERB4PO78jnL62t1J0bkxhGfubRruF3HlvGDU+OrNSd1BCC49Pps2LnTjwhbektTi6+dCDXDr4Rzqt7OeFR3haW23lyBvqsfWns+GUZm378joIsbz94yuBUeo48i9DISEIjImndsbO/SyAsKrrerYSGEhZV+cS11W6nXY9e9SssqSekL/MV1LwX3ARLgK9JbJJ39MvZk3oy6pruzPvHKrL2FbJzVSYbFh+9OKxikI+KD8UeZiUnvahSWaeNS+GXz7xdLKOu7s7A8zqQe6iYn2ZvJyzSxsgJ3RARQiOt/l8LrjIPB7bn0rm/94rgjB25/kB/9qQetO8ZxwfTfmH09T39XaPXPDaM8Gg7ETFHh+aWf/6TUqL57dOj8LhNQiNsmKbC5XAz7+lV/m6WmMQwOvRqxcafDvi3H3NrH9qkxLB+cTqDzu+AaSocRS4SOkahTEW3oYn+/Vx+X4VegHhI6HC0y3bC1KEA7F6bhT3MSnJPb8PHUezijQeW4Ciq/YR8fQXVp/RQ2UyGH7mSjJid3Dnw9/UuJyTC+7I01U/Lulj7zZd89+arIEJMYhK9R43mwt/fg8XaNDkzTgpJfY8+T+zdfPVoYSwWg2v+MIw3H/pfpSBf7pwbehGTEEa7HrH8+sUef6C/4zXvAAhlKn+gH3ieN31WXJsIxt9bc/eoLcTiD/IAiZ2j6TI4gdPGdaFVW+9FU7c+e+bRYaJUDqw1lVl+3sowhJBwGxMeHEpeZgltUo4O9U0Z2JrPX1rHjU+NJLq19yKxUVcfPVEaHe+d12tkW+/xKYU7MwtP7hHcmZmUrl1H2Z7d4DFRpgdMBab3udVUmKbJXtME3+NsogiLtGGxjq61/vURVIE+KTeGSGccUcOK/YmJ6iM82tsSaNcttoFqVjemx8PhPTvJ3LOb4rxcctL3UZCdiaOwkLzDB2nTtTtj73uU6ITEZqlfs2vt+2dLORsiW+hr0Iy6DU5gw4/e1u4tT48iJMLqH8hQbtilKaz8Mo2Q8KMhRgxhykujcTnrP1zYarNw8W2Vh0VXDPL1FRJuJalTFMrj8Q8m6NA9it//a5R3uGZZmX9++cN7evPo9ME//onCrytfmGXr0AEjNATvaAWL95eFYYBheM8BGQYYghgWooxSLI10oWZQBfroUu+38a3XTDqhcsKj7Vw37TT/t3hTKcnPY+fK5ayYP4+8w5WH0rXt1pPwmFjEYuHM625puUEeoPOZcOHfYPD1zV2Tk4KnqBhPdpZvxBNUDD7KNI+Zh++kMKaqfn7VgObx4MkvwCwqQpkeIjLtQBQdExyUfTkPh39djm6H4qph3nk5b+46OjzVt48iUx0zD6W8rV+Xy3udgFmxHubR41PelrF32Kz3OMzCQkpWrvTV0axQD+/6/hFMVR8NLPbaa4k443SUw4GtfXvChw5t8H3UR1AFerszhjJrISEhJ/4NH9cmogFqFLgjGQd479F7cTlKiWvbnvP/73Y6DxxKRFwr7ygRa1C9VSfGsMDI25u7Fo1OKYUqKcFTWIhjyxbc2dl4srNxZ+fgyctDOctwpu2lbMeOJq1XfpsR0OtGPGt/4dDcDxt+Bzabt7VbPt7c99f/MAzv8PPy1rFvvr1TJ8L69QOLpfIw5PJrGfxlcLRc7wrVL6th+dFlFdYRwZqYQPTFF/uvhD6ZBFX0sHhCcFtLjr/iSaakIJ9Pn/4zAJP+8gxtu/dq9pOnWt0opTCLS/Dk5QKCJz8Pz5FcUCburCwcm7egPG5vi9XjAY/H2/L0/VUuF+6DB1Eul3e+aWIWFeGu5m5rRnQ0lthYjJAQLHFxxP9+CvZOnRCLhaNBiaPBkmqCmG/o6DGBTcTbzVAxoFksWKKjMSKjEItBZ7fC8l0mQ0bfRljEnZWDcPlFUBUCcHkdykdQVQzQ/oBdMYjrz36DC6pAL8qCklMvXcCmH74l92AG46Y+Rrse+uTiyc4sKaFg4ULy532M6XCAaeLcswezpOZGhhEejoSGeoOx74KjSn+tVqxJSd7+XMOCWAzEZsferSuYirCBA7B37IglPh4jpGES2tWXDTj75nbNWgetboIq0FvEQIyT7wKw2mxYvJBlH75P2+496T5sZHNXR6tAKYVyOPDk5qLcbpTbAx43ubNmkfvBLCwJrQnt7f1ijh7QH3tyMpbYWACMiEisCa0RqxWx2Qjp1euk/EmvtQxBFehRBhgn35DI6iil2L78fyx87V/EJ3fksnsebu4qtThKKVRpKWZpKcrhwCxzgjIpWb2arOdfwJObW+O2tg4d6PLZfIxGvDGHpjWUIAv0FpBTo0W/e/UKvnjxHxgWCxfcdnfLHkXTBErWrKFgwVc409JwZ2XhOXLE21J3VX+BSkjPnsRNmoiEhXn7w+12sFgRqwUMg7CBA3WQ104ZQRboDbCcGi36fN/wydtee4fw6BrysWs1MsvKvH3ibjfK48EsKcV96CDunCM4tmzBU5CPWey9AYkn5wglv/6KhIVhT+mMrU0bQvv2wRoXhyU2FgkLwwgNRewhYAhGWDgRI0foQK4FjeAK9Nj9aaRPdk6HA4CQ8Ea4wUKQ8hQVU7pmNUU//EjeRx+hymrIQ2K1YomJwRId7R/NET95Mq2n3IahX2+tBQqqQC9GCIScGkOzXGUODIu1ZaUuqAN3VhalGzZQun49ZTt24ty5E+e+faAUEhJCxIgRRJxxBmK3eUet2GwYdjv2rl2xd+qEEXrq3nRa0xpacAV6U5BTZGCDy+HAroPRMbL/M4OCb76mbPMW7wyLhZAuKYT06E70+HGE9upFxIgRumWuaXUQVIEe00BOkT56p6MUW2jL6gM2HQ5cBw9iFpegHKU4tm/HmZaGa99+ynbswCwpwZObi71TJ+Kuu46oMed7T3rqoK5pJySoAr2oU2ccvcvhwBbELfrCH36g5JdfEYtBSM+eONP2cmTmzGMuKpLwcKwJrQkd0B9LVDT2Th1pddNNiE75oGkNJqD/JhG5CPgnYAH+q5T6e5XlDwLlGaasQG8gQSl15HjbNiTDNBDLqXFlrMtRWm3XjXK5yP7PDG8Lt7CQkO7dscTG4MrMxJ2Zhdhs3odhULZjB+68XAx7CBISgiU2lpBu3bCnpGBNTCBs4ECsCQkNdqGO6/Bh3FnZeI7k4M45gmv/fsp27/ZeVFRUiHK6vPXauROzqMiXc0TAd3u/sIEDibnicizR0Uh4OKE9emBt21Zf8q5pjey4gV5ELMArwBggHVghIp8ppTaXr6OUegZ4xrf+WOA+X5A/7rYNSZRR6R6VJzOnw4Et5NhAnz1jBtkvv4w9JQXl8VC8zHuDDUtsLNaEBN8Vmm6Ux421VTwRw09DOcswy5y4s7PI/+STyq1mw/AG1tBQwocPo93f/16nwO/ct4+ixYspXrGCom+/O2a5vXNnJDwMS0QkRkwMeNyEjziN0F69iZ/8O5TThftghvfy/qgoHdQ1rRkE0qIfDuxUSu0GEJHZwHigpmA9CZhVz21PSKiEERcR2RhFNzhXmYPIuGNvBOzatx9ru7Z0/WoBAGZpKRhGwPlNlGniycnBlZFB6YaNuHOy8eTl4c7MouCzzzFCQmnzxONgs4Fp+hJhgfJ4OPLWWzg2b8ESG0vs1VchISGk33Enzt27wWYjdtJEIs84A0t8PNa4OIyoKKzx8bVXyG7H0r1+d7XXNK1hBBLo2wP7K0ynA6dVt6KIhAMXAXfWdduGEGYJo2Nsm8YqvkF5++iPPRlrOsswKrT063rRjhgG1oQErAnerptySimynn+BnNdfJ2/uXIzoaMyiIuxdUrC1b+/tBso4iCUmBk9+Prnvv+/ftvXtt9P6jtv9Xwqapp1aAgn01f3WrumM51hgqVLqSF23FZHJwGSAjh07BlCtY3k8CsNyanQN1NhHX+ZEGiE7oYiQ+MD9GBHhlK7fgDUpEeV0UrZ9B+6Dhwjp3p3Wk28j9tprKPhyARlTpxIzfjxxN1xPaN++OiGXpp3CAgn06UCHCtPJQEYN607kaLdNnbZVSs0AZgCkpqbWa+hM18EJx71f5MnCWUOLXpWVIY10OzGA1lOmHHedmMsuJXL0aCyRTXvzFU3TGkcggX4F0F1EUoADeIP5dVVXEpEY4Gzghrpu21DOv7lPYxXdoEyPx9t1U83JWFVWhmFv3nzjgA7ymhZEjhvolVJuEbkT+AbvEMk3lVKbRGSKb/lrvlWvABYqpYqPt21DH8SpxON2s3rBfJQySUrpesxy0+n05mjRNE1rIAGNo1dKLQAWVJn3WpXpt4C3Atm2Jfv107ksm+s90dlp4OBjlitn4/TRa5rWcukzbE3sSEY6AOf99vfYa+ijNxqxj17TtJZHB/omVpCVSYe+Axh04aXVLldlZYhNB3pN0xqODvRNrCDrcK13kzLLynTXjaZpDUoH+ibkdrkoysslunX1gV4phaegAEuMPhmraVrD0YG+CRXmZIFSxCQmVbtclZSAy4UlRt9aUNO0hqMDfRPK3pcGUGPXjSc/H8CbHEzTNK2B6KTfTeiz5/4KQExC5RZ90ZL/kfXii3hycwF0i17TtAalW/RNyGLz3h82Kr51pfm5H3yAY9MmwgYNIuaqKwkfNqw5qqdpWpDSLfom1Kp9B6JbH3sjEHdWFhFnnkn7559rppppmhbMdIu+CXmcTizVjJF3Z2VhTUhohhppmtYS6EDfhNwuJzZ75UCvTBN3To4O9JqmNRod6JuQ2+n099OX8+TmgtuNtXXrGrbSNE07MTrQNyG304m1SgriktWrAXSLXtO0RqMDfRNyO51Yq7ToM59+BgB7l5TmqJKmaS2ADvRNxDQ9mB73MS16s7iYqIsvIrRHj2aqmaZpwU4H+ibicboAjumjN0tLsbVt1xxV0jSthdCBvol4PG4ALNajgV6ZJqq0FCPs2Lz0mqZpDUUH+iZiejwAGJajL7kqLfXOC9eBXtO0xqMDfRMx3d4WvWE5ejGy6Qv0olv0mqY1Ih3om4hplrfoLUfnlZR454WHN0udNE1rGXSgbyKmu5pAX+rwzqvm3rGapmkNRQf6JuJv0VuPdt0oZxkAom8GrmlaI9KBvon4++iNoy165XQCIHYd6DVNazwBBXoRuUhEtonIThF5pIZ1RovIWhHZJCI/VpifJiIbfMtWNlTFTzWe8lE31mMDvaEDvaZpjei4+ehFxAK8AowB0oEVIvKZUmpzhXVigX8DFyml9olI1XvlnaOUym64ap96VHmg1y16TdOaWCAt+uHATqXUbqWUE5gNjK+yznXAx0qpfQBKqcyGreapr7xFb6l4MlYHek3TmkAggb49sL/CdLpvXkU9gDgR+UFEVonIbyosU8BC3/zJJ1bdU5e/RV/pZKwO9JqmNb5AbiUo1cxT1ZQzFDgPCAN+FpHlSqntwBlKqQxfd84iEdmqlPrpmJ14vwQmA3Ts2LEux3BKKE+BULnrxpv/Rgd6TdMaUyAt+nSgQ4XpZCCjmnW+VkoV+/rifwIGAiilMnx/M4FP8HYFHUMpNUMplaqUSk0IwtzsZi0nY3Wg1zStMQUS6FcA3UUkRUTswETgsyrrzAfOFBGriIQDpwFbRCRCRKIARCQCuADY2HDVP3WY+mSspmnN5LhdN0opt4jcCXwDWIA3lVKbRGSKb/lrSqktIvI1sB4wgf8qpTaKSBfgExEp39cHSqmvG+tgTmZmeddNhT56d+ZhAKSaG4ZrmqY1lED66FFKLQAWVJn3WpXpZ4Bnqszbja8Lp6UzPSZwNAVCyeo15Lz+X8RmwwgNqW1TTdO0E6KvjG0i/ha9xYIyTfbeeCMAbf/+N8Qa0PetpmlavehA30Q87vIbj1hRLhd4PERfeikxl17azDXTNC3Y6UDfRDwu360ErTZvoAdC+/ZtzippmtZC6EDfRPwtetvRQK9H22ia1hR0oG8iHnd5i9569EKpKjcK1zRNaww60DcRf9eNzYZy+cbP60CvaVoT0IG+iXgq3DNWpz7QNK0p6UDfRDxuFxarFRE52kevW/SapjUBHeibiOl2YfEFdn/qAx3oNU1rAjrQNxG3y43F6gv0etSNpmlNSAf6JuJxebtuAN11o2lak9KBvolU6rrRgV7TtCakA30TcTnLsNq9yctUmQPQXTeapjUNHeibSGlBAWFR0QDkzpoNgL1jh9o20TRNaxA60DeRkvw8wqNjUC4XxcuXY4mPxxId3dzV0jStBdCBvgl4Cgspyc8jLDKSsh07wO0m6ZFHmrtamqa1EDoReiNQHg/Z/36VggULcO7Zg8NqwdG3M8533mfP8/8GIHx4tbfO1TRNa3A60DeC4mXLyH7lFaxt2yI2G67+fcBTRMdLx9I6MgZ7xw7YkhKbu5qaprUQOtA3ME9hIVkvvAhA1y+/QGw2dq5dCc9OJ3HSJBI6d2neCmqa1uLoQN/Ajrz9Do7Nm4m6+CKM8HAA3A7vcMry4ZWapmlNSQf6BuJMS2P/HXfi3LULW7t2tPvHP/zLXM4yAGz6JuCapjUDHegbgCszk91jx6FcLkL69Kbtn5/EqHAxlMvhC/T20OaqoqZpLZgO9CeobM8eMp99DuVy0e7pfxAzbtwx67h1i17TtGYU0Dh6EblIRLaJyE4RqXYAuIiMFpG1IrJJRH6sy7anGk9BAWW7duHcv5/0399O8U8/EX3JxUSPHVvt+q6yMsQwMCz6e1XTtKZ33MgjIhbgFWAMkA6sEJHPlFKbK6wTC/wbuEgptU9EEgPd9lR04N57KV72s3+63bPPEnPZpTWun595CFtIKCLSFNXTNE2rJJAm5nBgp1JqN4CIzAbGAxWD9XXAx0qpfQBKqcw6bHtKUR4Pxb/8iiUujoT778PeqRPhw4bVuL7b6WTniuX0OO30JqylpmnaUYEE+vbA/grT6cBpVdbpAdhE5AcgCvinUuqdALc96XkKCjjyzrsUfPklrgMHwOOhzZ/+RPRFF9a6XeGRbD59+i+4nWX0OuPsJqqtpmlaZYEE+ur6G1Q15QwFzgPCgJ9FZHmA23p3IjIZmAzQsWPHAKrVdI689RbZ/36V8OHDCRs8GEtsLFHnnnPc7bYu/YnMPbsASO7Tr7GrqWmaVq1AAn06UDGfbjKQUc062UqpYqBYRH4CBga4LQBKqRnADIDU1NRqvwyag3K5yP/8C8JSh9LpnbfrtO2v8z8C4JK7pmIL0UMrT3Uul4v09HQcvgvgNK05hIaGkpycjK0ONy4KJNCvALqLSApwAJiIt0++ovnAyyJiBex4u2deALYGsO1Jx52VhWPrVlyHDpH9yr9xHzpEwt1316mMnPT9OAoLAOg9anQj1FJraunp6URFRdG5c2d9Yl1rFkopcnJySE9PJyUlJeDtjhvolVJuEbkT+AawAG8qpTaJyBTf8teUUltE5GtgPWAC/1VKbQSobtu6HlxTO/y3v1OwYIF/Ovy004iuZVRNdQ5s9R7mtdP+3qB105qPw+HQQV5rViJCfHw8WVlZddouoIHdSqkFwIIq816rMv0M8Ewg257s3EeOANDlqwXY2rTBCAurcxk5B/ZjDQmhfc8+DV09rRnpIK81t/p8BvUVPNUwi4uJOPNMQurw06iq0sICImLjEEPf20XTtOalo1A1zKIijMiIEyrD43Ritembf2sNy2KxMGjQIAYOHMiQIUNYtmxZg5Z/880389FH3kEE//d//8fmzafsJS9aBbpFXw2zuBgj4sQCvdvlxGrXgV5rWGFhYaxduxaAb775hkcffZQff/yx9o3q6b///W+jlKs1PR3oq2EWF2M50UDvLMOiW/RB68+fb2JzRkGDltmnXTR/Gts34PULCgqIi4sDoKioiPHjx5Obm4vL5eKpp55i/PjxFBcXc80115Ceno7H4+GJJ57g2muvZdWqVdx///0UFRXRunVr3nrrLdq2bVup/NGjR/Pss8+SmppKZGQk99xzD1988QVhYWHMnz+fpKQksrKymDJlCvv27QPgxRdf5Iwzzmi4F0VrEDrQV2E6nd4WfXT0CZXjdrqwhuhslVrDKi0tZdCgQTgcDg4ePMj3338PeMdWf/LJJ0RHR5Odnc2IESMYN24cX3/9Ne3atePLL78EID8/H5fLxV133cX8+fNJSEhgzpw5PPbYY7z55ps17re4uJgRI0Ywffp0HnroIV5//XUef/xx7rnnHu677z5GjRrFvn37uPDCC9myZUuTvBZa4HSgr8J96BAAtrbtTqwcp5PQyMiGqJJ2EqpLy7shVey6+fnnn/nNb37Dxo0bUUrxhz/8gZ9++gnDMDhw4ACHDx+mf//+TJ06lYcffpjLLruMM888k40bN7Jx40bGjBkDgMfjOaY1X5Xdbueyyy4DYOjQoSxatAiAb7/9tlI/fkFBAYWFhURFRTXC0Wv1pQN9Fa4M74W7trZtTqgcbx+9btFrjWfkyJFkZ2eTlZXFggULyMrKYtWqVdhsNjp37ozD4aBHjx6sWrWKBQsW8Oijj3LBBRdwxRVX0LdvX37++efj78THZrP5h/VZLBbcbjcApmny888/E1aPIcha09GjbqooWbESDIOQXr1OqBy3U5+M1RrX1q1b8Xg8xMfHk5+fT2JiIjabjcWLF7N3714AMjIyCA8P54YbbmDq1KmsXr2anj17kpWV5Q/0LpeLTZvqdx3jBRdcwMsvv+yfLv+1oZ1cdIu+irKdO7F37IjVd5KrPnauWE5B1mFcKV0bsGaadrSPHryXw7/99ttYLBauv/56xo4dS2pqKoMGDaKXr6GyYcMGHnzwQQzDwGaz8eqrr2K32/noo4+4++67yc/Px+12c++999K3b927o/71r39xxx13MGDAANxuN2eddRavvfba8TfUmpQoddLkD/NLTU1VK1eubPL9KqXY2rsP4aedRqe336p3OXP/8gf2bVzPuAf+QPfhOg99sNiyZQu9e/du7mpoWrWfRRFZpZRKrW593XVTQckvvwJgbd263mU4HaWkb9lM6tgJOshrmnZS0IHex7lvH4ee+gsAiQ8/VO9yCnOyMT1uEjt3aaiqaZqmnRAd6H1y3nwT14EMkv/9b2yJifUux+NyAegx9JqmnTR0oPcp27GTsH79ArpzVG3czjIAnedG07SThg70PmZBPpbYmBMux+30tej10EpN004SLX54pelwULZjJ+68PEJPMO0BgNulW/Sapp1cWnSL3iwpIW3iJNKuvhpPVjaWqBMP9B7dotcamcfjYfDgwf6UBABz586lb9++GIZBfYYmb926lZEjRxISEsKzzz57zPLbbruNpUuXAvDSSy/Rs2dP+vbty0MPHTtwIS0tjX79+tW5Do1JRLjxxhv90263m4SEhEqvYWN46623yMio9jbZNTrR97I6LTrQF3z9DWVbt2Lr1BEAS0xDtOid3rJ0i15rJP/85z+PGUPdr18/Pv74Y84666x6ldmqVSv+9a9/MXXq1GqX//LLL4wYMYLFixczf/581q9fz6ZNm2pcv6mUp2I4noiICDZu3EhpaSkAixYton379o1ZNaB+gf5E38vqtOium5JflmNp3ZrOH3zAkbffIXbixBMu0+30BnqrPfA7tGunoK8egUMbGrbMNv3h4trvMZyens6XX37JY489xvPPP++ff6IXciUmJpKYmOjPclnRli1b6NGjBxaLhVdffZVHHnmEEN+ossQ6jFB7/fXXmTFjBk6nk27duvHuu+/i8XgYMGAA27dvx2azUVBQwIABA9ixYwf79u3jjjvuICsri/DwcF5//XV69erFzTffTKtWrVizZg1Dhgxh3Lhx3HPPPYC35f7TTz9Vm1Tt4osv5ssvv+Sqq65i1qxZTJo0iSVLlgDe7Jx33XUXGzZswO12M23aNMaPH09aWho33ngjxcXFALz88sucfvrp/PDDD0ybNo3WrVuzceNGhg4dynvvvVfpNn8fffQRK1eu5PrrrycsLIyff/6ZZcuWMXXqVNxuN8OGDePVV1/1v5blGuOivBbdoi/bvYfQHj2wxseTeP99J5T2oFx5i14nNNMaw7333svTTz+N0YS3qPzqq6+46KKLANi+fTtLlizhtNNO4+yzz2bFihUBlzNhwgRWrFjBunXr6N27N2+88QZRUVGMHj3a/wUze/ZsrrzySmw2G5MnT+all15i1apVPPvss9x+++3+srZv3863337Lc889x7PPPssrr7zC2rVrWbJkSY0J1iZOnMjs2bNxOBysX7+e0047zb9s+vTpnHvuuaxYsYLFixfz4IMPUlxcTGJiIosWLWL16tXMmTOHu+++27/NmjVrePHFF9m8eTO7d+/2d22Vu+qqq0hNTeX9999n7dq1iAg333wzc+bM8X+hvPrqqwG/fieiRbfozZISbO1OLB1xVf4WvU236IPacVrejeGLL74gMTGRoUOH8sMPPzTZfr/55htmzpwJeLtKcnNzWb58OStWrOCaa65h9+7dAd2weuPGjTz++OPk5eVRVFTEhRdeCHhvWfj0009z+eWXM3PmTF5//XWKiopYtmwZV199tX/7srIy//Orr74ai8UCwBlnnMH999/P9ddfz4QJE0hOTq52/wMGDCAtLY1Zs2ZxySWXVFq2cOFCPvvsM//5CYfDwb59+2jXrh133nkna9euxWKxsH37dv82w4cP9+9r0KBBpKWlMWrUqBqPf9u2baSkpNCjRw8AbrrpJl555RXuvffe4752J6rFB3ojPLxBy/Q4dYteaxxLly7ls88+Y8GCBTgcDgoKCrjhhht47733Atr+scce87ecA80yWVJSQl5eHu18DaLk5GQmTJiAiDB8+HAMwyA7O5uEhITjlnXzzTfz6aefMnDgQN566y3/l9UZZ5xBWloaP/74Ix6Ph379+lFQUEBsbGyN9YyocAe4Rx55hEsvvZQFCxYwYsQIvv32W39St6rGjRvH1KlT+eGHH8jJyfHPV0oxb948evbsWWn9adOmkZSUxLp16zBNk9DQUP+yil0uFVM316Q584q16K6bhrg3bFVulwsxDAxfa0PTGsrf/vY30tPTSUtLY/bs2Zx77rkBB3nwdk+sXbu2TqmEFy9ezDnnHL2I8PLLL/ff1Wr79u04nU5aB5gbqrCwkLZt2+JyuXj//fcrLfvNb37DpEmTuOWWWwCIjo4mJSWFuXPnAt4guW7dumrL3bVrF/379+fhhx8mNTWVrVu31liH3/72t/zxj3+kf//+leZfeOGFvPTSS/5gvGbNGsB7R662bdtiGIb/nEJdREVFUVhYCECvXr1IS0tj586dALz77rucffbZdSqvvgIK9CJykYhsE5GdIvJINctHi0i+iKz1Pf5YYVmaiGzwzW/6lJQ1UEo1Sove7SzTrXmtyX3yySckJyfz888/c+mll/q7RQJ16NAhkpOTef7553nqqadITk6moKCgUv88eAPl7t276devHxMnTuTtt9+utttm27ZtJCcn+x9z587lL3/5C6eddhpjxow5psV9/fXXk5uby6RJk/zz3n//fd544w0GDhxI3759mT9/frV1f/HFF+nXrx8DBw4kLCyMiy++uMbjTE5O9p+4reiJJ57A5XIxYMAA+vXrxxNPPAHA7bffzttvv82IESPYvn17pV8Sgbj55puZMmUKgwYNQinFzJkzufrqq+nfvz+GYTBlypRjtjnR97JaSqlaH4AF2AV0AezAOqBPlXVGA1/UsH0a0Pp4+6n4GDp0qGpsnrIytblnL5X12n8atNxF//23euXWSQ1apnZy2Lx5c3NXockNHjxYOZ3ORt/P3Llz1Q033NDo+wkW1X0WgZWqhpgaSB/9cGCnUmo3gIjMBsYDm2vd6iTnyc0DwIhs4K4b3aLXgsjq1asbfR933XUXX331FQsWLGj0fbVUgQT69sD+CtPpwGnVrDdSRNYBGcBUpVT5vckUsFBEFPAfpdSME6lwQynznT0P6da9Qcv1uFx6DL2m1cFLL73U3FUIeoEE+urGTVU9fbwa6KSUKhKRS4BPgfIIeoZSKkNEEoFFIrJVKfXTMTsRmQxMBujYsWOg9a+38puA2zs17L7cTqe+KlbTtJNKICdj04EOFaaT8bba/ZRSBUqpIt/zBYBNRFr7pjN8fzOBT/B2BR1DKTVDKZWqlEoNZKjWiTJLSgAafNRNSUG+znOjadpJJZAW/Qqgu4ikAAeAicB1FVcQkTbAYaWUEpHheL9AckQkAjCUUoW+5xcATzboEdSTWeoL9DVcRVcXRUdy+OqV5zi0awfO0lI6DRh8wmVqmqY1lOMGeqWUW0TuBL7BOwLnTaXUJhGZ4lv+GnAV8HsRcQOlwERf0E8CPvENv7ICHyilvm6kY6kTVVKC2O2Itf7XjCml+OKFv5OxYytFR3Jo1b4Dyb36Mmz8VQ1YU03TtBMT0Dh6pdQCpVQPpVRXpdR037zXfEEepdTLSqm+SqmBSqkRSqllvvm7ffMG+pZPb7xDqRuzpOSEW/OFOdls/2Up4TGxjL3vEW55/lXGTL6T2KQ2DVRLTTtq//79nHPOOfTu3Zu+ffvyz3/+079s2rRptG/fnkGDBjFo0KB6j2AZP348I0eOrDSvYtndu3dnwoQJbN5cedDdmjVrEBG++eabSvObKz2wVlmLuzJWKUX2f2ZQuPgHJKLuF0sp06SkIJ9fPvmQz1/4GwCjb7yVHiNqznGhaQ3BarXy3HPPsWXLFpYvX84rr7xSKeDed999/itfq+ZyCUReXh6rV68mLy+PPXv2VFpWXvaOHTu49tprOffcc8nKyvIvnzVrFqNGjWLWrFmVtmuu9MBaZS0u141j/XqyXngBAHvXrv75yjTJ3r+XsuJixDCwhYZSWljAkYx08g4dJO9QBnmHDpKfeQiPL6eFYbEQn9yRNt16NMuxaM3nH7/+g61Har7Uvj56terFw8MfrnF527Ztadu2LeC9tL53794cOHCAPn36NMj+582bx9ixY0lKSmL27Nk8+uij1a537bXX8uWXX/LBBx9wzz33oJTio48+YtGiRZx55pk4HI5KOWFqSw+sNY0WFeiVx0PWP//ln44ZO9b/fOOP37LwtX9VtxnWkBDiktoSn9yRLkOHExETS9vuvUjq2h1D57XRmkFaWhpr1qyplGr35Zdf5p133iE1NZXnnnuOuDqm3Z41axZ/+tOfSEpK4qqrrqox0AMMGTLEn1Nm6dKlpKSk0LVrV0aPHs2CBQuYMGGCf92JEyfy5JNPctlll7F+/Xp++9vf6kDfxFpMoFdKcfhvf6d42TISHrif+FtvRSrk9D60YzshERGMvfdRlDJxlTlwu1y0696L6ITEgNKwai1HbS3vxlZUVMSVV17Jiy++SLTvPse///3veeKJJxARnnjiCR544AHefPPNgMs8fPgwO3fuZNSoUYgIVquVjRs31nhLQFUhE+OsWbOY6Ltpz8SJE3n33XcrBfra0gNrTaPFBPqCH3/g4OxZSEJrLBecz+E9u3AUF2G63Xg8btK3bCSxc1c6DRjU3FXVtBq5XC6uvPJKf+71cklJSf7nv/vd76o92VlbmuI5c+aQm5tLSkoKAAUFBcyePZunnnqq2nqsWbOG1NRUPB4P8+bN47PPPmP69OkopcjJyaGwsLDSXZ5qSg+sNY2gDvQbFy9i7cIF5B3OoKy4GPp29i546K5q1+9/7gVNVzlNqyOlFLfeeiu9e/fm/vvvr7Ts4MGD/v77Tz75pNqW+PTp05k+vfqBb7NmzeLrr7/2j7jZs2cPY8aMqTbQz5s3j4ULF/Lcc8/x7bffMnDgwEqjbW666SY+/fTTSqNtfvvb3xITE0P//v2b9KYpmldQBvojGeks/3gOW5YsJiapDb1OPxtj336c331PwtQHCImNIzQqmtCICCw2GxarDYvNRqu2ejSAdvJaunQp7777Lv3792fQoEEA/PWvf+WSSy7hoYce8t+urnPnzvznP/8JuNy0tDT27dvHiBEj/PNSUlKIjo7ml19+AeCFF17gvffeo7i4mH79+vH999+TkJDArFmzuOKKKyqVd+WVV/Lqq69WCvQ1pQfWmoZU7Gs7WaSmpqqVK+ueul55FOnL1/GN76Rqv3POZ+CYi7Ha7OR+9BG5b79D54/mYoTo7JJa3e3ITqNXz+rvXKRpDcWwHX9wx5YtW465ibiIrFJKpVa3ftC06JVSZL+9CdleyEXtvXepYTvkbN/oW6MLEedNI+vVTTWWoWm18YyLxH24pLmroQUzQ7C3i2zwYoMn0Je6ceYXs7twPYWheZxz8+RKy/M/nU/R0qW0f+bpZqqhdqrLNw9jaRV6/BU1rZ4aa3Rf0AR6I9zGarWY7dn/IzwmFnsbOPTUdKLOO5eYcePIn5eOyt9GeP/Gz4ypBSdjSzaWcH2vAe3UEzSBHvDf8GPcnVPZN/k2yrZsoXjZMmLGjUOVOhokU6WmadqpJqhy3bidTlq170CcR1G2ZQsAtjbe8cVmaSmiA72maS1QUAV60/RgsVjANAGwxMSg3B7vMkepbtFrmtYiBVeg93gQi8Uf3CUkBOXxPlclOtBrp7ba0hSD996rPXv2pG/fvjz00EP12kd90hSPHj2anj17+lMkX3XVVf7tnn32WQBmzpzpX2632/3XAjzyyCOAd5x+aGgo+fn59aq3Vrug6qM3TdObYMz0BXq7HXyZJk2HA0uruiV50rSTSXma4iFDhlBYWMjQoUMZM2YMffr0YfHixcyfP5/169cTEhJCZmZmncsvT1McGRnJnj17/OkQwJumeOrUqYA3XcK5557Lhg0bKL/t5/vvv09qarVDuAG45ZZbuOUW77Dnzp07s3jxYlq3bu1fPmvWLIYNG8Ynn3zCzTffXOe6a7ULrkDv8WAYFpTH23UjdjtmcbF3WWkptjB95avWMA799a+UbWnYNMUhvXvR5g9/qHF5bWmKX331VR555BFCfBcDJiYm1nn/9U1TfKJ27dpFUVERzzzzDH/96191oG8EQdV1ozyeY1r05V03ZmkJRqgeA60Fh6ppirdv386SJUs47bTTOPvss1mxYkWdyyzPFT9p0qRjbiBSVcU0xQDXX3+9v2vmwQcfrNd+zzzzTLZt21avXyNa7YKqRe/xeLDarP7gXrHrRpU6kHDdR681jNpa3o2tujTFbreb3Nxcli9fzooVK7jmmmvYvXt3wBfgnEiaYjh+101tZs+ezSeffIJhGEyYMIG5c+dyxx131KssrXpB16IX4+ioG7Hb8OTlcfhvf8eTm4s1vvVxStC0k1tNaYqTk5OZMGECIsLw4cMxDIPs7OxK2z722GP+VndVFdMUd+7cmbS0NGbPnl1jPdasWXNMrpX6WL9+PTt27GDMmDF07tyZ2bNnH/fXhFZ3QRXoTdPbdVPeojfsdgCOvP02YalDiZt4bXNWT9NOSG1pii+//HK+//57wNuN43Q6K53sBG+a4vJ7ylZVnqY4LS2NtLQ0Vq1aVWOgL09TPGnSpBM+plmzZjFt2jT/fjMyMjhw4AB79+494bK1o4Ir0Hs8GBbr0Ra9zRvoQwcOoPN772FtrVv02qmrPE3x999/72+ZL1iwAPDme9+9ezf9+vVj4sSJvP322wF32wSaprh8eOV7773nT1NcrmIf/fnnn++f/9RTT5GcnOx/VDV79uxj0hxfccUVtf6a0OouqNIUv/XA7bRqn8zZPQeS8cBUosaMoXDRIqIuuojkF19ohJpqLUl1qWE1rTnUNU1xQC16EblIRLaJyE4ReaSa5aNFJF9E1voefwx024ZkmiZGpT56b4ve4jthpWma1hIdd9SNiFiAV4AxQDqwQkQ+U0ptrrLqEqXUZfXctkGUD6/0j7qxeZOcGRERjbE7TdO0U0IgLfrhwE6l1G6llBOYDYwPsPwT2bbOyk/GUp72wDeeXgd6TdNaskACfXtgf4XpdN+8qkaKyDoR+UpE+tZx2wZhut2IcbRFrxxlABiROtBrmtZyBXLBVHWn7quewV0NdFJKFYnIJcCnQPcAt/XuRGQyMBmgY8eOAVTrWN5cN4a/j950lAK6Ra9pWssWSIs+HehQYToZyKi4glKqQClV5Hu+ALCJSOtAtq1QxgylVKpSKrXisK26MKv00atSBwBGeHi9ytM0TQsGgQT6FUB3EUkRETswEfis4goi0kZ8g3ZFZLiv3JxAtm1I/nH0nvIWvS/Q6xa9FgRqS1N87bXX+sexd+7cudqrX2vz1ltvkZCQwKBBg+jVqxcvvHB0OPK0adMQEXbu3Omf98ILLyAilA+DfvPNN+nfvz8DBgygX79+zJ8/H4Cbb76ZlJQUBg0axJAhQ/j555+PmT9w4EC+++47f9lOp5N7772Xrl270r17d8aPH096erp/ucViYdCgQfTr14+rr76akhJ9w/bjOW6gV0q5gTuBb4AtwIdKqU0iMkVEpvhWuwrYKCLrgH8BE5VXtds2xoFAhStjfSdhLVFRAFjjdHpi7dRXnqZ4y5YtLF++nFdeecWfF37OnDn+q16vvPLKSukRAnXttdeydu1ali5dyvTp09m//+jptf79+1e6iOmjjz6iT58+AKSnpzN9+nT+97//sX79epYvX86AAQP86z7zzDOsXbuWv//979x2223HzH/xxReZMmWKf/4f/vAHCgsL2b59Ozt27ODyyy9nwoQJ/vw6YWFhrF27lo0bN2K323nttdfqfKwtTUBJzXzdMQuqzHutwvOXgZcD3baxnH+r94IpflwKQNu/PEnRkiWEVvjQaVpDWPLhdrL3FzVoma07RHLmNT1qXF5bmuJySik+/PBDfzqE+oiPj6dbt24cPHiQDh28Pa+XX3458+fP5/HHH2f37t3ExMRg8w1fzszMJCoqisjISAAiIyP9zys666yzKv0qKDdy5EgOHDgAQElJCTNnzmTPnj3eu8XhzWX/5ptv8v3333PeeedV2vbMM89k/fr19T7WliKoUiD0Pfs82nbrebRF36oVcRMnBnwpuKadKqqmKS63ZMkSkpKS6N69e73L3rdvHw6Ho1KrPDo6mg4dOrBx40ZmzZrFtdcezRs1cOBAkpKSSElJ4ZZbbuHzzz+vttzPP/+c/v37HzP/66+/5vLLLwdg586ddOzY0Z+Vs1xqaiqbNlXuDHC73Xz11VfVlqlVFlRpiv18ffT4WgSa1tBqa3k3turSFJcrz+1eH3PmzGHx4sVs27aN119/ndAq92+YOHEis2fP5ptvvuG7775j5syZgLfP/Ouvv2bFihV899133HfffaxatYpp06YB8OCDD/LUU0+RkJDAG2+84S/vwQcf5KGHHiIzM5Ply5cD3l8k1TXMKs4vLS31n4M488wzufXWW+t1vC1JULXoy5W36MUIysPTWrCa0hSDt4X78ccfV2ptV1RbmmLw9tFv2rSJJUuW8MADD3Do0KFKy8eOHcu7775bbYu7PD3yo48+yuzZs5k3b55/WXlf/KJFiyrlt3/mmWfYuXMnTz31FDfddBMA3bp1Y+/evRQWFlYqf/Xq1f4uqvI++rVr1/LSSy9h96U60WoWnJFQt+i1IFRbmmKAb7/9ll69elWbJRJqT1Nc0ciRI7nxxhuPufl4WFgY//jHP3jssccqzc/IyGD16tX+6bVr19KpU6eAjskwDO655x5M0+Sbb74hIiKCm266ifvvvx+Pb5j0O++8Q0lJCeeee25AZWrHCspArzxuENF981pQqS1NMXhT/jZEjniAhx9+mJkzZx7Tsp44cSJDhgypNM/lcjF16lR69erFoEGDmDNnzjFfErURER5//HGefvppAP72t78RGhpKjx496N69O3PnzuWTTz7R/88nIKjSFJfLfO55ct56i94b9Nl4reHoNMXayaJR0hSfapTp0f3zmqZpPsEZDT2m7p/XNE3zCcpAr1v0mqZpRwVnNNQtek3TNL+gDPS6Ra9pmnZUcEZD3aLXNE3zC8pAr1v0WjCqLU3x2rVrGTFiBIMGDSI1NZVff/21XvsYP348I0eOrDRv2rRptG/fnkGDBtG9e3cmTJjgz5pZbs2aNYgI33zzTaX5O3bs4LLLLqNr164MHTqUc845h59++gmoPTVybWWWpynu27cvAwcO5Pnnn8f03WxIq15wRkPdoteCUG1pih966CH+9Kc/sXbtWp588kkeeuihOpefl5fH6tWrycvLY8+ePZWW3Xfffaxdu5YdO3Zw7bXXcu6555KVleVfPmvWLEaNGsWsWbP88xwOB5deeimTJ09m165drFq1ipdeeondu3f716ktNXJ1ZcLRFAibNm1i0aJFLFiwgD//+c91Pt6WJDiTmukWvdbIFr81g8y9u4+/Yh0kdurCOTdPrnF5bWmKRYSCggIA8vPzadeuXZ33P2/ePMaOHUtSUhKzZ8/m0UcfrXa9a6+9li+//JIPPviAe+65B6UUH330EYsWLeLMM8/E4XAQGhrK+++/z8iRIxk3bpx/2379+lXKd1OuamrkmsqsKjExkRkzZjBs2DD/DVK0YwVlNFRuj27Ra0GtapriF198kQcffJAOHTowdepU/va3v9W5zPLMl5MmTTqmFV3VkCFD2Lp1K+BNzZCSkkLXrl0ZPXq0Py3Dpk2bjkmXUJOqqZFrKrM6Xbp0wTRNMjMzA9pXSxSULXplehAd6LVGVFvLu7FVl6b41Vdf5YUXXuDKK6/kww8/5NZbb+Xbb78NuMzDhw+zc+dORo0ahYhgtVrZuHFjta1vgIqpU2bNmsXEiRMBby6cd999t9o7XF1xxRXs2LGDHj168PHHHwM1p0YOtMzq6qMdKyhb9N4++uA8NK1lqylN8dtvv+2fvvrqq6s9GVtbmuI5c+aQm5tLSkoKnTt3Ji0trdKtA6tas2YNvXv3xuPxMG/ePJ588kk6d+7MXXfdxVdffUVhYSF9+/atlNXyk08+4a233uLIkSP+edWlRq6tzOrs3r0bi8VCYmLicV+/liooo6F31I1u0WvBpbY0xe3atePHH38E4Pvvv6/2DlO1pSmeNWsWX3/9NWlpaaSlpbFq1aoaA/28efNYuHAhkyZN4ttvv2XgwIHs37+ftLQ09u7dy5VXXsmnn37Kddddx9KlS/nss8/829Z0I++KqZFrK7OqrKwspkyZwp133qn752sRlF03etSNFozK0xT379/f3yr/61//yiWXXMLrr7/OPffcg9vtJjQ0lBkzZgRcblpaGvv27WPEiBH+eSkpKURHR/PLL78A8MILL/Dee+9RXFxMv379+P7770lISGDWrFlcccUVlcq78sorefXVV7nxxhv54osvuP/++7n33ntJSkoiKiqKxx9/vNp6PPzwwwwZMoTt27fXWmb5HaZcLhdWq5Ubb7yx2vz82lFBmaZ432234cnOIWXeRw1YK62l02mKtZOFTlMMukWvaZpWQXAGej2OXtM0zS+gaCgiF4nINhHZKSKP1LLeMBHxiMhVFealicgGEVkrIvXvj6kDpVv0mqZpfsc9GSsiFuAVYAyQDqwQkc+UUpurWe8fwDfHlsI5SqnsBqhvYDy6Ra9pmlYukGg4HNiplNqtlHICs4Hx1ax3FzAPaPbL05SpW/SapmnlAgn07YH9FabTffP8RKQ9cAXwWjXbK2ChiKwSkSa5nFB53LpFr2ma5hNINKzuKoSqYzJfBB5WSnmqWfcMpdQQ4GLgDhE5q9qdiEwWkZUisrJiVrx68Zhg1S16LbjUlqZ43bp1jBw5kv79+zN27Fh/grNAHT58mMsuu4yBAwfSp08fLrnkEv+yQFINDx48mO7du3PhhReybNmyhjlgreEopWp9ACOBbypMPwo8WmWdPUCa71GEt/vm8mrKmgZMPd4+hw4dqk7EriuuUPtum3JCZWhaVZs3b27W/WdkZKhVq1YppZQqKChQ3bt3V5s2bVJKKZWamqp++OEHpZRSb7zxhnr88cfrVPbkyZPViy++6J9et26dUkqp0tJS1b17dzV//nz/sg0bNqiZM2cqpZSaOXOmuuOOO/zLvv/+e5WUlNTsr1Wwq+71BVaqGmJqIFfGrgC6i0gKcACYCFxX5csipfy5iLwFfKGU+lREIgBDKVXoe34B8GR9vpDqRI+60RpZ3ue7cGYUN2iZ9nYRxI7tWuPy2tIUb9u2jbPO8v5YHjNmDBdeeCF/+ctfAt73wYMHueCCC/zT5Vkk65JqGOCcc85h8uTJzJgx45gbiWjN57hdN0opN3An3tE0W4APlVKbRGSKiEw5zuZJwP9EZB3wK/ClUurrE6308SiHA7HZGns3mtZsqqYp7tevnz+nzNy5cyvdwCMQd9xxB7feeivnnHMO06dPJyMjA6hbquFyFVMYayeHgHLdKKUWAAuqzKvuxCtKqZsrPN8NDDyB+tWZY9t2nHv3En3ppU25W62Fqa3l3diqS1P85ptvcvfdd/Pkk08ybtw47HZ7ncq88MIL2b17N19//TVfffUVgwcPZuPGjcesV12q4arUSZhWpaULuqEpBV98DkDkOec0c000reHVlKa4V69eLFy4kFWrVjFp0iS6dj32i6i2NMUArVq14rrrruPdd99l2LBh/PTTTwGlGq6qPIWxdvIIqkCvlKJo6VJC+/UjrH/1fYiadqpStaQpLr+7kmmaPPXUU0yZcmyvam1pir///nt/CuHCwkJ27dpFx44d65RqGODHH39kxowZ/O53v6vPIWqNJGjSFJsOB2kTJ1G2dSttp09v7upoWoOrLU3xrFmzeOWVVwCYMGECt9xyS53KXrVqFXfeeSdWqxXTNPm///s/hg0bBnDcVMNz5szhf//7HyUlJaSkpDBv3jzdoj/JBFWa4gMPPURoz160uvkmfStBrcHpNMXayaKuaYqDpkUP0P7pp5u7CpqmaSedoOqj1zRN046lA72m1cHJ2NWptSz1+QzqQK9pAQoNDSUnJ0cHe63ZKKXIyckhNDS0TtsFVR+9pjWm5ORk0tPTOeGke5p2AkJDQ0lOTq7TNjrQa1qAbDYbKSkpx19R004yuutG0zQtyOlAr2maFuR0oNc0TQtyJ+WVsSKSBeyt5+atgaa7EfnJQR9zy6CPOfidyPF2UkolVLfgpAz0J0JEVtZ0GXCw0sfcMuhjDn6Ndby660bTNC3I6UCvaZoW5IIx0M9o7go0A33MLYM+5uDXKMcbdH30mqZpWmXB2KLXNE3TKtCBXtM0LcgFTaAXkYtEZJuI7BSRR5q7Pg1FRDqIyGIR2SIim0TkHt/8ViKySER2+P7GVdjmUd/rsE1ELmy+2p8YEbGIyBoR+cI3HdTHLCKxIvKRiGz1vd8jW8Ax3+f7XG8UkVkiEhpsxywib4pIpohsrDCvzscoIkNFZINv2b9ERAKuhFLqlH8AFmAX0AWwA+uAPs1drwY6trbAEN/zKGA70Ad4GnjEN/8R4B++5318xx8CpPheF0tzH0c9j/1+4APgC990UB8z8Dbwf77ndiA2mI8ZaA/sAcJ80x8CNwfbMQNnAUOAjRXm1fkYgV+BkYAAXwEXB1qHYGnRDwd2KqV2K6WcwGxgfDPXqUEopQ4qpVb7nhcCW/D+g4zHGxjw/b3c93w8MFspVaaU2gPsxPv6nFJEJBm4FPhvhdlBe8wiEo03ILwBoJRyKqXyCOJj9rECYSJiBcKBDILsmJVSPwFHqsyu0zGKSFsgWin1s/JG/XcqbHNcwRLo2wP7K0yn++YFFRHpDAwGfgGSlFIHwftlACT6VguW1+JF4CHArDAvmI+5C5AFzPR1V/1XRCII4mNWSh0AngX2AQeBfKXUQoL4mCuo6zG29z2vOj8gwRLoq+urCqpxoyISCcwD7lVKFdS2ajXzTqnXQkQuAzKVUqsC3aSaeafUMeNt2Q4BXlVKDQaK8f6kr8kpf8y+funxeLso2gERInJDbZtUM++UOuYA1HSMJ3TswRLo04EOFaaT8f4EDAoiYsMb5N9XSn3sm33Y93MO399M3/xgeC3OAMaJSBrebrhzReQ9gvuY04F0pdQvvumP8Ab+YD7m84E9SqkspZQL+Bg4neA+5nJ1PcZ03/Oq8wMSLIF+BdBdRFJExA5MBD5r5jo1CN+Z9TeALUqp5yss+gy4yff8JmB+hfkTRSRERFKA7nhP4pwylFKPKqWSlVKd8b6X3yulbiC4j/kQsF9EevpmnQdsJoiPGW+XzQgRCfd9zs/Dew4qmI+5XJ2O0de9UygiI3yv1W8qbHN8zX1GugHPbF+Cd0TKLuCx5q5PAx7XKLw/0dYDa32PS4B44Dtgh+9vqwrbPOZ7HbZRhzPzJ+MDGM3RUTdBfczAIGCl773+FIhrAcf8Z2ArsBF4F+9ok6A6ZmAW3nMQLrwt81vrc4xAqu912gW8jC+zQSAPnQJB0zQtyAVL142maZpWAx3oNU3TgpwO9JqmaUFOB3pN07QgpwO9pmlakNOBXtM0LcjpQK9pmhbk/h+qA8vOY7tcQwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Day 14-2 cell [62]\n",
    "plt.plot(fit_model_00.history[\"accuracy\"])\n",
    "plt.plot(fit_model_A41.history[\"accuracy\"])\n",
    "plt.plot(fit_model_A25.history[\"accuracy\"])\n",
    "plt.plot(fit_model_A26.history[\"accuracy\"])\n",
    "plt.plot(fit_model_A27.history[\"accuracy\"])\n",
    "plt.plot(fit_model_A28.history[\"accuracy\"])\n",
    "plt.plot(fit_model_A29.history[\"accuracy\"])\n",
    "\n",
    "plt.title(\"accuracy_function - Training\")\n",
    "plt.legend([\"Baseline\", \n",
    "            \"41 - 1/6 Layers Mean to 1\",\n",
    "            \"25 - ADAM\",\n",
    "            \"26 - ADELTA\",\n",
    "            \"27 - RMSPROP\",\n",
    "            \"28 - ADAGRAD\",\n",
    "            \"29 - SGD\"\n",
    "           ])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Save each of your alternative models as an HDF5 file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the file path for the first alternative model\n",
    "file_path = Path(\"./Resources/AlphabetSoup_A01.h5\")\n",
    "# Export your model to a HDF5 file\n",
    "nn_A41.save(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the file path for the fifth alternative model\n",
    "file_path = Path(\"./Resources/AlphabetSoup_A02.h5\")\n",
    "# Export your model to a HDF5 file\n",
    "nn_A25.save(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the file path for the sixth alternative model\n",
    "file_path = Path(\"./Resources/AlphabetSoup_A03.h5\")\n",
    "# Export your model to a HDF5 file\n",
    "nn_A26.save(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the file path for the seventh alternative model\n",
    "file_path = Path(\"./Resources/AlphabetSoup_A04.h5\")\n",
    "# Export your model to a HDF5 file\n",
    "nn_A27.save(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the file path for the eighth alternative model\n",
    "file_path = Path(\"./Resources/AlphabetSoup_A05.h5\")\n",
    "# Export your model to a HDF5 file\n",
    "nn_A28.save(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the file path for the ninth alternative model\n",
    "file_path = Path(\"./Resources/AlphabetSoup_A06.h5\")\n",
    "# Export your model to a HDF5 file\n",
    "nn_A29.save(file_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
